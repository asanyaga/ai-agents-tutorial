{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33335ae9",
   "metadata": {},
   "source": [
    "# Agents Tool Use: Adding Actions to your code assistant\n",
    "\n",
    "## Introduction\n",
    " So far, our **AI Code Assistant** has learned how to think - it can read code and make intelligent suggestions.\n",
    "\n",
    " But it still can't do anything.  \n",
    " It just talks about code instead of working with code.\n",
    "\n",
    " In this part, we will change that by introducing **tools** ; external actions your agent can perform beyond text generation.\n",
    "\n",
    " By the end of this tutorial, your assistant will:\n",
    " * Choose between **analyzing** code or **reading** from a file,\n",
    " * Dynamically decide what to do,\n",
    " * And execute the right tool to complete the job\n",
    "\n",
    " ## Concept: Tools are Actions\n",
    " In the real worlds, a human code reviewer might:\n",
    " * Open files\n",
    " * Edit code\n",
    " * Run tests\n",
    "\n",
    "\n",
    " LLMs cannot directly do that.\n",
    " With tool calling, we can connect them to real Python functions that perform these tasks.\n",
    " Tools are just code functions that the LLM can request via a formatted response to be executed. LLMs cannot directly execute this code\n",
    "\n",
    "## The Tool Registry\n",
    "As we have seen above, tools are really just functions that we call after giving it a task and asking it to determine the best function to use.  \n",
    "The tools registry is simply a list of callable functions that the LLM has access to. We will give our agent multiple tools, like:\n",
    "\n",
    "\n",
    " | Tool Name      | Description                      | Function                  |\n",
    " | -------------- | -------------------------------- | ------------------------- |\n",
    "| `read_file`    | Read code from a file            | `def read_file(path)`      |\n",
    "| `analyze_code` | Analyze a string of code         | LLM reasoning via API call |\n",
    "\n",
    "The agent decides; \"Do I need to read a file, test or analyze the given snippet?\"  \n",
    "We are keeping our tools simple here so that we focus on the concepts.\n",
    "\n",
    "Let's go ahead and build some tools, and give them to the code assistant that decides which tool to use based on user instruction, and run the tool.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc47d88",
   "metadata": {},
   "source": [
    "### Step 1 Define the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "979f27a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from typing import Callable, Dict\n",
    "\n",
    "def read_file(filepath: str) -> str:\n",
    "    \"\"\"Read contents of a Python file\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        return f\"File not found: {filepath}\"\n",
    "    \n",
    "    with open(filepath, \"r\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def analyze_code(code: str) -> str:\n",
    "    \"\"\"Ask an LLM to analyze the provided code.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful code review assistant.\n",
    "    Analyze the following Python code and suggest one improvement.\n",
    "\n",
    "    Code:\n",
    "    {code}\n",
    "    \"\"\"\n",
    "\n",
    "    response = openai.responses.create(model=\"gpt-4.1-mini\",input=[{\"role\":\"user\",\"content\":prompt}])\n",
    "\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d9aedc",
   "metadata": {},
   "source": [
    "### Step 2: Build a simple tool registry\n",
    "The tools registry is a class that holds a list of Python functions, and allows us to execute a given function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7c7b79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolRegistry:\n",
    "    \"\"\"Holds available tools and dispatches them by name.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.tools: Dict[str,Callable] = {}\n",
    "    \n",
    "    def register(self, name:str, func: Callable):\n",
    "        self.tools[name] = func\n",
    "\n",
    "    def call(self, name:str, *args, **kwargs):\n",
    "        if name not in self.tools:\n",
    "            return f\"Unknown tool: {name}\"\n",
    "        return self.tools[name](*args, **kwargs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f501cb",
   "metadata": {},
   "source": [
    "### Step 3: Agent with tool use\n",
    "We will build on the CodeReview agent from the agent loop tutorial [CodeReviewAgent](/code_review_agent.ipynb)  \n",
    "Most recent LLM models have support for *function calling*. Function calling is a capability of language models that enables them to output text in a structured format suitable for executing a function or code.\n",
    "\n",
    "**NOTE:**  Not all LLM models have function calling and structured output capabilities. For models that dont support function calling, this capability is achieved with well structured prompts and instructions.\n",
    "1. Initialize the agent with a `ToolRegistry`\n",
    "```python\n",
    "class CodeReviewAgentWithTools:\n",
    "    def __init__(self, tool_registry: ToolRegistry, model= \"gpt-4o-mini\"):\n",
    "        self.tools = tool_registry\n",
    "        self.model = model\n",
    "```\n",
    "2. Update the prompt in `think()` method to instruct the model to output its response in a specicic format, JSON. Here we are choosing JSON because it is a popular well supported format. LLMs will have seen a lot of JSON examples in their training and should be able to output the tools calls in the specified JSON format.\n",
    "\n",
    "```python\n",
    "    def think(self, user_input: str):\n",
    "        \"\"\"LLM decides which tool to use\"\"\"\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        You are a code assistant with access to the tools below.\n",
    "\n",
    "        Available tools:\n",
    "        - read_file(filepath)\n",
    "        - analyze_code(code)\n",
    "        - write_tests(test_code)\n",
    "\n",
    "        Decide which tool is most appropriate based on the user input below.\n",
    "        Reply ONLY with the tool call to make in JSON format {{\"tool\": \"tool_name\", \"args\": [\"arg1\", \"arg2\"]}} (e.g., {{\"tool\":\"patch_file\", \"args\":[\"file_path\",\"content\"]}}\n",
    "\n",
    "        Examples:\n",
    "        read_file(\"main.py\")\n",
    "        analyze_code(\"def foo():pass\")\n",
    "        write_tests(\"def foo():pass\")\n",
    "\n",
    "        user input: {user_input}\n",
    "        \"\"\"\n",
    "\n",
    "        response = openai.responses.create(model=self.model, input=[{\"role\":\"user\",\"content\":prompt}])\n",
    "\n",
    "        return response.output_text\n",
    "```\n",
    "3. Update the `act()` method to call a tool based on the LLM selected tool. \n",
    "```python\n",
    "    def act(self, decision:str):\n",
    "        \"\"\"Execute the chosen tool and output the result.\"\"\"\n",
    "        try:\n",
    "            parsed = json.loads(decision)\n",
    "            tool_name = parsed[\"tool\"]\n",
    "            args = parsed.get(\"args\",[])\n",
    "\n",
    "            result = self.tools.call(tool_name,*args)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error executing tool: {e}\"\n",
    "            return error_msg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f889fe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "class CodeReviewAgentWithTools:\n",
    "    def __init__(self, tool_registry: ToolRegistry, model= \"gpt-4o-mini\"):\n",
    "        self.tools = tool_registry\n",
    "        self.model = model\n",
    "    \n",
    "    def think(self, user_input: str):\n",
    "        \"\"\"LLM decides which tool to use\"\"\"\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        You are a code assistant with access to the tools below.\n",
    "\n",
    "        Available tools:\n",
    "        - read_file(filepath)\n",
    "        - analyze_code(code)\n",
    "\n",
    "        Decide which tool is most appropriate based on the user input below.\n",
    "        Reply ONLY with the tool call to make in JSON format {{\"tool\": \"tool_name\", \"args\": [\"arg1\", \"arg2\"]}} (e.g., {{\"tool\":\"read_file\", \"args\":[\"sample.py\"]}}\n",
    "\n",
    "        Examples:\n",
    "        read_file(\"main.py\")\n",
    "        analyze_code(\"def foo():pass\")\n",
    "\n",
    "        user input: {user_input}\n",
    "        \"\"\"\n",
    "\n",
    "        response = openai.responses.create(model=self.model, input=[{\"role\":\"user\",\"content\":prompt}])\n",
    "\n",
    "        return response.output_text\n",
    "    \n",
    "    def act(self, decision:str):\n",
    "        \"\"\"Execute the chosen tool and record the result.\"\"\"\n",
    "        try:\n",
    "            parsed = json.loads(decision)\n",
    "            tool_name = parsed[\"tool\"]\n",
    "            args = parsed.get(\"args\",[])\n",
    "\n",
    "            result = self.tools.call(tool_name,*args)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error executing tool: {e}\"\n",
    "            return error_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b724c392",
   "metadata": {},
   "source": [
    "### Register tools and run a demo\n",
    "1. Add tools to the registry\n",
    "2. Create the agent with tools\n",
    "3. Ask the agent what to do given a user request and tools\n",
    "4. Ask the agent to act on the decision\n",
    "\n",
    "**NOTE:** Usually the above sequence of actions will run consecutively and you might have seen agent frameworks that have a single `run` or similar command. We are keeping things simple here to demonstrate concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caf2b9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Decision:\n",
      "{\"tool\":\"read_file\",\"args\":[\"sample.py\"]}\n",
      "Tool Call Result:\n",
      "def divide(a, b):\n",
      "    return a / b\n"
     ]
    }
   ],
   "source": [
    "registry = ToolRegistry()\n",
    "\n",
    "# Register the tools we defined above\n",
    "registry.register(\"read_file\", read_file)\n",
    "registry.register(\"analyze_code\",analyze_code)\n",
    "\n",
    "agent = CodeReviewAgentWithTools(registry)\n",
    "\n",
    "user_request = \"Please review the code in sample.py\"\n",
    "\n",
    "# We give the agent an observation (user request) to think about and give us a decision\n",
    "decision = agent.think(user_input=user_request)\n",
    "print(f\"Agent Decision:\\n{decision}\")\n",
    "\n",
    "# We give the agent the decision to act on\n",
    "result = agent.act(decision)\n",
    "print(f\"Tool Call Result:\\n{result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a21ba7",
   "metadata": {},
   "source": [
    "## Human in the loop (HIL)\n",
    "Human in the loop is the concept of requiring that the agent requests and receives human consent before performing actions. Agents are meant to be autonomous human assistants but they are not infallible and more importantly cannot be held accountable.  \n",
    "In situations where actions might be risky, it is good practice to require that a human review an agent's actions and approve or reject the agents proposed action.\n",
    "\n",
    "Below we shall implement a simple human in the loop by having the agent ask for human input before performing an action. If the response is \"YES\" the agent performs the action, otherwise it just exits\n",
    "\n",
    "We will modify our CodeAssistent agent to request input before acting\n",
    "\n",
    "```python\n",
    "    def act(self, decision: str):\n",
    "        \"\"\"Execute the chosen tool command with humn consent.\"\"\"\n",
    "        response = input(f\"I want to act on {decision}. Reply with YES or NO\")\n",
    "\n",
    "        #....existing act code...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f411bd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "class CodeReviewAgentWithToolsHIL:\n",
    "    def __init__(self, tool_registry: ToolRegistry, model= \"gpt-4o-mini\"):\n",
    "        self.tools = tool_registry\n",
    "        self.model = model\n",
    "    \n",
    "    def think(self, user_input: str):\n",
    "        \"\"\"LLM decides which tool to use\"\"\"\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        You are a code assistant with access to the tools below.\n",
    "\n",
    "        Available tools:\n",
    "        - read_file(filepath)\n",
    "        - analyze_code(code)\n",
    "\n",
    "        Decide which tool is most appropriate based on the user input below.\n",
    "        Reply ONLY with the tool call to make in JSON format {{\"tool\": \"tool_name\", \"args\": [\"arg1\", \"arg2\"]}}  \n",
    "\n",
    "        Example:\n",
    "        {{\"tool\":\"read_file\", \"args\":[\"sample.py\"]}}\n",
    "\n",
    "        user input: {user_input}\n",
    "        \"\"\"\n",
    "\n",
    "        response = openai.responses.create(model=self.model, input=[{\"role\":\"user\",\"content\":prompt}])\n",
    "\n",
    "        return response.output_text\n",
    "    \n",
    "    def act(self, decision: str):\n",
    "        \"\"\"Execute the chosen tool command.\"\"\"\n",
    "        response = input(f\"I want to act on {decision}. Reply with YES or NO\")\n",
    "        if response.lower()==\"yes\":\n",
    "            try:\n",
    "                parsed = json.loads(decision)\n",
    "                tool_name = parsed[\"tool\"]\n",
    "                args = parsed.get(\"args\",[])\n",
    "\n",
    "                result = self.tools.call(tool_name,*args)\n",
    "                return result\n",
    "            except Exception as e:\n",
    "                error_msg = f\"Error executing tool: {e}\"\n",
    "                return error_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f7780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_snippet = \"\"\"\n",
    "def divide(a,b):\n",
    "    return a/b\n",
    "\"\"\"\n",
    "\n",
    "agent_hil = CodeReviewAgentWithToolsHIL(tool_registry=registry)\n",
    "user_request = f\"Please review this code {code_snippet}\"\n",
    "\n",
    "# We give the agent an observation (user request) to think about and give us a decision\n",
    "decision = agent_hil.think(user_input=user_request)\n",
    "print(f\"Agent Decision: \\n{decision}\")\n",
    "\n",
    "# We give the agent the decision to act on\n",
    "result=agent_hil.act(decision=decision)\n",
    "print(f\"Tool Call Result:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f794aba",
   "metadata": {},
   "source": [
    "### What's next\n",
    "In the next part of this series we will give our agent memory.\n",
    "\n",
    "We will see why memory is an important part of agents and how to use and manage the different types of memory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
