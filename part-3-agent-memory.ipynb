{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01d71aa1",
   "metadata": {},
   "source": [
    "# Adding Memory to our Agent. Short-Term and Long-Term memory in practice\n",
    "\n",
    "## Introduction\n",
    "In our previous articles, we built our code review agent that use tools to read files and analyze code. But there's a critical limitation:**our agent has no memory between interactions**. Every time we call `think()`, the agent starts fresh, with no knowledge of previous conversations or actions.\n",
    "\n",
    "Imagine asking the agent to \"review the last file i mentioned\" or \"compare this code to what you saw earlier\". Without code it cant do either. In this article, we'll transform our stateless agent into one that remembers conversations, learns from interactions, and manages its memory efficiently.\n",
    "\n",
    "We will cover:\n",
    "* Why memory matters for agents\n",
    "* Short-term memory\n",
    "* Long-term memory\n",
    "* Memory summarization techniques\n",
    "* Context window managements and trimming strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d08a777",
   "metadata": {},
   "source": [
    "## Why memory matters\n",
    "**Memory enables continuity**. Without it agents can't:\n",
    "* Reference previous questions or answers\n",
    "* Build on past interactions\n",
    "* Learn user preferences\n",
    "* Handle multi-turn workflows (e.g. \"read the file, then analyze it, then write tests\")\n",
    "Real world conversations have context. Our agent needs memory to maintain that context and provide intelligent, contextual responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a751ad99",
   "metadata": {},
   "source": [
    "## Short term memory: Conversation History\n",
    "**Sort-term memory** stores the recent conversation between user and agent. This is the foundation of a multi-turn dialogue.\n",
    "\n",
    "### Implementation: Adding a Message Buffer\n",
    "Let's add a simple conversation history to our agent:\n",
    "\n",
    "### The agent code so far"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea23e433",
   "metadata": {},
   "source": [
    "```python\n",
    "class CodeReviewAgentWithTools:\n",
    "    def __init__(self, tool_registry: ToolRegistry, model= \"gpt-4o-mini\"):\n",
    "        self.tools = tool_registry\n",
    "        self.model = model\n",
    "    \n",
    "    def think(self, user_input: str):\n",
    "        \"\"\"LLM decides which tool to use\"\"\"\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        You are a code assistant with access to the tools below.\n",
    "\n",
    "        Available tools:\n",
    "        - read_file(filepath)\n",
    "        - analyze_code(code)\n",
    "        - write_tests(test_code)\n",
    "\n",
    "        Decide which tool is most appropriate based on the user input below.\n",
    "        Reply ONLY with the tool name and argument if needed.\n",
    "\n",
    "        Examples:\n",
    "        read_file(\"main.py\")\n",
    "        analyze_code(\"def foo():pass\")\n",
    "        \n",
    "\n",
    "        user input: {user_input}\n",
    "        \"\"\"\n",
    "\n",
    "        response = openai.responses.create(model=self.model, input=[{\"role\":\"user\",\"content\":prompt}])\n",
    "\n",
    "        return response.output_text\n",
    "    \n",
    "    def act(self, decision: str):\n",
    "        \"\"\"Execute the chosen tool command.\"\"\"\n",
    "\n",
    "        try:\n",
    "            if \"(\" in decision and \")\" in decision:\n",
    "                name, arg = decision.split(\"(\",1)\n",
    "                arg = arg.strip(\")'\\\"\")\n",
    "                return self.tools.call(name.strip(),arg)\n",
    "            else:\n",
    "                return self.tools.call(decision)\n",
    "        except Exception as e:\n",
    "            return f\"Error executing tool {e}\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c332bb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from typing import Dict, Callable\n",
    "\n",
    "def read_file(filepath: str) -> str:\n",
    "    \"\"\"Read contents of a Python file\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        return f\"File not found: {filepath}\"\n",
    "    \n",
    "    with open(filepath, \"r\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def analyze_code(code: str) -> str:\n",
    "    \"\"\"Ask an LLM to analyze the provided code.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful code review assistant.\n",
    "    Analyze the following Python code and suggest one improvement.\n",
    "\n",
    "    Code:\n",
    "    {code}\n",
    "    \"\"\"\n",
    "\n",
    "    response = openai.responses.create(model=\"gpt-4.1-mini\",input=[{\"role\":\"user\",\"content\":prompt}])\n",
    "\n",
    "    return response.output_text\n",
    "\n",
    "\n",
    "class ToolRegistry:\n",
    "    \"\"\"Holds available tools and dispatches them by name.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.tools: Dict[str,Callable] = {}\n",
    "    \n",
    "    def register(self, name:str, func: Callable):\n",
    "        self.tools[name] = func\n",
    "\n",
    "    def call(self, name:str, *args, **kwargs):\n",
    "        if name not in self.tools:\n",
    "            return f\"Unknown tool: {name}\"\n",
    "        return self.tools[name](*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c4eab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeReviewAgentWithSTMemory:\n",
    "    def __init__(self,tools_registry: ToolRegistry, model=\"gpt-4o-mini\"):\n",
    "        self.tools = tools_registry\n",
    "        self.model = model\n",
    "        self.conversation_history = [] # Short-term memory\n",
    "\n",
    "    def think(self, user_input:str):\n",
    "        \"\"\"LLM decides which tool to use with conversation context.\"\"\"\n",
    "        # Add user message to history\n",
    "        self.conversation_history.append({\"role\":\"user\",\"content\":user_input})\n",
    "\n",
    "        # Build prompt with system instructions\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\":\"system\",\n",
    "                \"content\":\"\"\"You are a code assistant with access to these tools:\n",
    "                - read_file(filepath)\n",
    "                - analyze_code(code)\n",
    "\n",
    "                Decide which tool to use based on the conversation.\n",
    "                Reply ONLY with the tool name and argument.\n",
    "                Examples: read_file(\"main.py\") or analyze_code(\"def foo():pass\")\n",
    "\n",
    "                \"\"\"\n",
    "            }\n",
    "        ] + self.conversation_history\n",
    "\n",
    "        response = openai.responses.create(model=self.model, input=messages)\n",
    "\n",
    "        decision = response.output_text\n",
    "\n",
    "        # Add assistant's decision to conversation history\n",
    "        self.conversation_history.append({\n",
    "            \"role\":\"assistant\",\n",
    "            \"content\": decision\n",
    "        })\n",
    "\n",
    "        return decision\n",
    "    \n",
    "    def act(self, decision:str):\n",
    "        \"\"\"Execute the chosen tool and record the result.\"\"\"\n",
    "        try:\n",
    "            if \"(\" in decision and \")\" in decision:\n",
    "                name, arg = decision.split(\"(\",1)\n",
    "                arg = arg.strip(\")'\\\"\")\n",
    "                result = self.tools.call(name.strip(),arg)\n",
    "            else:\n",
    "                result = self.tools.call(decision)\n",
    "\n",
    "            #Store tool call result in conversation history\n",
    "            self.conversation_history.append({\n",
    "                \"role\":\"system\",\n",
    "                \"content\":f\"Tool result: {result}\"\n",
    "            })\n",
    "\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error executing tool: {e}\"\n",
    "            self.conversation_history.append({\n",
    "                \"role\":\"system\",\n",
    "                \"content\": error_msg\n",
    "            })\n",
    "            return error_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4850b573",
   "metadata": {},
   "source": [
    "### What changed\n",
    "1. **`conversation_history` list**: Stores all messages as dictionaries with `role` and `content`\n",
    "2. **Messages passed to LLM**: Instead of a single prompt string, we send the entire conversation\n",
    "3. **Tool call result stored**: After each action we append the result to history so the agent can reference it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db536ca2",
   "metadata": {},
   "source": [
    "### Let's give it a try by simulating a multi-turn conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efff9650",
   "metadata": {},
   "outputs": [],
   "source": [
    "registry = ToolRegistry()\n",
    "registry.register(\"read_file\",read_file)\n",
    "registry.register(\"analyze_code\",analyze_code)\n",
    "\n",
    "agent_with_st_memory = CodeReviewAgentWithSTMemory(registry)\n",
    "\n",
    "# Multi-turn conversation\n",
    "decision1 = agent_with_st_memory.think(\"read the file sample.py\")\n",
    "print(f\"Agent first decision: {decision1}\")\n",
    "\n",
    "result1 = agent_with_st_memory.act(decision1)\n",
    "print(f\"Agent first action result: {result1}\")\n",
    "\n",
    "#The agent remembers what it read. We don't need to provide it the code\n",
    "decision2 = agent_with_st_memory.think(\"Analyze that code\")\n",
    "print(f\"Agent second decision: {decision2}\")\n",
    "\n",
    "result2 = agent_with_st_memory.act(decision2)\n",
    "print(f\"Second decision result: {result2}\")\n",
    "\n",
    "print(f\"Chat history : {agent_with_st_memory.conversation_history}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4faf0e49",
   "metadata": {},
   "source": [
    "***Key insight:** The LLM sees the full conversation each time, allowing it to understand context and references like \"that code\" or \"the last file\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937bfc17",
   "metadata": {},
   "source": [
    "## Long-term Memory: Persistent Knowledge\n",
    "Short-term memory is ephemeral-it exists only during a session. Long term memory persists across sessions and stores important information the agent should remember \n",
    "indefinitely.\n",
    "\n",
    "### Use cases for long term memory\n",
    "* **User preferences**: \"I prefer tests with pytest, not unittest\"\n",
    "* **Project context**: \"this is a fastapi web api with sqlalchemy models\"\n",
    "* **Learned patterns**: \"user often asks for sql injection vulnerabilities\"\n",
    "* **Important facts**: File paths, project structure, common issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5e11e3",
   "metadata": {},
   "source": [
    "### Implementation adding a knowledge store\n",
    "Let's add long term memory to our agent\n",
    "1. **Add** `long_term_memory` which here we implement as a simple key value store\n",
    "2. **Add** `remember()` adds/update key value in the long term memory\n",
    "3. **Add** `recall()` retrieves a particular item from the long term memory\n",
    "4. **Add** `get_relevant_memories()` gets and formats the long term memories to include in the system message\n",
    "5. **Add** `save_long_term_memory()` to persist long term memory to disk. This makes sure it persists between agent sessions\n",
    "6. **Add** `load_long_term_memory()` load long term memory when the agent initializes\n",
    "6. Update the system message to include the long term memory as relevant memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f4264b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Dict, Callable\n",
    "class CodeReviewAgentWithLTMemory:\n",
    "    def __init__(self,tools_registry: ToolRegistry, model=\"gpt-4o-mini\",memory_file=\"agent_memory.json\"):\n",
    "        self.tools = tools_registry\n",
    "        self.model = model\n",
    "        self.conversation_history = [] # Short-term memory\n",
    "        self.memory_file = memory_file\n",
    "        self.load_long_term_memory() # Long-term memory (key-value store)\n",
    "\n",
    "    def remember(self, key:str, value: str):\n",
    "        \"\"\"Retrieve information from long term memory.\"\"\"\n",
    "        self.long_term_memory[key] = value\n",
    "        self.save_long_term_memory()\n",
    "    \n",
    "    def recall(self,key:str) -> str:\n",
    "        \"\"\"Retrieve information from long term memory\"\"\"\n",
    "        return self.long_term_memory.get(key,\"No memory found for this key.\")\n",
    "    \n",
    "    def get_relevant_memories(self) -> str:\n",
    "        \"\"\"Format long term memories for inclusion in prompts.\"\"\"\n",
    "        if not self.long_term_memory:\n",
    "            return \"No stored memories\"\n",
    "        \n",
    "        memories = \"\\n\".join([f\"- {k}:{v}\" for k, v in self.long_term_memory.items()])\n",
    "        return f\"Relevant memories:\\n{memories}\"\n",
    "    \n",
    "    def save_long_term_memory(self):\n",
    "        \"\"\"Persist long term memory to JSON file\"\"\"\n",
    "        try:\n",
    "            with open(self.memory_file,\"w\") as f:\n",
    "                json.dump(self.long_term_memory,f,indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save memory to {self.memory_file}:  {e}\")\n",
    "\n",
    "    def load_long_term_memory(self):\n",
    "        \"\"\"Load long term memory from JSON file\"\"\"\n",
    "        if os.path.exists(self.memory_file):\n",
    "            try:\n",
    "                with open(self.memory_file, 'r') as f:\n",
    "                    self.long_term_memory = json.load(f)\n",
    "                print(f\"Loaded {len(self.long_term_memory)} memories from {self.memory_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load memory from {self.memory_file}: {e}\")\n",
    "        else:\n",
    "            self.long_term_memory = {}\n",
    "\n",
    "    def think(self, user_input:str):\n",
    "        \"\"\"LLM decides which tool to use with both short term and long term context.\"\"\"\n",
    "        # Add user message to history\n",
    "        self.conversation_history.append({\"role\":\"user\",\"content\":user_input})\n",
    "\n",
    "        #Include long term memory in system context\n",
    "        system_message_context = f\"\"\"You are a code assistant with access to these tools:\n",
    "                - read_file(filepath)\n",
    "                - analyze_code(code)\n",
    "\n",
    "                {self.get_relevant_memories()}\n",
    "\n",
    "                Decide which tool to use based on the conversation and relevant memories.\n",
    "                Reply ONLY with the tool name and argument.\n",
    "                Examples: read_file(\"main.py\") or analyze_code(\"def foo():pass\")\n",
    "\n",
    "                \"\"\"\n",
    "\n",
    "        # Build prompt with system instructions\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\":\"system\",\n",
    "                \"content\":system_message_context\n",
    "            }\n",
    "        ] + self.conversation_history\n",
    "\n",
    "        response = openai.responses.create(model=self.model, input=messages)\n",
    "\n",
    "        decision = response.output_text\n",
    "\n",
    "        # Add assistant's decision to conversation history\n",
    "        self.conversation_history.append({\n",
    "            \"role\":\"assistant\",\n",
    "            \"content\": decision\n",
    "        })\n",
    "\n",
    "        return decision\n",
    "    \n",
    "    def act(self, decision:str):\n",
    "        \"\"\"Execute the chosen tool and record the result.\"\"\"\n",
    "        try:\n",
    "            if \"(\" in decision and \")\" in decision:\n",
    "                name, arg = decision.split(\"(\",1)\n",
    "                arg = arg.strip(\")'\\\"\")\n",
    "                result = self.tools.call(name.strip(),arg)\n",
    "            else:\n",
    "                result = self.tools.call(decision)\n",
    "\n",
    "            #Store tool call result in conversation history\n",
    "            self.conversation_history.append({\n",
    "                \"role\":\"system\",\n",
    "                \"content\":f\"Tool result: {result}\"\n",
    "            })\n",
    "\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error executing tool: {e}\"\n",
    "            self.conversation_history.append({\n",
    "                \"role\":\"system\",\n",
    "                \"content\": error_msg\n",
    "            })\n",
    "            return error_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63451892",
   "metadata": {},
   "source": [
    "### Let's give it a try with multiturn conversations with access to persistent memory across sessions\n",
    "**Key insight:** Long term memory provides persistent context that informs every interaction, enabling the agent to personalize its behaviour and remember important facts across sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c03d18e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 memories from agent_memory.json\n",
      "First decision: analyze_code(\"def divide(a,b):\\n    return a/b\")\n"
     ]
    }
   ],
   "source": [
    "# Multi-turn conversation with long term memory\n",
    "registry = ToolRegistry()\n",
    "registry.register(\"read_file\",read_file)\n",
    "registry.register(\"analyze_code\",analyze_code)\n",
    "\n",
    "agent_with_lt_memory1 = CodeReviewAgentWithLTMemory(registry)\n",
    "code_snippet = \"\"\"\n",
    "def divide(a,b):\n",
    "    return a/b\n",
    "\"\"\"\n",
    "decision1_with_ltm1 = agent_with_lt_memory1.think(f\"Analyze this code:{code_snippet}\")\n",
    "\n",
    "print(f\"First decision: {decision1_with_ltm1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e620951b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent_with_lt_memory1.conversation_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e55c014",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_with_lt_memory1.remember(\"documentation\",\"add comprehensive documentation and doc string to ALL code you suggest\")\n",
    "agent_with_lt_memory1\n",
    "result2_with_ltm1 = agent_with_lt_memory1.act(decision1_with_ltm1)\n",
    "print(f\"Result with LTM: {result2_with_ltm1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8d53f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent_with_lt_memory1.conversation_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d465f97f",
   "metadata": {},
   "source": [
    "## Memory summarization: Keeping Context Compact\n",
    "As conversations, grow so does the memory footprint. A 50 turn conversation might contain thousands of tokens. Summarization compresses old conversation turns into consise summaries, preserving essential information while reducing token usgae.\n",
    "\n",
    "### When to summarize\n",
    "* After N turns\n",
    "* When conversation history exceeds a token threshold\n",
    "* When moving to a new topic or task\n",
    "\n",
    "Here, we are going to implement a simple periodic summarization where we use an LLM to generate a summary from the conversation history and trim the conversation to the last few turns.  \n",
    "1. Add `summarize_after` parameter to agent initialization to set after how many messages to summarize\n",
    "2. Add `conversation_summary` to keep the conversation summary\n",
    "3. Add `summarize_history()`: Periodically use LLM to summarize conversation history when the `summarize_after` message limit is reached\n",
    "4. Include the `conversation_summary` in the system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b56ae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeReviewAgentWithSTMemorySummarization:\n",
    "    def __init__(self,tools_registry: ToolRegistry, model=\"gpt-4o-mini\",memory_file=\"agent_memory.json\",summarize_after=10):\n",
    "        self.tools = tools_registry\n",
    "        self.model = model\n",
    "        self.conversation_history = [] # Short-term memory\n",
    "        self.memory_file = memory_file\n",
    "        self.load_long_term_memory() # Long-term memory (key-value store)\n",
    "        self.conversation_summary = \"\" # Summarized conversation history\n",
    "        self.summarize_after = summarize_after\n",
    "        self.turns_since_summary = 0\n",
    "\n",
    "    \n",
    "    def summarize_history(self):\n",
    "        \"\"\"Use LLM to summarize the conversation so far.\"\"\"\n",
    "        if len(self.conversation_history) < 3:\n",
    "            return\n",
    "        \n",
    "        history_text = \"\\n\".join([f\"{msg[\"role\"]}:{msg[\"content\"]}\" for msg in self.conversation_history])\n",
    "\n",
    "        summary_prompt = f\"\"\"Summarize this conversation in 3-4 sentences,\n",
    "        preserving key fact, decisions, and actions taken:\n",
    "        {history_text}\n",
    "\n",
    "        Previous Summary: {self.conversation_summary or 'None'}\n",
    "        \"\"\"\n",
    "\n",
    "        response = openai.responses.create(model=self.model, input=[{\"role\":\"user\",\"content\":summary_prompt}])\n",
    "\n",
    "        self.conversation_summary = response.output_text\n",
    "\n",
    "        # Keep only the last few turns + the summary\n",
    "        recent_turns = self.conversation_history[-4:] # Keep the last 4 messages (2 user/assistant exchanges)\n",
    "\n",
    "        self.conversation_history = recent_turns\n",
    "        self.turns_since_summary = 0\n",
    "\n",
    "\n",
    "    def remember(self, key:str, value: str):\n",
    "        \"\"\"Retrieve information from long term memory.\"\"\"\n",
    "        self.long_term_memory[key] = value\n",
    "        self.save_long_term_memory()\n",
    "    \n",
    "    def recall(self,key:str) -> str:\n",
    "        \"\"\"Retrieve information from long term memory\"\"\"\n",
    "        return self.long_term_memory.get(key,\"No memory found for this key.\")\n",
    "    \n",
    "    def get_relevant_memories(self) -> str:\n",
    "        \"\"\"Format long term memories for inclusion in prompts.\"\"\"\n",
    "        if not self.long_term_memory:\n",
    "            return \"No stored memories\"\n",
    "        \n",
    "        memories = \"\\n\".join([f\"- {k}:{v}\" for k, v in self.long_term_memory.items()])\n",
    "        return f\"Relevant memories:\\n{memories}\"\n",
    "    \n",
    "    def save_long_term_memory(self):\n",
    "        \"\"\"Persist long term memory to JSON file\"\"\"\n",
    "        try:\n",
    "            with open(self.memory_file,\"w\") as f:\n",
    "                json.dump(self.long_term_memory,f,indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save memory to {self.memory_file}:  {e}\")\n",
    "\n",
    "    def load_long_term_memory(self):\n",
    "        \"\"\"Load long term memory from JSON file\"\"\"\n",
    "        if os.path.exists(self.memory_file):\n",
    "            try:\n",
    "                with open(self.memory_file, 'r') as f:\n",
    "                    self.long_term_memory = json.load(f)\n",
    "                print(f\"Loaded {len(self.long_term_memory)} memories from {self.memory_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load memory from {self.memory_file}: {e}\")\n",
    "        else:\n",
    "            self.long_term_memory = {}\n",
    "\n",
    "    def think(self, user_input:str):\n",
    "        \"\"\"LLM decides which tool to use with both short term and long term context.\"\"\"\n",
    "        # Add user message to history\n",
    "        self.conversation_history.append({\"role\":\"user\",\"content\":user_input})\n",
    "\n",
    "        self.turns_since_summary += 1\n",
    "\n",
    "        # Check if we should summarize\n",
    "        if self.turns_since_summary >= self.summarize_after:\n",
    "            self.summarize_history()\n",
    "\n",
    "        #Include long term memory & summary in system context\n",
    "        system_message_context = f\"\"\"You are a code assistant with access to these tools:\n",
    "                - read_file(filepath)\n",
    "                - analyze_code(code)\n",
    "\n",
    "                {self.get_relevant_memories()}\n",
    "\n",
    "                Conversation Summary: {self.conversation_summary or 'This is the start of the conversation'}\n",
    "\n",
    "                Decide which tool to use based on the conversation, conversation summary and relevant memories.\n",
    "                Reply ONLY with the tool name and argument.\n",
    "                Examples: read_file(\"main.py\") or analyze_code(\"def foo():pass\")\n",
    "\n",
    "                \"\"\"\n",
    "\n",
    "        # Build prompt with system instructions\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\":\"system\",\n",
    "                \"content\":system_message_context\n",
    "            }\n",
    "        ] + self.conversation_history\n",
    "\n",
    "        response = openai.responses.create(model=self.model, input=messages)\n",
    "\n",
    "        decision = response.output_text\n",
    "\n",
    "        # Add assistant's decision to conversation history\n",
    "        self.conversation_history.append({\n",
    "            \"role\":\"assistant\",\n",
    "            \"content\": decision\n",
    "        })\n",
    "\n",
    "        return decision\n",
    "    \n",
    "    def act(self, decision:str):\n",
    "        \"\"\"Execute the chosen tool and record the result.\"\"\"\n",
    "        try:\n",
    "            if \"(\" in decision and \")\" in decision:\n",
    "                name, arg = decision.split(\"(\",1)\n",
    "                arg = arg.strip(\")'\\\"\")\n",
    "                result = self.tools.call(name.strip(),arg)\n",
    "            else:\n",
    "                result = self.tools.call(decision)\n",
    "\n",
    "            #Store tool call result in conversation history\n",
    "            self.conversation_history.append({\n",
    "                \"role\":\"system\",\n",
    "                \"content\":f\"Tool result: {result}\"\n",
    "            })\n",
    "\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error executing tool: {e}\"\n",
    "            self.conversation_history.append({\n",
    "                \"role\":\"system\",\n",
    "                \"content\": error_msg\n",
    "            })\n",
    "            return error_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b7ba84",
   "metadata": {},
   "source": [
    "## Context Window Management and Forgetting\n",
    "Every LLM has a **context window** - a maximum number of tokens it can process at once.  \n",
    "When conversation history + long term memory + prompt + response exceed this limit, the LLM call my fail or return an incomplete response.  \n",
    "For this reason we need to manage the context window limits.\n",
    "\n",
    "### Strategies for Managing the Context Window\n",
    "1. **Token Counting**: Estimate or count tokens before sending to the LLM\n",
    "2. **Trimming**: Remove the oldest messages beyond a threshold\n",
    "3. **Selctive forgetting**: Drop less important messages\n",
    "4. **Hierarchical Summarization** Sumarize summaries for very long interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dadf961",
   "metadata": {},
   "source": [
    "### Implement Token Aware Trimming\n",
    "Below we shall look at a simple implementation of token aware trinning\n",
    "1. Token counting. We use `tiktoken` to accurately count tokens\n",
    "2. Add `trim_history_to_fit()`: Removes the oldest messages when over budget. This is called every time the agent calls `think()`\n",
    "3. Add `max_context_tokens` to configure token limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c539bcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken # OpenAI token counting library\n",
    "\n",
    "class CodeReviewAgentWithTrimming:\n",
    "    def __init__(self,tools_registry: ToolRegistry, model=\"gpt-4o-mini\",memory_file=\"agent_memory.json\",summarize_after=10,max_context_tokens=6000):\n",
    "        self.tools = tools_registry\n",
    "        self.model = model\n",
    "        self.conversation_history = [] # Short-term memory\n",
    "        self.memory_file = memory_file\n",
    "        self.load_long_term_memory() # Long-term memory (key-value store)\n",
    "        self.conversation_summary = \"\" # Summarized conversation history\n",
    "        self.summarize_after = summarize_after\n",
    "        self.turns_since_summary = 0\n",
    "        self.max_context_tokens = max_context_tokens\n",
    "\n",
    "        # Initialize tokenizer for the model\n",
    "        try:\n",
    "            self.tokenizer = tiktoken.encoding_for_model(model)\n",
    "        except:\n",
    "            self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    def count_tokens(self, text:str) -> int:\n",
    "        \"\"\"Count tokens in a string\"\"\"\n",
    "        return len(self.tokenizer.encode(text))\n",
    "    \n",
    "    def trim_history_to_fit(self, system_message:str):\n",
    "        \"\"\"Remove old messages until we fit within the token budget\"\"\"\n",
    "\n",
    "        # Count tokens in system message\n",
    "        fixed_tokens = self.count_tokens(system_message)\n",
    "\n",
    "        # Count tokens in conversation history\n",
    "        history_tokens = sum([self.count_tokens(msg[\"content\"]) for msg in self.conversation_history])\n",
    "\n",
    "        total_tokens = fixed_tokens + history_tokens\n",
    "\n",
    "        while total_tokens > self.max_context_tokens and len(self.conversation_history) > 2:\n",
    "            removed_msg = self.conversation_history.pop(0)\n",
    "            total_tokens -= self.count_tokens(removed_msg[\"content\"])\n",
    "\n",
    "        return total_tokens\n",
    "\n",
    "\n",
    "    def summarize_history(self):\n",
    "        \"\"\"Use LLM to summarize the conversation so far.\"\"\"\n",
    "        if len(self.conversation_history) < 3:\n",
    "            return\n",
    "        \n",
    "        history_text = \"\\n\".join([f\"{msg[\"role\"]}:{msg[\"content\"]}\" for msg in self.conversation_history])\n",
    "\n",
    "        summary_prompt = f\"\"\"Summarize this conversation in 3-4 sentences,\n",
    "        preserving key fact, decisions, and actions taken:\n",
    "        {history_text}\n",
    "\n",
    "        Previous Summary: {self.conversation_summary or 'None'}\n",
    "        \"\"\"\n",
    "\n",
    "        response = openai.responses.create(model=self.model, input=[{\"role\":\"user\",\"content\":summary_prompt}])\n",
    "\n",
    "        self.conversation_summary = response.output_text\n",
    "\n",
    "        # Keep only the last few turns + the summary\n",
    "        recent_turns = self.conversation_history[-4:] # Keep the last 4 messages (2 user/assistant exchanges)\n",
    "\n",
    "        self.conversation_history = recent_turns\n",
    "        self.turns_since_summary = 0\n",
    "\n",
    "\n",
    "    def remember(self, key:str, value: str):\n",
    "        \"\"\"Retrieve information from long term memory.\"\"\"\n",
    "        self.long_term_memory[key] = value\n",
    "        self.save_long_term_memory()\n",
    "    \n",
    "    def recall(self,key:str) -> str:\n",
    "        \"\"\"Retrieve information from long term memory\"\"\"\n",
    "        return self.long_term_memory.get(key,\"No memory found for this key.\")\n",
    "    \n",
    "    def get_relevant_memories(self) -> str:\n",
    "        \"\"\"Format long term memories for inclusion in prompts.\"\"\"\n",
    "        if not self.long_term_memory:\n",
    "            return \"No stored memories\"\n",
    "        \n",
    "        memories = \"\\n\".join([f\"- {k}:{v}\" for k, v in self.long_term_memory.items()])\n",
    "        return f\"Relevant memories:\\n{memories}\"\n",
    "    \n",
    "    def save_long_term_memory(self):\n",
    "        \"\"\"Persist long term memory to JSON file\"\"\"\n",
    "        try:\n",
    "            with open(self.memory_file,\"w\") as f:\n",
    "                json.dump(self.long_term_memory,f,indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save memory to {self.memory_file}:  {e}\")\n",
    "\n",
    "    def load_long_term_memory(self):\n",
    "        \"\"\"Load long term memory from JSON file\"\"\"\n",
    "        if os.path.exists(self.memory_file):\n",
    "            try:\n",
    "                with open(self.memory_file, 'r') as f:\n",
    "                    self.long_term_memory = json.load(f)\n",
    "                print(f\"Loaded {len(self.long_term_memory)} memories from {self.memory_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load memory from {self.memory_file}: {e}\")\n",
    "        else:\n",
    "            self.long_term_memory = {}\n",
    "\n",
    "    def think(self, user_input:str):\n",
    "        \"\"\"LLM decides which tool to use with both short term and long term context.\"\"\"\n",
    "        # Add user message to history\n",
    "        self.conversation_history.append({\"role\":\"user\",\"content\":user_input})\n",
    "\n",
    "        self.turns_since_summary += 1\n",
    "\n",
    "        # Check if we should summarize\n",
    "        if self.turns_since_summary >= self.summarize_after:\n",
    "            self.summarize_history()\n",
    "\n",
    "        #Include long term memory & summary in system context\n",
    "        system_message_context = f\"\"\"You are a code assistant with access to these tools:\n",
    "                - read_file(filepath)\n",
    "                - analyze_code(code)\n",
    "\n",
    "                {self.get_relevant_memories()}\n",
    "\n",
    "                Conversation Summary: {self.conversation_summary or 'This is the start of the conversation'}\n",
    "\n",
    "                Decide which tool to use based on the conversation, conversation summary and relevant memories.\n",
    "                Reply ONLY with the tool name and argument.\n",
    "                Examples: read_file(\"main.py\") or analyze_code(\"def foo():pass\")\n",
    "\n",
    "                \"\"\"\n",
    "\n",
    "        self.trim_history_to_fit(system_message_context)\n",
    "        \n",
    "        # Build prompt with system instructions\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\":\"system\",\n",
    "                \"content\":system_message_context\n",
    "            }\n",
    "        ] + self.conversation_history\n",
    "\n",
    "        response = openai.responses.create(model=self.model, input=messages)\n",
    "\n",
    "        decision = response.output_text\n",
    "\n",
    "        # Add assistant's decision to conversation history\n",
    "        self.conversation_history.append({\n",
    "            \"role\":\"assistant\",\n",
    "            \"content\": decision\n",
    "        })\n",
    "\n",
    "        return decision\n",
    "    \n",
    "    def act(self, decision:str):\n",
    "        \"\"\"Execute the chosen tool and record the result.\"\"\"\n",
    "        try:\n",
    "            if \"(\" in decision and \")\" in decision:\n",
    "                name, arg = decision.split(\"(\",1)\n",
    "                arg = arg.strip(\")'\\\"\")\n",
    "                result = self.tools.call(name.strip(),arg)\n",
    "            else:\n",
    "                result = self.tools.call(decision)\n",
    "\n",
    "            #Store tool call result in conversation history\n",
    "            self.conversation_history.append({\n",
    "                \"role\":\"system\",\n",
    "                \"content\":f\"Tool result: {result}\"\n",
    "            })\n",
    "\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error executing tool: {e}\"\n",
    "            self.conversation_history.append({\n",
    "                \"role\":\"system\",\n",
    "                \"content\": error_msg\n",
    "            })\n",
    "            return error_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bb3675",
   "metadata": {},
   "source": [
    "### What's next\n",
    "That concludes the first part of the series where we implemented the simple building blocks of AI agents.\n",
    "\n",
    "In the next part of the series we will look at more advanced patterns such as routing, planning and orchestration and multi agent workflows. \n",
    "\n",
    "We will also start to dive deeper into the practical considerations for deploying real world agents such as evaluating agents, observability, guardrails and security."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6f1075",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
