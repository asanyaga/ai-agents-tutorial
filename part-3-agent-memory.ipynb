{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01d71aa1",
   "metadata": {},
   "source": [
    "# Adding Memory to our Agent: Short-Term and Long-Term memory in practice\n",
    "\n",
    "## Introduction\n",
    "In our previous tutorials, we built a code review agent that uses tools. But there's a critical limitation:**our agent has no memory between interactions**. Every time we call `think()`, the agent starts fresh, with no knowledge of previous conversations or actions.\n",
    "\n",
    "Imagine asking the agent to \"review the last file I mentioned\" or \"compare this code to what you saw earlier\". Without memory it can't do either. In this article, we'll transform our stateless agent into one that remembers conversations, learns from interactions, and manages its memory efficiently.\n",
    "\n",
    "We will cover:\n",
    "* Why memory matters for agents\n",
    "* Short-term memory\n",
    "* Long-term memory\n",
    "* Memory summarization techniques\n",
    "* Context window management strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d08a777",
   "metadata": {},
   "source": [
    "## Why memory matters\n",
    "**Memory enables continuity**. Without it agents can't:\n",
    "* Reference previous questions or answers\n",
    "* Build on past interactions\n",
    "* Learn user preferences\n",
    "* Handle multi-turn workflows (e.g. \"read the file, then analyze it, then write tests\")\n",
    "Real world conversations have context. Our agent needs memory to maintain that context and provide intelligent, contextual responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a751ad99",
   "metadata": {},
   "source": [
    "## Short term memory: Conversation History\n",
    "**Short-term memory** stores the recent conversation between user and agent. This is the foundation of a multi-turn dialogue.\n",
    "\n",
    "### Implementation: Adding a Message Buffer\n",
    "Let's add a simple conversation history to our agent [CodeReveiwAgentWithTools](/code_review_agent_with_tools.ipynb)\n",
    "1. Inititalize a list for conversation history\n",
    "```python\n",
    "class CodeReviewAgentWithSTMemory:\n",
    "    def __init__(self,tools_registry: ToolRegistry, model=\"gpt-4o-mini\"):\n",
    "        self.tools = tools_registry\n",
    "        self.model = model\n",
    "        self.conversation_history = [] # Short-term memory\n",
    "```\n",
    "\n",
    "2. Update `think()` to add user input and LLM responses to `conversation_history` and include the conversation history in the prompt\n",
    "```python\n",
    "    def think(self, user_input:str):\n",
    "        \"\"\"LLM decides which tool to use with conversation context.\"\"\"\n",
    "        # Add user message to history\n",
    "        self.conversation_history.append({\"role\":\"user\",\"content\":user_input})\n",
    "\n",
    "        # Build prompt with system instructions\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\":\"system\",\n",
    "                \"content\":\"\"\"You are a code assistant with access to these tools:\n",
    "                - read_file(filepath)\n",
    "                - analyze_code(code)\n",
    "\n",
    "                Decide which tool to use based on the conversation.\n",
    "                If a tool call is needed Reply ONLY with the tool call to make in JSON format {{\"tool\": \"tool_name\", \"args\": [\"arg1\", \"arg2\"]}} (e.g., {{\"tool\":\"read_file\", \"args\":[\"sample.py\"]}}\n",
    "        Examples:\n",
    "        - read_file(\"main.py\")\n",
    "        - patch_file(filepath, content)\n",
    "        - print_review(review: str)\n",
    "\n",
    "        If the task is complete respond with JSON {{\"done: true, \"summary:\"The task is complete because\"}} where the summary is the reason why the task is complete    \n",
    "\n",
    "                \"\"\"\n",
    "            }\n",
    "        ] + self.conversation_history\n",
    "\n",
    "        response = openai.responses.create(model=self.model, input=messages)\n",
    "\n",
    "        decision = response.output_text\n",
    "\n",
    "        # Add assistant's decision to conversation history\n",
    "        self.conversation_history.append({\n",
    "            \"role\":\"assistant\",\n",
    "            \"content\": decision\n",
    "        })\n",
    "\n",
    "        return decision\n",
    "```\n",
    "\n",
    "3. Update `act()` to add tool call results to conversation history\n",
    "```python\n",
    "    def act(self, decision:str):\n",
    "        \"\"\"Execute the chosen tool and record the result.\"\"\"\n",
    "        try:\n",
    "            parsed = json.loads(decision)\n",
    "            tool_name = parsed[\"tool\"]\n",
    "            args = parsed.get(\"args\",[])\n",
    "\n",
    "            result = self.tools.call(tool_name,*args)\n",
    "\n",
    "            #Store tool call result in conversation history\n",
    "            self.conversation_history.append({\n",
    "                \"role\":\"system\",\n",
    "                \"content\":f\"Tool result: {result}\"\n",
    "            })\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error executing tool: {e}\"\n",
    "            self.conversation_history.append({\n",
    "                \"role\":\"system\",\n",
    "                \"content\": error_msg\n",
    "            })\n",
    "            return error_msg\n",
    "```\n",
    "### What changed\n",
    "1. **`conversation_history` list**: Stores all messages as dictionaries with `role` and `content`\n",
    "2. **Messages passed to LLM**: Instead of a single prompt string, we send the entire conversation\n",
    "3. **Tool call result stored**: After each action we append the result to history so the agent can reference it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7ebe06",
   "metadata": {},
   "source": [
    "### Tool Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c332bb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from typing import Callable, Dict\n",
    "import json\n",
    "\n",
    "\n",
    "def read_file(filepath: str) -> str:\n",
    "    \"\"\"Read contents of a Python file\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        return f\"File not found: {filepath}\"\n",
    "    \n",
    "    with open(filepath, \"r\") as f:\n",
    "        return f.read()\n",
    "    \n",
    "def patch_file(filepath: str, content: str) -> str:\n",
    "    \"\"\"Writes the given content to a file, completely replacing its current content.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, \"w\") as f:\n",
    "            f.write(content)\n",
    "        return f\"File successfully updated: {filepath}. New content written.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error writing to file {filepath}: {e}\"\n",
    "\n",
    "def print_review(review: str):\n",
    "    print(f\"Review: {review}\")\n",
    "    return f\"Printed review: {review}\"\n",
    "\n",
    "class ToolRegistry:\n",
    "    \"\"\"Holds available tools and dispatches them by name.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.tools: Dict[str,Callable] = {}\n",
    "    \n",
    "    def register(self, name:str, func: Callable):\n",
    "        self.tools[name] = func\n",
    "\n",
    "    def call(self, name:str, *args, **kwargs):\n",
    "        if name not in self.tools:\n",
    "            return f\"Unknown tool: {name}\"\n",
    "        return self.tools[name](*args, **kwargs)\n",
    "\n",
    "\n",
    "class ToolRegistry:\n",
    "    \"\"\"Holds available tools and dispatches them by name.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.tools: Dict[str,Callable] = {}\n",
    "    \n",
    "    def register(self, name:str, func: Callable):\n",
    "        self.tools[name] = func\n",
    "\n",
    "    def call(self, name:str, *args, **kwargs):\n",
    "        if name not in self.tools:\n",
    "            return f\"Unknown tool: {name}\"\n",
    "        return self.tools[name](*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c4eab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "class CodeReviewAgentWithSTMemory:\n",
    "    def __init__(self,tools_registry: ToolRegistry, model=\"gpt-4o-mini\"):\n",
    "        self.tools = tools_registry\n",
    "        self.model = model\n",
    "        self.conversation_history = [] # Short-term memory\n",
    "\n",
    "    def think(self, user_input:str):\n",
    "        \"\"\"LLM decides which tool to use with conversation context.\"\"\"\n",
    "        # Add user message to history\n",
    "        self.conversation_history.append({\"role\":\"user\",\"content\":user_input})\n",
    "\n",
    "        # Build prompt with system instructions\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\":\"system\",\n",
    "                \"content\":f\"\"\"You are a code assistant with access to the tools below.\n",
    "\n",
    "                Available tools:\n",
    "                - read_file(filepath)\n",
    "                - patch_file(filepath, content)\n",
    "                - print_review(review: str)\n",
    "\n",
    "\n",
    "                Decide which tool is most appropriate based on the conversation.\n",
    "                If a tool call is needed Reply ONLY with the tool call to make in JSON format {{\"tool\": \"tool_name\", \"args\": [\"arg1\", \"arg2\"]}} (e.g., {{\"tool\":\"read_file\", \"args\":[\"sample.py\"]}}\n",
    "                 Examples:\n",
    "                - read_file(\"main.py\")\n",
    "                - patch_file(filepath, content)\n",
    "                - print_review(review: str)\n",
    "\n",
    "                If the task is complete respond with JSON {{\"done: true, \"summary:\"The task is complete because\"}} where the summary is the reason why the task is complete\n",
    "                \"\"\"\n",
    "            }\n",
    "        ] + self.conversation_history\n",
    "\n",
    "        response = openai.responses.create(model=self.model, input=messages)\n",
    "\n",
    "        decision = response.output_text\n",
    "\n",
    "        # Add assistant's decision to conversation history\n",
    "        self.conversation_history.append({\n",
    "            \"role\":\"assistant\",\n",
    "            \"content\": decision\n",
    "        })\n",
    "\n",
    "        return decision\n",
    "    \n",
    "    def act(self, decision:str):\n",
    "        \"\"\"Execute the chosen tool and record the result.\"\"\"\n",
    "        print(f\"Acting on decision: {decision}\")\n",
    "        try:\n",
    "            parsed = json.loads(decision)\n",
    "            tool_name = parsed[\"tool\"]\n",
    "            args = parsed.get(\"args\",[])\n",
    "\n",
    "            result = self.tools.call(tool_name,*args)\n",
    "\n",
    "            #Store tool call result in conversation history\n",
    "            self.conversation_history.append({\n",
    "                \"role\":\"system\",\n",
    "                \"content\":f\"Tool result: {result}\"\n",
    "            })\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error executing tool: {e}\"\n",
    "            self.conversation_history.append({\n",
    "                \"role\":\"system\",\n",
    "                \"content\": error_msg\n",
    "            })\n",
    "            return error_msg\n",
    "        \n",
    "    def run(self, user_input: str, max_steps:int=3):\n",
    "        original_input = user_input\n",
    "        for step in range(max_steps):\n",
    "            print(f\"Step: {step+1} of {max_steps}\")\n",
    "            decision = self.think(user_input)\n",
    "            try:\n",
    "                decision_parsed = json.loads(decision)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Could not parse decision:{decision}. Error: {e}\")\n",
    "                user_input = f\"Your response was not valid JSON.\\nOriginal user request: {original_input}\"\n",
    "            \n",
    "            if decision_parsed.get(\"done\"):\n",
    "                print(f\"Task complete\\nAssistant Repose:{decision}\")\n",
    "                return decision_parsed.get(\"summary\")\n",
    "            \n",
    "            result = self.act(decision)\n",
    "            user_input = f\"Original user request: {original_input}\\nLast assistant response{decision}\\nLast tool result: {result}. continue with original user request\"\n",
    "\n",
    "        print(\"Loop complete. (max steps reached)\")\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db536ca2",
   "metadata": {},
   "source": [
    "### Let's give it a try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efff9650",
   "metadata": {},
   "outputs": [],
   "source": [
    "registry = ToolRegistry()\n",
    "registry.register(\"read_file\",read_file)\n",
    "registry.register(\"patch_file\",patch_file)\n",
    "registry.register(\"print_review\",print_review)\n",
    "\n",
    "agent_with_st_memory = CodeReviewAgentWithSTMemory(registry)\n",
    "\n",
    "# Multi-turn conversation\n",
    "result = agent_with_st_memory.run(\"Review the code in sample.py and print the review\",max_steps=5)\n",
    "\n",
    "print(f\"Agent Result: {result}\")\n",
    "\n",
    "print(f\"Agent Chat History : {agent_with_st_memory.conversation_history}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4faf0e49",
   "metadata": {},
   "source": [
    "***Key insight:** The LLM sees the full conversation each time, allowing it to understand context and references like \"that code\" or \"the last file\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937bfc17",
   "metadata": {},
   "source": [
    "## Long-term Memory: Persistent Knowledge\n",
    "Short-term memory is exists only during a session. Long term memory persists across sessions and stores important information the agent should remember \n",
    "every time it is performing a task.\n",
    "\n",
    "### Use cases for long term memory\n",
    "* **User preferences**: \"I prefer tests with pytest, not unittest\"\n",
    "* **Project context**: \"this is a fastapi web api with sqlalchemy models\"\n",
    "* **Learned patterns**: \"user often asks for sql injection vulnerabilities\"\n",
    "* **Important facts**: File paths, project structure, common issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5e11e3",
   "metadata": {},
   "source": [
    "### Implementation: Adding a long term knowledge store\n",
    "Let's add long term memory to our agent\n",
    "1. **Add** `long_term_memory` and `memory_file` which we will implement as a simple key value store persisted in a `.json` file\n",
    "```python\n",
    "class CodeReviewAgentWithLTMemory:\n",
    "    def __init__(self,tools_registry: ToolRegistry, model=\"gpt-4o-mini\",memory_file=\"agent_memory.json\"):\n",
    "        #...rest of init ..\n",
    "```\n",
    "2. **Add** `remember()` adds/updates key value in the long term memory\n",
    "```python\n",
    "    def remember(self, key:str, value: str):\n",
    "        \"\"\"Retrieve information from long term memory.\"\"\"\n",
    "        self.long_term_memory[key] = value\n",
    "        self.save_long_term_memory()\n",
    "```\n",
    "3. **Add** `recall()` retrieves a particular item from the long term memory\n",
    "```python\n",
    "    def recall(self,key:str) -> str:\n",
    "        \"\"\"Retrieve information from long term memory\"\"\"\n",
    "        return self.long_term_memory.get(key,\"No memory found for this key.\")\n",
    "```\n",
    "4. **Add** `get_relevant_memories()` gets and formats the long term memories to include in the system message\n",
    "```python\n",
    "    def get_relevant_memories(self) -> str:\n",
    "        \"\"\"Format long term memories for inclusion in prompts.\"\"\"\n",
    "        if not self.long_term_memory:\n",
    "            return \"No stored memories\"\n",
    "        \n",
    "        memories = \"\\n\".join([f\"- {k}:{v}\" for k, v in self.long_term_memory.items()])\n",
    "        return f\"Relevant memories:\\n{memories}\"\n",
    "```\n",
    "5. **Add** `save_long_term_memory()` to persist long term memory to disk. This makes sure it persists between agent sessions\n",
    "```python\n",
    "    def save_long_term_memory(self):\n",
    "        \"\"\"Persist long term memory to JSON file\"\"\"\n",
    "        try:\n",
    "            with open(self.memory_file,\"w\") as f:\n",
    "                json.dump(self.long_term_memory,f,indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save memory to {self.memory_file}:  {e}\")\n",
    "```\n",
    "6. **Add** `load_long_term_memory()` load long term memory when the agent initializes\n",
    "```python\n",
    "    def load_long_term_memory(self):\n",
    "        \"\"\"Load long term memory from JSON file\"\"\"\n",
    "        if os.path.exists(self.memory_file):\n",
    "            try:\n",
    "                with open(self.memory_file, 'r') as f:\n",
    "                    self.long_term_memory = json.load(f)\n",
    "                print(f\"Loaded {len(self.long_term_memory)} memories from {self.memory_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load memory from {self.memory_file}: {e}\")\n",
    "        else:\n",
    "            self.long_term_memory = {}\n",
    "\n",
    "```\n",
    "7. Update the prompt's system message to include the long term memory as `relevant_memories`\n",
    "```python\n",
    "    def think(self, user_input:str):\n",
    "        \"\"\"LLM decides which tool to use with both short term and long term context.\"\"\"\n",
    "        #...existing code...\n",
    "\n",
    "        #Include long term memory in system context\n",
    "        system_message_context = f\"\"\"You are a code assistant with access to these tools:\n",
    "                - read_file(filepath)\n",
    "                - analyze_code(code)\n",
    "\n",
    "                {self.get_relevant_memories()}\n",
    "\n",
    "                Decide which tool to use based on the conversation and relevant memories.\n",
    "                If a tool call is needed Reply ONLY with the tool call to make in JSON format {{\"tool\": \"tool_name\", \"args\": [\"arg1\", \"arg2\"]}} (e.g., {{\"tool\":\"read_file\", \"args\":[\"sample.py\"]}}\n",
    "        Examples:\n",
    "        - read_file(\"main.py\")\n",
    "        - patch_file(filepath, content)\n",
    "        - print_review(review: str)\n",
    "\n",
    "        If the task is complete respond with JSON {{\"done: true, \"summary:\"The task is complete because\"}} where the summary is the reason why the task is complete    \n",
    "\n",
    "                \"\"\"\n",
    "        #... existing code\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4264b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Dict, Callable\n",
    "class CodeReviewAgentWithLTMemory:\n",
    "    def __init__(self,tools_registry: ToolRegistry, model=\"gpt-4o-mini\",memory_file=\"agent_memory.json\"):\n",
    "        self.tools = tools_registry\n",
    "        self.model = model\n",
    "        self.conversation_history = [] # Short-term memory\n",
    "        self.memory_file = memory_file\n",
    "        self.load_long_term_memory() # Long-term memory (key-value store)\n",
    "\n",
    "    def remember(self, key:str, value: str):\n",
    "        \"\"\"Save information to long term memory.\"\"\"\n",
    "        self.long_term_memory[key] = value\n",
    "        self.save_long_term_memory()\n",
    "    \n",
    "    def recall(self,key:str) -> str:\n",
    "        \"\"\"Retrieve information from long term memory\"\"\"\n",
    "        return self.long_term_memory.get(key,\"No memory found for this key.\")\n",
    "    \n",
    "    def get_relevant_memories(self) -> str:\n",
    "        \"\"\"Format long term memories for inclusion in prompts.\"\"\"\n",
    "        if not self.long_term_memory:\n",
    "            return \"No stored memories\"\n",
    "        \n",
    "        memories = \"\\n\".join([f\"- {k}:{v}\" for k, v in self.long_term_memory.items()])\n",
    "        return f\"Relevant memories:\\n{memories}\"\n",
    "    \n",
    "    def save_long_term_memory(self):\n",
    "        \"\"\"Persist long term memory to JSON file\"\"\"\n",
    "        try:\n",
    "            with open(self.memory_file,\"w\") as f:\n",
    "                json.dump(self.long_term_memory,f,indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save memory to {self.memory_file}:  {e}\")\n",
    "\n",
    "    def load_long_term_memory(self):\n",
    "        \"\"\"Load long term memory from JSON file\"\"\"\n",
    "        if os.path.exists(self.memory_file):\n",
    "            try:\n",
    "                with open(self.memory_file, 'r') as f:\n",
    "                    self.long_term_memory = json.load(f)\n",
    "                print(f\"Loaded {len(self.long_term_memory)} memories from {self.memory_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load memory from {self.memory_file}: {e}\")\n",
    "        else:\n",
    "            self.long_term_memory = {}\n",
    "\n",
    "    def think(self, user_input:str):\n",
    "        \"\"\"LLM decides which tool to use with both short term and long term context.\"\"\"\n",
    "        # Add user message to history\n",
    "        self.conversation_history.append({\"role\":\"user\",\"content\":user_input})\n",
    "\n",
    "        #Include long term memory in system context\n",
    "        system_message_context = f\"\"\"You are a code assistant with access to the tools below.\n",
    "\n",
    "                Available tools:\n",
    "                - read_file(filepath)\n",
    "                - patch_file(filepath, content)\n",
    "                - print_review(review: str)\n",
    "\n",
    "\n",
    "                {self.get_relevant_memories()}\n",
    "\n",
    "       If a tool call is needed Reply ONLY with the tool call to make in JSON format {{\"tool\": \"tool_name\", \"args\": [\"arg1\", \"arg2\"]}} (e.g., {{\"tool\":\"read_file\", \"args\":[\"sample.py\"]}}\n",
    "        Examples:\n",
    "        - read_file(\"main.py\")\n",
    "        - patch_file(filepath, content)\n",
    "        - print_review(review: str)\n",
    "\n",
    "        If the task is complete respond with JSON {{\"done: true, \"summary:\"The task is complete because\"}} where the summary is the reason why the task is complete\n",
    "                \"\"\"\n",
    "\n",
    "        # Build prompt with system instructions\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\":\"system\",\n",
    "                \"content\":system_message_context\n",
    "            }\n",
    "        ] + self.conversation_history\n",
    "\n",
    "        response = openai.responses.create(model=self.model, input=messages)\n",
    "\n",
    "        decision = response.output_text\n",
    "\n",
    "        # Add assistant's decision to conversation history\n",
    "        self.conversation_history.append({\n",
    "            \"role\":\"assistant\",\n",
    "            \"content\": decision\n",
    "        })\n",
    "\n",
    "        return decision\n",
    "    \n",
    "    def act(self, decision:str):\n",
    "        \"\"\"Execute the chosen tool and record the result.\"\"\"\n",
    "        try:\n",
    "            if \"(\" in decision and \")\" in decision:\n",
    "                name, arg = decision.split(\"(\",1)\n",
    "                arg = arg.strip(\")'\\\"\")\n",
    "                result = self.tools.call(name.strip(),arg)\n",
    "            else:\n",
    "                result = self.tools.call(decision)\n",
    "\n",
    "            #Store tool call result in conversation history\n",
    "            self.conversation_history.append({\n",
    "                \"role\":\"system\",\n",
    "                \"content\":f\"Tool result: {result}\"\n",
    "            })\n",
    "\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error executing tool: {e}\"\n",
    "            self.conversation_history.append({\n",
    "                \"role\":\"system\",\n",
    "                \"content\": error_msg\n",
    "            })\n",
    "            return error_msg\n",
    "\n",
    "    def run(self, user_input: str, max_steps:int=3):\n",
    "        original_input = user_input\n",
    "        for step in range(max_steps):\n",
    "            print(f\"Step: {step+1} of {max_steps}\")\n",
    "            decision = self.think(user_input)\n",
    "            try:\n",
    "                decision_parsed = json.loads(decision)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Could not parse decision:{decision}. Error: {e}\")\n",
    "                user_input = f\"Your response was not valid JSON.\\nOriginal user request: {original_input}\"\n",
    "                \n",
    "            if decision_parsed.get(\"done\"):\n",
    "                print(f\"Task complete\\nAssistant Repose:{decision}\")\n",
    "                return decision_parsed.get(\"summary\")\n",
    "            \n",
    "            result = self.act(decision)\n",
    "            user_input = f\"Original user request: {original_input}\\nLast assistant response{decision}\\nLast tool result: {result}. continue with original user request\"\n",
    "\n",
    "        print(\"Loop complete. (max steps reached)\")\n",
    "        return result       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63451892",
   "metadata": {},
   "source": [
    "### Demo: Long term memory persists across agent sessions\n",
    "**Key insight:** Long term memory provides persistent context that informs every interaction, enabling the agent to personalize its behaviour and remember important facts across sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03d18e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-turn conversation with long term memory\n",
    "registry = ToolRegistry()\n",
    "registry.register(\"read_file\",read_file)\n",
    "registry.register(\"print_review\",print_review)\n",
    "registry.register(\"patch_file\",patch_file)\n",
    "\n",
    "agent_with_lt_memory1 = CodeReviewAgentWithLTMemory(registry)\n",
    "\n",
    "code_snippet = \"\"\"\n",
    "def divide(a,b):\n",
    "    return a/b\n",
    "\"\"\"\n",
    "\n",
    "agent_with_lt_memory1.remember(\"documentation\",\"add comprehensive documentation and doc string to ALL code generated\")\n",
    "decision1_with_ltm1 = agent_with_lt_memory1.run(f\"Review this code:{code_snippet}\")\n",
    "\n",
    "print(f\"First Agent Long Term Memory {agent_with_lt_memory1.long_term_memory}\")\n",
    "print((f\"First Agent Conversion History: {agent_with_lt_memory1.conversation_history}\"))\n",
    "\n",
    "\n",
    "# New session has long term memory\n",
    "agent_with_lt_memory2 = CodeReviewAgentWithLTMemory(registry)\n",
    "print(f\"Second Agent Long Term Memory {agent_with_lt_memory2.long_term_memory}\")\n",
    "print((f\"Second Agent Conversion History: {agent_with_lt_memory2.conversation_history}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d465f97f",
   "metadata": {},
   "source": [
    "## Memory summarization: Keeping Context Compact\n",
    "As conversations grow so does the memory footprint. A 50 turn conversation might contain thousands of tokens. Summarization compresses old conversation turns into consise summaries, preserving essential information while reducing token usgae.\n",
    "\n",
    "### When to summarize\n",
    "* After a number of conversation turns\n",
    "* When conversation history exceeds a token threshold\n",
    "* When moving to a new topic or task\n",
    "\n",
    "Let's implement a simple periodic summarization where we use an LLM to generate a summary from the conversation history and trim the conversation to the last few turns.  \n",
    "1. Add `summarize_after` parameter to agent initialization to set after how many messages to summarize\n",
    "```python\n",
    "class CodeReviewAgentWithSTMemorySummarization:\n",
    "    def __init__(self,tools_registry: ToolRegistry, model=\"gpt-4o-mini\",memory_file=\"agent_memory.json\",summarize_after=10):\n",
    "        # ...exisiting init code...\n",
    "        self.conversation_summary = \"\" # Summarized conversation history\n",
    "        self.summarize_after = summarize_after # Number of conversation turns after which to summarize\n",
    "        self.turns_since_summary = 0 # Track number of turns sinse last summary\n",
    "```\n",
    "2. Add `conversation_summary` to keep the conversation summary\n",
    "3. Add `summarize_history()`: Periodically use LLM to summarize conversation history when the `summarize_after` message limit is reached\n",
    "```python\n",
    "    def summarize_history(self):\n",
    "        \"\"\"Use LLM to summarize the conversation so far.\"\"\"\n",
    "        if len(self.conversation_history) < 3:\n",
    "            return\n",
    "        \n",
    "        history_text = \"\\n\".join([f\"{msg[\"role\"]}:{msg[\"content\"]}\" for msg in self.conversation_history])\n",
    "\n",
    "        summary_prompt = f\"\"\"Summarize this conversation in 3-4 sentences,\n",
    "        preserving key fact, decisions, and actions taken:\n",
    "        {history_text}\n",
    "\n",
    "        Previous Summary: {self.conversation_summary or 'None'}\n",
    "        \"\"\"\n",
    "\n",
    "        response = openai.responses.create(model=self.model, input=[{\"role\":\"user\",\"content\":summary_prompt}])\n",
    "\n",
    "        self.conversation_summary = response.output_text\n",
    "\n",
    "        # Keep only the last few turns + the summary\n",
    "        recent_turns = self.conversation_history[-4:] # Keep the last 4 messages (2 user/assistant exchanges)\n",
    "\n",
    "        self.conversation_history = recent_turns\n",
    "        self.turns_since_summary = 0\n",
    "\n",
    "```\n",
    "4. Include the `conversation_summary` in the system prompt\n",
    "```python\n",
    "    def think(self, user_input:str):\n",
    "        \"\"\"LLM decides which tool to use with both short term and long term context.\"\"\"\n",
    "        # Add user message to history\n",
    "        self.conversation_history.append({\"role\":\"user\",\"content\":user_input})\n",
    "\n",
    "        self.turns_since_summary += 1\n",
    "\n",
    "        # Check if we should summarize\n",
    "        if self.turns_since_summary >= self.summarize_after:\n",
    "            self.summarize_history()\n",
    "\n",
    "        #Include long term memory & summary in system context\n",
    "        system_message_context = f\"\"\"You are a code assistant with access to these tools:\n",
    "                - read_file(filepath)\n",
    "                - analyze_code(code)\n",
    "\n",
    "                {self.get_relevant_memories()}\n",
    "\n",
    "                Conversation Summary: {self.conversation_summary or 'This is the start of the conversation'}\n",
    "\n",
    "                Decide which tool to use based on the conversation, conversation summary and relevant memories.\n",
    "                If a tool call is needed Reply ONLY with the tool call to make in JSON format {{\"tool\": \"tool_name\", \"args\": [\"arg1\", \"arg2\"]}} (e.g., {{\"tool\":\"read_file\", \"args\":[\"sample.py\"]}}\n",
    "                 Examples:\n",
    "                 - read_file(\"main.py\")\n",
    "                 - patch_file(filepath, content)\n",
    "                 - print_review(review: str)\n",
    "\n",
    "                If the task is complete respond with JSON {{\"done: true, \"summary:\"The task is complete because\"}} where the summary is the reason why the task is complete               \n",
    "\n",
    "                \"\"\"\n",
    "        # ...rest of think code...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b56ae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeReviewAgentWithSTMemorySummarization:\n",
    "    def __init__(self,tools_registry: ToolRegistry, model=\"gpt-4o-mini\",memory_file=\"agent_memory.json\",summarize_after=10):\n",
    "        self.tools = tools_registry\n",
    "        self.model = model\n",
    "        self.conversation_history = [] # Short-term memory\n",
    "        self.memory_file = memory_file\n",
    "        self.load_long_term_memory() # Long-term memory (key-value store)\n",
    "        self.conversation_summary = \"\" # Summarized conversation history\n",
    "        self.summarize_after = summarize_after # Number of conversation turns after which to summarize\n",
    "        self.turns_since_summary = 0 # Track number of turns sinse last summary\n",
    "\n",
    "    \n",
    "    def summarize_history(self):\n",
    "        \"\"\"Use LLM to summarize the conversation so far.\"\"\"\n",
    "        if len(self.conversation_history) < 3:\n",
    "            return\n",
    "        \n",
    "        history_text = \"\\n\".join([f\"{msg[\"role\"]}:{msg[\"content\"]}\" for msg in self.conversation_history])\n",
    "\n",
    "        summary_prompt = f\"\"\"Summarize this conversation in 3-4 sentences,\n",
    "        preserving key fact, decisions, and actions taken:\n",
    "        {history_text}\n",
    "\n",
    "        Previous Summary: {self.conversation_summary or 'None'}\n",
    "        \"\"\"\n",
    "\n",
    "        response = openai.responses.create(model=self.model, input=[{\"role\":\"user\",\"content\":summary_prompt}])\n",
    "\n",
    "        self.conversation_summary = response.output_text\n",
    "\n",
    "        # Keep only the last few turns + the summary\n",
    "        recent_turns = self.conversation_history[-4:] # Keep the last 4 messages (2 user/assistant exchanges)\n",
    "\n",
    "        self.conversation_history = recent_turns\n",
    "        self.turns_since_summary = 0\n",
    "\n",
    "\n",
    "    def remember(self, key:str, value: str):\n",
    "        \"\"\"Retrieve information from long term memory.\"\"\"\n",
    "        self.long_term_memory[key] = value\n",
    "        self.save_long_term_memory()\n",
    "    \n",
    "    def recall(self,key:str) -> str:\n",
    "        \"\"\"Retrieve information from long term memory\"\"\"\n",
    "        return self.long_term_memory.get(key,\"No memory found for this key.\")\n",
    "    \n",
    "    def get_relevant_memories(self) -> str:\n",
    "        \"\"\"Format long term memories for inclusion in prompts.\"\"\"\n",
    "        if not self.long_term_memory:\n",
    "            return \"No stored memories\"\n",
    "        \n",
    "        memories = \"\\n\".join([f\"- {k}:{v}\" for k, v in self.long_term_memory.items()])\n",
    "        return f\"Relevant memories:\\n{memories}\"\n",
    "    \n",
    "    def save_long_term_memory(self):\n",
    "        \"\"\"Persist long term memory to JSON file\"\"\"\n",
    "        try:\n",
    "            with open(self.memory_file,\"w\") as f:\n",
    "                json.dump(self.long_term_memory,f,indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save memory to {self.memory_file}:  {e}\")\n",
    "\n",
    "    def load_long_term_memory(self):\n",
    "        \"\"\"Load long term memory from JSON file\"\"\"\n",
    "        if os.path.exists(self.memory_file):\n",
    "            try:\n",
    "                with open(self.memory_file, 'r') as f:\n",
    "                    self.long_term_memory = json.load(f)\n",
    "                print(f\"Loaded {len(self.long_term_memory)} memories from {self.memory_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load memory from {self.memory_file}: {e}\")\n",
    "        else:\n",
    "            self.long_term_memory = {}\n",
    "\n",
    "    def think(self, user_input:str):\n",
    "        \"\"\"LLM decides which tool to use with both short term and long term context.\"\"\"\n",
    "        # Add user message to history\n",
    "        self.conversation_history.append({\"role\":\"user\",\"content\":user_input})\n",
    "\n",
    "        self.turns_since_summary += 1\n",
    "\n",
    "        # Check if we should summarize\n",
    "        if self.turns_since_summary >= self.summarize_after:\n",
    "            self.summarize_history()\n",
    "\n",
    "        #Include long term memory & summary in system context\n",
    "        system_message_context = f\"\"\"You are a code assistant with access to the tools below.\n",
    "\n",
    "                Available tools:\n",
    "                - read_file(filepath)\n",
    "                - patch_file(filepath, content)\n",
    "                - print_review(review: str)\n",
    "\n",
    "\n",
    "                {self.get_relevant_memories()}\n",
    "        Conversation Summary: {self.conversation_summary or 'This is the start of the conversation'}\n",
    "        Decide which tool to use based on the conversation, conversation summary and relevant memories.\n",
    "        If a tool call is needed Reply ONLY with the tool call to make in JSON format {{\"tool\": \"tool_name\", \"args\": [\"arg1\", \"arg2\"]}} (e.g., {{\"tool\":\"read_file\", \"args\":[\"sample.py\"]}}\n",
    "        Examples:\n",
    "        - read_file(\"main.py\")\n",
    "        - patch_file(filepath, content)\n",
    "        - print_review(review: str)\n",
    "\n",
    "        If the task is complete respond with JSON {{\"done: true, \"summary:\"The task is complete because\"}} where the summary is the reason why the task is complete\n",
    "                \"\"\"\n",
    "        # Build prompt with system instructions\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\":\"system\",\n",
    "                \"content\":system_message_context\n",
    "            }\n",
    "        ] + self.conversation_history\n",
    "\n",
    "        response = openai.responses.create(model=self.model, input=messages)\n",
    "\n",
    "        decision = response.output_text\n",
    "\n",
    "        # Add assistant's decision to conversation history\n",
    "        self.conversation_history.append({\n",
    "            \"role\":\"assistant\",\n",
    "            \"content\": decision\n",
    "        })\n",
    "\n",
    "        return decision\n",
    "    \n",
    "    def act(self, decision:str):\n",
    "        \"\"\"Execute the chosen tool and record the result.\"\"\"\n",
    "        try:\n",
    "            if \"(\" in decision and \")\" in decision:\n",
    "                name, arg = decision.split(\"(\",1)\n",
    "                arg = arg.strip(\")'\\\"\")\n",
    "                result = self.tools.call(name.strip(),arg)\n",
    "            else:\n",
    "                result = self.tools.call(decision)\n",
    "\n",
    "            #Store tool call result in conversation history\n",
    "            self.conversation_history.append({\n",
    "                \"role\":\"system\",\n",
    "                \"content\":f\"Tool result: {result}\"\n",
    "            })\n",
    "\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error executing tool: {e}\"\n",
    "            self.conversation_history.append({\n",
    "                \"role\":\"system\",\n",
    "                \"content\": error_msg\n",
    "            })\n",
    "            return error_msg\n",
    "        \n",
    "    def run(self, user_input: str, max_steps:int=3):\n",
    "        original_input = user_input\n",
    "        for step in range(max_steps):\n",
    "            print(f\"Step: {step+1} of {max_steps}\")\n",
    "            decision = self.think(user_input)\n",
    "            try:\n",
    "                decision_parsed = json.loads(decision)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Could not parse decision:{decision}. Error: {e}\")\n",
    "                user_input = f\"Your response was not valid JSON.\\nOriginal user request: {original_input}\"\n",
    "            \n",
    "            if decision_parsed.get(\"done\"):\n",
    "                print(f\"Task complete\\nAssistant Repose:{decision}\")\n",
    "                return decision_parsed.get(\"summary\")\n",
    "            \n",
    "            result = self.act(decision)\n",
    "            user_input = f\"Original user request: {original_input}\\nLast assistant response{decision}\\nLast tool result: {result}. continue with original user request\"\n",
    "\n",
    "        print(\"Loop complete. (max steps reached)\")\n",
    "        return result       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b7ba84",
   "metadata": {},
   "source": [
    "## Context Window Management\n",
    "Every LLM has a **context window** - a maximum number of tokens it can process at once.  \n",
    "When conversation history + long term memory + prompt + response exceed this limit, the LLM call my fail or return an incomplete response.  \n",
    "For this reason we need to manage the context window limits.\n",
    "\n",
    "### Strategies for Managing the Context Window\n",
    "1. **Token Counting**: Estimate or count tokens before sending to the LLM\n",
    "2. **Trimming**: Remove the oldest messages beyond a threshold\n",
    "3. **Selctive forgetting**: Drop less important messages\n",
    "4. **Hierarchical Summarization** Sumarize summaries for very long interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dadf961",
   "metadata": {},
   "source": [
    "### Implement Token Aware Trimming\n",
    "Below is a simple implementation of token aware trimming\n",
    "\n",
    "1. Token counting. We use `tiktoken` to accurately count tokens\n",
    "```python\n",
    "import tiktoken # OpenAI token counting library\n",
    "```\n",
    "2. Add `trim_history_to_fit()`: Removes the oldest messages when over budget. This is called every time the agent calls `think()`\n",
    "```python\n",
    "    def trim_history_to_fit(self, system_message:str):\n",
    "        \"\"\"Remove old messages until we fit within the token budget\"\"\"\n",
    "\n",
    "        # Count tokens in system message\n",
    "        fixed_tokens = self.count_tokens(system_message)\n",
    "\n",
    "        # Count tokens in conversation history\n",
    "        history_tokens = sum([self.count_tokens(msg[\"content\"]) for msg in self.conversation_history])\n",
    "\n",
    "        total_tokens = fixed_tokens + history_tokens\n",
    "\n",
    "        while total_tokens > self.max_context_tokens and len(self.conversation_history) > 2:\n",
    "            removed_msg = self.conversation_history.pop(0)\n",
    "            total_tokens -= self.count_tokens(removed_msg[\"content\"])\n",
    "\n",
    "        return total_tokens\n",
    "```\n",
    "3. Update `think()` to trim history\n",
    "```python\n",
    "    def think(self, user_input:str):\n",
    "        \"\"\"LLM decides which tool to use with both short term and long term context.\"\"\"\n",
    "        # Add user message to history\n",
    "        self.conversation_history.append({\"role\":\"user\",\"content\":user_input})\n",
    "\n",
    "        self.turns_since_summary += 1\n",
    "\n",
    "        # Check if we should summarize\n",
    "        if self.turns_since_summary >= self.summarize_after:\n",
    "            self.summarize_history()\n",
    "\n",
    "        #Include long term memory & summary in system context\n",
    "        system_message_context = f\"\"\"You are a code assistant with access to the tools below.\n",
    "\n",
    "                Available tools:\n",
    "                - read_file(filepath)\n",
    "                - patch_file(filepath, content)\n",
    "                - print_review(review: str)\n",
    "\n",
    "                {self.get_relevant_memories()}\n",
    "\n",
    "                Conversation Summary: {self.conversation_summary or 'This is the start of the conversation'}\n",
    "\n",
    "                Decide which tool to use based on the conversation, conversation summary and relevant memories.\n",
    "                If a tool call is needed Reply ONLY with the tool call to make in JSON format {{\"tool\": \"tool_name\", \"args\": [\"arg1\", \"arg2\"]}} (e.g., {{\"tool\":\"read_file\", \"args\":[\"sample.py\"]}}\n",
    "                Examples:\n",
    "                - read_file(\"main.py\")\n",
    "                - patch_file(filepath, content)\n",
    "                - print_review(review: str)\n",
    "\n",
    "                If the task is complete respond with JSON {{\"done: true, \"summary:\"The task is complete because\"}} where the summary is the reason why the task is complete    \n",
    "                \"\"\"\n",
    "\n",
    "        self.trim_history_to_fit(system_message_context)\n",
    "\n",
    "        #...existing think code...\n",
    "```\n",
    "3. Add `max_context_tokens` to configure token limits\n",
    "```python\n",
    "class CodeReviewAgentWithTrimming:\n",
    "    def __init__(self,tools_registry: ToolRegistry, model=\"gpt-4o-mini\",memory_file=\"agent_memory.json\",summarize_after=10,max_context_tokens=6000):\n",
    "        # ...existing init code...\n",
    "        self.max_context_tokens = max_context_tokens\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b657174",
   "metadata": {},
   "source": [
    "## Code review agent with memory and context management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c539bcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken # OpenAI token counting library\n",
    "\n",
    "class CodeReviewAgentWithContext:\n",
    "    def __init__(self,tools_registry: ToolRegistry, model=\"gpt-4o-mini\",memory_file=\"agent_memory.json\",summarize_after=10,max_context_tokens=6000):\n",
    "        self.tools = tools_registry\n",
    "        self.model = model\n",
    "        self.conversation_history = [] # Short-term memory\n",
    "        self.memory_file = memory_file\n",
    "        self.load_long_term_memory() # Long-term memory (key-value store)\n",
    "        self.conversation_summary = \"\" # Summarized conversation history\n",
    "        self.summarize_after = summarize_after\n",
    "        self.turns_since_summary = 0\n",
    "        self.max_context_tokens = max_context_tokens\n",
    "\n",
    "        # Initialize tokenizer for the model\n",
    "        try:\n",
    "            self.tokenizer = tiktoken.encoding_for_model(model)\n",
    "        except:\n",
    "            self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    def count_tokens(self, text:str) -> int:\n",
    "        \"\"\"Count tokens in a string\"\"\"\n",
    "        return len(self.tokenizer.encode(text))\n",
    "    \n",
    "    def trim_history_to_fit(self, system_message:str):\n",
    "        \"\"\"Remove old messages until we fit within the token budget\"\"\"\n",
    "\n",
    "        # Count tokens in system message\n",
    "        fixed_tokens = self.count_tokens(system_message)\n",
    "\n",
    "        # Count tokens in conversation history\n",
    "        history_tokens = sum([self.count_tokens(msg[\"content\"]) for msg in self.conversation_history])\n",
    "\n",
    "        total_tokens = fixed_tokens + history_tokens\n",
    "\n",
    "        while total_tokens > self.max_context_tokens and len(self.conversation_history) > 2:\n",
    "            removed_msg = self.conversation_history.pop(0)\n",
    "            total_tokens -= self.count_tokens(removed_msg[\"content\"])\n",
    "\n",
    "        return total_tokens\n",
    "\n",
    "    def summarize_history(self):\n",
    "        \"\"\"Use LLM to summarize the conversation so far.\"\"\"\n",
    "        if len(self.conversation_history) < 3:\n",
    "            return\n",
    "        \n",
    "        history_text = \"\\n\".join([f\"{msg[\"role\"]}:{msg[\"content\"]}\" for msg in self.conversation_history])\n",
    "\n",
    "        summary_prompt = f\"\"\"Summarize this conversation in 3-4 sentences,\n",
    "        preserving key fact, decisions, and actions taken:\n",
    "        {history_text}\n",
    "\n",
    "        Previous Summary: {self.conversation_summary or 'None'}\n",
    "        \"\"\"\n",
    "\n",
    "        response = openai.responses.create(model=self.model, input=[{\"role\":\"user\",\"content\":summary_prompt}])\n",
    "\n",
    "        self.conversation_summary = response.output_text\n",
    "\n",
    "        # Keep only the last few turns + the summary\n",
    "        recent_turns = self.conversation_history[-4:] # Keep the last 4 messages (2 user/assistant exchanges)\n",
    "\n",
    "        self.conversation_history = recent_turns\n",
    "        self.turns_since_summary = 0\n",
    "\n",
    "\n",
    "    def remember(self, key:str, value: str):\n",
    "        \"\"\"Retrieve information from long term memory.\"\"\"\n",
    "        self.long_term_memory[key] = value\n",
    "        self.save_long_term_memory()\n",
    "    \n",
    "    def recall(self,key:str) -> str:\n",
    "        \"\"\"Retrieve information from long term memory\"\"\"\n",
    "        return self.long_term_memory.get(key,\"No memory found for this key.\")\n",
    "    \n",
    "    def get_relevant_memories(self) -> str:\n",
    "        \"\"\"Format long term memories for inclusion in prompts.\"\"\"\n",
    "        if not self.long_term_memory:\n",
    "            return \"No stored memories\"\n",
    "        \n",
    "        memories = \"\\n\".join([f\"- {k}:{v}\" for k, v in self.long_term_memory.items()])\n",
    "        return f\"Relevant memories:\\n{memories}\"\n",
    "    \n",
    "    def save_long_term_memory(self):\n",
    "        \"\"\"Persist long term memory to JSON file\"\"\"\n",
    "        try:\n",
    "            with open(self.memory_file,\"w\") as f:\n",
    "                json.dump(self.long_term_memory,f,indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save memory to {self.memory_file}:  {e}\")\n",
    "\n",
    "    def load_long_term_memory(self):\n",
    "        \"\"\"Load long term memory from JSON file\"\"\"\n",
    "        if os.path.exists(self.memory_file):\n",
    "            try:\n",
    "                with open(self.memory_file, 'r') as f:\n",
    "                    self.long_term_memory = json.load(f)\n",
    "                print(f\"Loaded {len(self.long_term_memory)} memories from {self.memory_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load memory from {self.memory_file}: {e}\")\n",
    "        else:\n",
    "            self.long_term_memory = {}\n",
    "\n",
    "    def think(self, user_input:str):\n",
    "        \"\"\"LLM decides which tool to use with both short term and long term context.\"\"\"\n",
    "        # Add user message to history\n",
    "        self.conversation_history.append({\"role\":\"user\",\"content\":user_input})\n",
    "\n",
    "        self.turns_since_summary += 1\n",
    "\n",
    "        # Check if we should summarize\n",
    "        if self.turns_since_summary >= self.summarize_after:\n",
    "            self.summarize_history()\n",
    "\n",
    "        #Include long term memory & summary in system context\n",
    "        system_message_context = f\"\"\"You are a code assistant with access to the tools below.\n",
    "\n",
    "                Available tools:\n",
    "                - read_file(filepath)\n",
    "                - patch_file(filepath, content)\n",
    "                - print_review(review: str)\n",
    "\n",
    "                {self.get_relevant_memories()}\n",
    "\n",
    "                Conversation Summary: {self.conversation_summary or 'This is the start of the conversation'}\n",
    "\n",
    "                Decide which tool to use based on the conversation, conversation summary and relevant memories.\n",
    "                If a tool call is needed Reply ONLY with the tool call to make in JSON format {{\"tool\": \"tool_name\", \"args\": [\"arg1\", \"arg2\"]}} (e.g., {{\"tool\":\"read_file\", \"args\":[\"sample.py\"]}}\n",
    "                Examples:\n",
    "                - read_file(\"main.py\")\n",
    "                - patch_file(filepath, content)\n",
    "                - print_review(review: str)\n",
    "\n",
    "                If the task is complete respond with JSON {{\"done: true, \"summary:\"The task is complete because\"}} where the summary is the reason why the task is complete    \n",
    "                \"\"\"\n",
    "\n",
    "        self.trim_history_to_fit(system_message_context)\n",
    "        \n",
    "        # Build prompt with system instructions\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\":\"system\",\n",
    "                \"content\":system_message_context\n",
    "            }\n",
    "        ] + self.conversation_history\n",
    "\n",
    "        response = openai.responses.create(model=self.model, input=messages)\n",
    "\n",
    "        decision = response.output_text\n",
    "\n",
    "        # Add assistant's decision to conversation history\n",
    "        self.conversation_history.append({\n",
    "            \"role\":\"assistant\",\n",
    "            \"content\": decision\n",
    "        })\n",
    "\n",
    "        return decision\n",
    "    \n",
    "    def act(self, decision:str):\n",
    "        \"\"\"Execute the chosen tool and record the result.\"\"\"\n",
    "        try:\n",
    "            if \"(\" in decision and \")\" in decision:\n",
    "                name, arg = decision.split(\"(\",1)\n",
    "                arg = arg.strip(\")'\\\"\")\n",
    "                result = self.tools.call(name.strip(),arg)\n",
    "            else:\n",
    "                result = self.tools.call(decision)\n",
    "\n",
    "            #Store tool call result in conversation history\n",
    "            self.conversation_history.append({\n",
    "                \"role\":\"system\",\n",
    "                \"content\":f\"Tool result: {result}\"\n",
    "            })\n",
    "\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error executing tool: {e}\"\n",
    "            self.conversation_history.append({\n",
    "                \"role\":\"system\",\n",
    "                \"content\": error_msg\n",
    "            })\n",
    "            return error_msg\n",
    "        \n",
    "    def run(self, user_input: str, max_steps:int=3):\n",
    "        original_input = user_input\n",
    "        for step in range(max_steps):\n",
    "            print(f\"Step: {step+1} of {max_steps}\")\n",
    "            decision = self.think(user_input)\n",
    "            try:\n",
    "                decision_parsed = json.loads(decision)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Could not parse decision:{decision}. Error: {e}\")\n",
    "                user_input = f\"Your response was not valid JSON.\\nOriginal user request: {original_input}\"\n",
    "            \n",
    "            if decision_parsed.get(\"done\"):\n",
    "                print(f\"Task complete\\nAssistant Repose:{decision}\")\n",
    "                return decision_parsed.get(\"summary\")\n",
    "            \n",
    "            result = self.act(decision)\n",
    "            user_input = f\"Original user request: {original_input}\\nLast assistant response{decision}\\nLast tool result: {result}. continue with original user request\"\n",
    "\n",
    "        print(\"Loop complete. (max steps reached)\")\n",
    "        return result       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70cbc8a",
   "metadata": {},
   "source": [
    "### Notes on Memory\n",
    "* We have shown storing long term memory and retrieving all of it. In practice, with large memory sizes, it may be more efficient to store in a e.g. a vector store or database and use retrieval based on user input to fetch long term memory that is relevant to the agent task.\n",
    "* In our example we showed conversation history as lasting only for the session. It may be useful for later reference to also persist chat history. This stored conversation history would not be considered part of the agent's long term memory to be used during task sessions.\n",
    "* **Context Engineering** In this tutorial we have shown context management only in relation to managing context window size. However, context window size is not the only reason we need to manage context. *Context engineering* is the strategies we use to decide what information our agent needs to do its job well.  \n",
    "Even with today's large context windows, throwing everything in is not always the best approach.  \n",
    "Irrelevant or poorly organized context can confuse the model, slow things down, and drive up costs. We'll dive deeper into context engineering strategies in a future tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bb3675",
   "metadata": {},
   "source": [
    "## What's next\n",
    "In the next part of the series we will look at more advanced patterns such as reasoning, planning and multi agent workflows. \n",
    "\n",
    "We will also start to dive deeper into the practical considerations for deploying real world agents such as observability, evaluating agents, guardrails and security."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
