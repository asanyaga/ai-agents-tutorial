{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bce9d724",
   "metadata": {},
   "source": [
    "# Planning and Task Decomposition in AI Agents\n",
    "In the previous tutorial we built a code review agent that uses the ReAct pattern to reason about tasks, call tools and manage memory. However, our agent still handles tasks linearly; it takes one step at a time without a comprehensive plan for complex, multi-step workflows.\n",
    "\n",
    "In this tutorial we will add **planning and task decomposition** capabilities to our agent.\n",
    "\n",
    "We will teach it to;\n",
    "1. Break down complex tasks into smaller sub tasks\n",
    "2. Create and follow execution plans\n",
    "3. Track progress through multi step workflows\n",
    "4. Adapt plans based on intermediate results\n",
    "\n",
    "We will demostrate these concepts by adding testing capabilities to our code review agent, which will require multi step coordination.\n",
    "\n",
    "Consider this request *Review the code, write tests for it, write tests for it, run and verify the tests*  \n",
    "This requires:\n",
    "* Reading the file\n",
    "* Analyzing and fixing bugs\n",
    "* Writing appropriate test cases\n",
    "* Running tests to verify the fix\n",
    "\n",
    "Our current agent could handle this, but might lose track of what's been done or miss steps. With explicit planning, we can ensure systematic execution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b78443e",
   "metadata": {},
   "source": [
    "### Adding Testing Tools\n",
    "We will be making changes to this code [CodeReviewAgentReAct](https://github.com/asanyaga/ai-agents-tutorial/blob/main/code-review-agent-ReAct.ipynb)\n",
    "\n",
    "First, let's add two new tools that will enable our testing workflow\n",
    "```python\n",
    "def write_test(file_path:str, test_code: str) -> str:\n",
    "    \"\"\"Write test code to a test file\"\"\"\n",
    "    try:\n",
    "        test_dir = os.path.dirname(file_path) or \"tests\"\n",
    "        if not os.path.exists(test_dir):\n",
    "            os.makedirs(test_dir)\n",
    "\n",
    "        with open(file_path, \"w\") as f:\n",
    "            f.write(test_code)\n",
    "        return f\"Test file created: {file_path}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error writing test file {file_path: {e}}\"\n",
    "\n",
    "def run_test(file_path: str) -> str:\n",
    "    \"\"\"Run a Python test file and return results\"\"\"\n",
    "    try:\n",
    "        import subprocess\n",
    "        result = subprocess.run(\n",
    "            [\"python\",\"-m\",\"pytest\", file_path,\"-v\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=30\n",
    "        )\n",
    "        return f\"Exit code {result.returncode}\\n\\nOuput:\\n{result.stdout}\\n\\nErrors:\\n{result.stderr}\"\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return \"Test execution timed out after 30 seconds\"\n",
    "    except Exception as e:\n",
    "        return f\"Error running tests: {e}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a8258fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict\n",
    "import openai\n",
    "import os\n",
    "\n",
    "## Set up the tools and tools registry\n",
    "def write_test(file_path:str, test_code: str) -> str:\n",
    "    \"\"\"Write test code to a test file\"\"\"\n",
    "    try:\n",
    "        test_dir = os.path.dirname(file_path) or \"tests\"\n",
    "        if not os.path.exists(test_dir):\n",
    "            os.makedirs(test_dir)\n",
    "\n",
    "        with open(file_path, \"w\") as f:\n",
    "            f.write(test_code)\n",
    "        return f\"Test file created: {file_path}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error writing test file {file_path: {e}}\"\n",
    "\n",
    "def run_test(file_path: str) -> str:\n",
    "    \"\"\"Run a Python test file and return results\"\"\"\n",
    "    try:\n",
    "        import subprocess\n",
    "        result = subprocess.run(\n",
    "            [\"python\",\"-m\",\"pytest\", file_path,\"-v\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=30\n",
    "        )\n",
    "        return f\"Exit code {result.returncode}\\n\\nOuput:\\n{result.stdout}\\n\\nErrors:\\n{result.stderr}\"\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return \"Test execution timed out after 30 seconds\"\n",
    "    except Exception as e:\n",
    "        return f\"Error running tests: {e}\"\n",
    "\n",
    "def read_file(file_path: str) -> str:\n",
    "    \"\"\"Read contents of a Python file\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        return f\"File not found: {file_path}\"\n",
    "    \n",
    "    with open(file_path, \"r\") as f:\n",
    "        return f.read()\n",
    "    \n",
    "def print_review(review: str):\n",
    "    print(f\"Review: {review}\")\n",
    "    return f\"Printed review: {review}\"\n",
    "\n",
    "def patch_file(filepath: str, content: str) -> str:\n",
    "    \"\"\"Writes the given content to a file, completely replacing its current content.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, \"w\") as f:\n",
    "            f.write(content)\n",
    "        return f\"File successfully updated: {filepath}. New content written.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error writing to file {filepath}: {e}\"\n",
    "        \n",
    "class ToolRegistry:\n",
    "    \"\"\"Holds available tools and dispatches them by name.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.tools: Dict[str,Callable] = {}\n",
    "    \n",
    "    def register(self, name:str, func: Callable):\n",
    "        self.tools[name] = func\n",
    "\n",
    "    def call(self, name:str, *args, **kwargs):\n",
    "        if name not in self.tools:\n",
    "            return f\"Unknown tool: {name}\"\n",
    "        return self.tools[name](*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bfbf5d",
   "metadata": {},
   "source": [
    "### Implement the planning agent\n",
    "\n",
    "1. **Add create_plan**\n",
    "This method asks the LLM to break down the user's request into discrete, actionable steps:\n",
    "```python\n",
    "    def create_plan(self, user_query:str) -> list:\n",
    "        \"\"\"Generate a step by step plan for the user's request\"\"\"\n",
    "        planning_prompt = f\"\"\"\n",
    "        Given this task:\"\"{user_query}\"\"\n",
    "        Create a detailed execution plan with numbered steps. Each step should be a specific action\n",
    "\n",
    "        Available tools:\n",
    "        - read_file(file): Read a file's contents\n",
    "        - analyze_code(code): Get code analysis and suggestions\n",
    "        - patch_file(file_path, content): Update a file\n",
    "        - write_test(file_path, text_code): Create a test file\n",
    "        - run_test(file_path): Execute tests\n",
    "\n",
    "        Format your response as a JSON list of steps\n",
    "        [\n",
    "        {{\"step\":1,\"action\":\"description\",\"tool\":\"tool_name\"}},\n",
    "        {{\"step\":1,\"action\":\"description\",\"tool\":\"tool_name\"}}\n",
    "        ]\n",
    "\n",
    "        Only include necessary steps. Be specific about which files to work with.\n",
    "        \"\"\"\n",
    "\n",
    "        resposnse = openai.responses.create(model=self.model,\n",
    "                                            input=[{\"role\":\"user\",\"content\":planning_prompt}])\n",
    "        \n",
    "        try:\n",
    "            plan = json.loads(resposnse.output_text)\n",
    "            self.current_plan = plan\n",
    "            self.plan_created = True\n",
    "            return plan\n",
    "        except json.JSONDecodeError:\n",
    "            self.current_plan = [{\"step\":1,\"action\":\"Proceed step by step\",\"tool\":\"read_file\"}]\n",
    "            self.plan_created= True\n",
    "            return self.current_plan\n",
    "```\n",
    "2. The '_build_lan_context()' method\n",
    "This method formats the current plan state for inclusion in the system prompt:\n",
    "```python\n",
    "    def _build_plan_context(self,next_step) -> str:\n",
    "    \"\"\"Format plan information for the prompt\"\"\"\n",
    "    completed = \"\\n\".join([f\"Step {step[\"step\"]}:{step[\"action\"]}\" for step in self.completed_steps])\n",
    "\n",
    "    if next_step:\n",
    "        current = f\"\\nCURRENT: Step {next_step[\"step\"]}: {next_step[\"action\"]}\"\n",
    "    else:\n",
    "        current = \"\\n All steps completed\"\n",
    "    \n",
    "    remaining = \"\\n\".join([f\" Step {step[\"step\"]}: {step[\"action\"]}\" for step in self.current_plan[len(self.completed_steps)+1:]])\n",
    "\n",
    "    execution_plan = f\"\"\"\n",
    "    Completed:\n",
    "    {completed if completed else \"None\"}\n",
    "    {current}\n",
    "    Remaining:\n",
    "    {remaining if remaining else \"None\"}\n",
    "    \"\"\"\n",
    "\n",
    "    return execution_plan\n",
    "```\n",
    "3. **Updated** ```build_system_prompt()```\n",
    "The system prompt now includes plan context and the additional testing tools:\n",
    "```python\n",
    "def build_system_prompt(self, plan_context: str = \"\") -> str:\n",
    "    \"\"\"Construct the ReAct system prompt with current context and plan.\"\"\"\n",
    "    return f\"\"\"You are a code review assistant using the ReAct pattern with planning.\n",
    "\n",
    "## Available Tools\n",
    "- read_file(filepath): Read contents of a file\n",
    "- patch_file(filepath, content): Replace file contents entirely\n",
    "- write_test(file_path, test_code): Create a test file\n",
    "- run_test(file_path): Execute tests and return results\n",
    "\n",
    "## Context\n",
    "{self.get_relevant_memories()}\n",
    "\n",
    "Conversation summary: {self.conversation_summary or 'This is the start of the conversation.'}\n",
    "\n",
    "{plan_context}\n",
    "\n",
    "## Response Format\n",
    "\n",
    "You MUST respond with valid JSON in one of these two formats:\n",
    "\n",
    "### Format 1: When you need to use a tool\n",
    "{{\n",
    "    \"thought\": \"Your reasoning about what to do and why\",\n",
    "    \"action\": {{\n",
    "        \"tool\": \"tool_name\",\n",
    "        \"args\": [\"arg1\", \"arg2\"]\n",
    "    }}\n",
    "}}\n",
    "\n",
    "### Format 2: When the task is complete\n",
    "{{\n",
    "    \"thought\": \"Your reasoning about why the task is complete\",\n",
    "    \"answer\": \"Your final response to the user\"\n",
    "}}\n",
    "\n",
    "## Rules\n",
    "1. Always include \"thought\" explaining your reasoning\n",
    "2. Include \"action\" when you need to call a tool\n",
    "3. Include \"answer\" only when ALL plan steps are complete\n",
    "4. Never include both \"action\" and \"answer\"\n",
    "5. Respond with ONLY valid JSON—no markdown, no extra text\n",
    "6. Follow the execution plan systematically\n",
    "7. After each successful action, the system will mark that step complete\n",
    "\n",
    "## Example\n",
    "\n",
    "User: Review auth.py and fix any bugs\n",
    "\n",
    "Response 1:\n",
    "{{\"thought\": \"I need to read the file first to see its contents.\", \"action\": {{\"tool\": \"read_file\", \"args\": [\"auth.py\"]}}}}\n",
    "\n",
    "Observation: def check(u): return db.user = u\n",
    "\n",
    "Response 2:\n",
    "{{\"thought\": \"There's a bug: using = (assignment) instead of == (comparison). I'll fix it.\", \"action\": {{\"tool\": \"patch_file\", \"args\": [\"auth.py\", \"def check(u): return db.user == u\"]}}}}\n",
    "\n",
    "Observation: File successfully updated: auth.py\n",
    "\n",
    "Response 3:\n",
    "{{\"thought\": \"The bug is fixed. The comparison operator is now correct.\", \"answer\": \"Fixed auth.py: changed assignment operator (=) to comparison operator (==) in the return statement.\"}}\n",
    "\"\"\"\n",
    "\n",
    "        self.trim_history_to_fit(system_message_context)\n",
    "        \n",
    "        # Build prompt with system instructions\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\":\"system\",\n",
    "                \"content\":system_message_context\n",
    "            }\n",
    "        ] + self.conversation_history\n",
    "\n",
    "        response = openai.responses.create(model=self.model, input=messages)\n",
    "\n",
    "        decision = response.output_text\n",
    "\n",
    "        # Add assistant's decision to conversation history\n",
    "        self.conversation_history.append({\n",
    "            \"role\":\"assistant\",\n",
    "            \"content\": decision\n",
    "        })\n",
    "\n",
    "        return decision\n",
    "```\n",
    "\n",
    "4. **Update** ```run`` to include the plan\n",
    "```python\n",
    "    def run(self, user_query: str, max_iterations=10):\n",
    "        \"\"\"\n",
    "        Main execution loop with planning and ReAct pattern.\n",
    "        \n",
    "        Args:\n",
    "            user_query: The user's request\n",
    "            max_iterations: Maximum number of think-act cycles\n",
    "        \n",
    "        Returns:\n",
    "            Final response string\n",
    "        \"\"\"\n",
    "        current_input = user_query\n",
    "\n",
    "        for step in range(max_iterations):\n",
    "            print(f\"\\n{'-'*60}\")\n",
    "            print(f\"Step {step+1} of {max_iterations}\")\n",
    "            print(f\"{'-'*60}\")\n",
    "\n",
    "            llm_response = self.think(current_input)\n",
    "            print(f\"\\nAgent's LLM Response:\\n{llm_response}\")\n",
    "\n",
    "            # Parse the JSON response\n",
    "            try:\n",
    "                parsed_response = json.loads(llm_response)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"\\nJSON Parse Error: {e}\")\n",
    "                current_input = (\n",
    "                    f\"Your response was not valid JSON. Error: {e}\\n\"\n",
    "                    f\"Respond with ONLY valid JSON matching the required format.\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            # Handle plan creation response\n",
    "            if parsed_response.get(\"plan_created\"):\n",
    "                print(f\"\\nPlan Created:\")\n",
    "                print(parsed_response.get(\"plan\", \"\"))\n",
    "                current_input = \"Proceed with step 1 of the plan.\"\n",
    "                continue\n",
    "\n",
    "            # Print thought if present\n",
    "            if \"thought\" in parsed_response:\n",
    "                print(f\"\\nThought: {parsed_response['thought']}\")\n",
    "\n",
    "            # Check for final answer\n",
    "            if \"answer\" in parsed_response:\n",
    "                print(f\"\\nAnswer: {parsed_response['answer']}\")\n",
    "                print(f\"\\nProgress: {len(self.completed_steps)}/{len(self.current_plan)} steps completed\")\n",
    "                return parsed_response[\"answer\"]\n",
    "            \n",
    "            # Execute action if present\n",
    "            if \"action\" in parsed_response:\n",
    "                action = parsed_response[\"action\"]\n",
    "                tool_name = action.get(\"tool\", \"unknown\")\n",
    "                args = action.get(\"args\", [])\n",
    "\n",
    "                print(f\"\\nAction: {tool_name}({', '.join(repr(a) for a in args)})\")\n",
    "                \n",
    "                observation = self.act(action)\n",
    "                \n",
    "                # Truncate long observations for display\n",
    "                obs_display = observation[:500] + \"...\" if len(str(observation)) > 500 else observation\n",
    "                print(f\"\\nObservation: {obs_display}\")\n",
    "                \n",
    "                current_input = f\"Observation: {observation}\"\n",
    "            else:\n",
    "                # Neither action nor answer\n",
    "                print(\"\\nResponse missing both 'action' and 'answer'\")\n",
    "                current_input = (\n",
    "                    \"Your response must include either 'action' (to use a tool) \"\n",
    "                    \"or 'answer' (if the task is complete). Please try again.\"\n",
    "                )\n",
    "\n",
    "        print(f\"\\nMaximum Iterations ({max_iterations}) reached\")\n",
    "        print(f\"Progress: {len(self.completed_steps)}/{len(self.current_plan)} steps completed\")\n",
    "        return \"Task Incomplete: Max steps reached\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db60d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import json\n",
    "\n",
    "class CodeReviewAgentPlanning:\n",
    "    def __init__(self, tools_registry: ToolRegistry, model=\"gpt-4.1\", \n",
    "                 memory_file=\"agent_memory.json\", summarize_after=10, \n",
    "                 max_context_tokens=6000):\n",
    "        self.tools = tools_registry\n",
    "        self.model = model\n",
    "        self.conversation_history = []  # Short-term memory\n",
    "        self.memory_file = memory_file\n",
    "        self.load_long_term_memory()  # Long-term memory (key-value store)\n",
    "        self.conversation_summary = \"\"  # Summarized conversation history\n",
    "        self.summarize_after = summarize_after\n",
    "        self.turns_since_summary = 0\n",
    "        self.max_context_tokens = max_context_tokens\n",
    "        \n",
    "        # Planning-specific attributes\n",
    "        self.current_plan = []  # List of planned steps\n",
    "        self.completed_steps = []  # Track what has been done\n",
    "        self.plan_created = False\n",
    "\n",
    "        # Initialize tokenizer for the model\n",
    "        try:\n",
    "            self.tokenizer = tiktoken.encoding_for_model(model)\n",
    "        except:\n",
    "            self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Count tokens in a string\"\"\"\n",
    "        return len(self.tokenizer.encode(text))\n",
    "    \n",
    "    def trim_history_to_fit(self, system_message: str):\n",
    "        \"\"\"Remove old messages until we fit within the token budget\"\"\"\n",
    "        fixed_tokens = self.count_tokens(system_message)\n",
    "        history_tokens = sum([self.count_tokens(msg[\"content\"]) \n",
    "                            for msg in self.conversation_history])\n",
    "        total_tokens = fixed_tokens + history_tokens\n",
    "\n",
    "        while total_tokens > self.max_context_tokens and len(self.conversation_history) > 2:\n",
    "            removed_msg = self.conversation_history.pop(0)\n",
    "            total_tokens -= self.count_tokens(removed_msg[\"content\"])\n",
    "\n",
    "        return total_tokens\n",
    "\n",
    "    def summarize_history(self):\n",
    "        \"\"\"Use LLM to summarize the conversation so far.\"\"\"\n",
    "        if len(self.conversation_history) < 3:\n",
    "            return\n",
    "        \n",
    "        history_text = \"\\n\".join([\n",
    "            f\"{msg['role']}:{msg['content']}\" \n",
    "            for msg in self.conversation_history\n",
    "        ])\n",
    "\n",
    "        summary_prompt = f\"\"\"Summarize this conversation in 3-4 sentences,\n",
    "        preserving key facts, decisions, and actions taken:\n",
    "        {history_text}\n",
    "\n",
    "        Previous Summary: {self.conversation_summary or 'None'}\n",
    "        \"\"\"\n",
    "\n",
    "        response = openai.responses.create(\n",
    "            model=self.model, \n",
    "            input=[{\"role\": \"user\", \"content\": summary_prompt}]\n",
    "        )\n",
    "        self.conversation_summary = response.output_text\n",
    "\n",
    "        # Keep only the last few turns + the summary\n",
    "        recent_turns = self.conversation_history[-4:]\n",
    "        self.conversation_history = recent_turns\n",
    "        self.turns_since_summary = 0\n",
    "\n",
    "    def remember(self, key: str, value: str):\n",
    "        \"\"\"Store information in long term memory.\"\"\"\n",
    "        self.long_term_memory[key] = value\n",
    "        self.save_long_term_memory()\n",
    "    \n",
    "    def recall(self, key: str) -> str:\n",
    "        \"\"\"Retrieve information from long term memory\"\"\"\n",
    "        return self.long_term_memory.get(key, \"No memory found for this key.\")\n",
    "    \n",
    "    def get_relevant_memories(self) -> str:\n",
    "        \"\"\"Format long term memories for inclusion in prompts.\"\"\"\n",
    "        if not self.long_term_memory:\n",
    "            return \"No stored memories\"\n",
    "        \n",
    "        memories = \"\\n\".join([f\"- {k}: {v}\" for k, v in self.long_term_memory.items()])\n",
    "        return f\"Relevant memories:\\n{memories}\"\n",
    "    \n",
    "    def save_long_term_memory(self):\n",
    "        \"\"\"Persist long term memory to JSON file\"\"\"\n",
    "        try:\n",
    "            with open(self.memory_file, \"w\") as f:\n",
    "                json.dump(self.long_term_memory, f, indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save memory to {self.memory_file}: {e}\")\n",
    "\n",
    "    def load_long_term_memory(self):\n",
    "        \"\"\"Load long term memory from JSON file\"\"\"\n",
    "        if os.path.exists(self.memory_file):\n",
    "            try:\n",
    "                with open(self.memory_file, 'r') as f:\n",
    "                    self.long_term_memory = json.load(f)\n",
    "                print(f\"Loaded {len(self.long_term_memory)} memories from {self.memory_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load memory from {self.memory_file}: {e}\")\n",
    "                self.long_term_memory = {}\n",
    "        else:\n",
    "            self.long_term_memory = {}\n",
    "    \n",
    "    def create_plan(self, user_query: str) -> list:\n",
    "        \"\"\"Generate a step by step plan for the user's request\"\"\"\n",
    "        planning_prompt = f\"\"\"\n",
    "        Given this task: \"{user_query}\"\n",
    "        Create a detailed execution plan with numbered steps. \n",
    "        Each step should be a specific action.\n",
    "\n",
    "        Available tools:\n",
    "        - read_file(filepath): Read a file's contents\n",
    "        - patch_file(filepath, content): Update a file\n",
    "        - write_test(file_path, test_code): Create a test file\n",
    "        - run_test(file_path): Execute tests\n",
    "\n",
    "        Format your response as a JSON list of steps:\n",
    "        [\n",
    "            {{\"step\": 1, \"action\": \"description\", \"tool\": \"tool_name\"}},\n",
    "            {{\"step\": 2, \"action\": \"description\", \"tool\": \"tool_name\"}}\n",
    "        ]\n",
    "\n",
    "        Only include necessary steps. Be specific about which files to work with.\n",
    "        Respond with ONLY the JSON array—no markdown, no extra text.\n",
    "        \"\"\"\n",
    "\n",
    "        response = openai.responses.create(\n",
    "            model=self.model,\n",
    "            input=[{\"role\": \"user\", \"content\": planning_prompt}]\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            plan = json.loads(response.output_text)\n",
    "            self.current_plan = plan\n",
    "            self.plan_created = True\n",
    "            return plan\n",
    "        except json.JSONDecodeError:\n",
    "            self.current_plan = [\n",
    "                {\"step\": 1, \"action\": \"Proceed step by step\", \"tool\": \"read_file\"}\n",
    "            ]\n",
    "            self.plan_created = True\n",
    "            return self.current_plan\n",
    "    \n",
    "    def _build_plan_context(self, next_step) -> str:\n",
    "        \"\"\"Format plan information for the prompt\"\"\"\n",
    "        completed = \"\\n\".join([\n",
    "            f\"  ✓ Step {step['step']}: {step['action']}\" \n",
    "            for step in self.completed_steps\n",
    "        ])\n",
    "\n",
    "        if next_step:\n",
    "            current = f\"\\n→ CURRENT: Step {next_step['step']}: {next_step['action']}\"\n",
    "        else:\n",
    "            current = \"\\n→ All steps completed\"\n",
    "        \n",
    "        remaining = \"\\n\".join([\n",
    "            f\"  Step {step['step']}: {step['action']}\" \n",
    "            for step in self.current_plan[len(self.completed_steps)+1:]\n",
    "        ])\n",
    "\n",
    "        return f\"\"\"\n",
    "## Execution Plan Progress\n",
    "\n",
    "Completed:\n",
    "{completed if completed else \"  None yet\"}\n",
    "{current}\n",
    "\n",
    "Remaining:\n",
    "{remaining if remaining else \"  None\"}\n",
    "\"\"\"\n",
    "\n",
    "    def build_system_prompt(self, plan_context: str = \"\") -> str:\n",
    "        \"\"\"Construct the ReAct system prompt with current context and plan.\"\"\"\n",
    "        return f\"\"\"You are a code review assistant using the ReAct pattern with planning.\n",
    "\n",
    "## Available Tools\n",
    "- read_file(filepath): Read contents of a file\n",
    "- patch_file(filepath, content): Replace file contents entirely\n",
    "- write_test(file_path, test_code): Create a test file\n",
    "- run_test(file_path): Execute tests and return results\n",
    "\n",
    "## Context\n",
    "{self.get_relevant_memories()}\n",
    "\n",
    "Conversation summary: {self.conversation_summary or 'This is the start of the conversation.'}\n",
    "\n",
    "{plan_context}\n",
    "\n",
    "## Response Format\n",
    "\n",
    "You MUST respond with valid JSON in one of these two formats:\n",
    "\n",
    "### Format 1: When you need to use a tool\n",
    "{{\n",
    "    \"thought\": \"Your reasoning about what to do and why\",\n",
    "    \"action\": {{\n",
    "        \"tool\": \"tool_name\",\n",
    "        \"args\": [\"arg1\", \"arg2\"]\n",
    "    }}\n",
    "}}\n",
    "\n",
    "### Format 2: When the task is complete\n",
    "{{\n",
    "    \"thought\": \"Your reasoning about why the task is complete\",\n",
    "    \"answer\": \"Your final response to the user\"\n",
    "}}\n",
    "\n",
    "## Rules\n",
    "1. Always include \"thought\" explaining your reasoning\n",
    "2. Include \"action\" when you need to call a tool\n",
    "3. Include \"answer\" only when ALL plan steps are complete\n",
    "4. Never include both \"action\" and \"answer\"\n",
    "5. Respond with ONLY valid JSON—no markdown, no extra text\n",
    "6. Follow the execution plan systematically\n",
    "7. After each successful action, the system will mark that step complete\n",
    "\n",
    "## Example\n",
    "\n",
    "User: Review auth.py and fix any bugs\n",
    "\n",
    "Response 1:\n",
    "{{\"thought\": \"I need to read the file first to see its contents.\", \"action\": {{\"tool\": \"read_file\", \"args\": [\"auth.py\"]}}}}\n",
    "\n",
    "Observation: def check(u): return db.user = u\n",
    "\n",
    "Response 2:\n",
    "{{\"thought\": \"There's a bug: using = (assignment) instead of == (comparison). I'll fix it.\", \"action\": {{\"tool\": \"patch_file\", \"args\": [\"auth.py\", \"def check(u): return db.user == u\"]}}}}\n",
    "\n",
    "Observation: File successfully updated: auth.py\n",
    "\n",
    "Response 3:\n",
    "{{\"thought\": \"The bug is fixed. The comparison operator is now correct.\", \"answer\": \"Fixed auth.py: changed assignment operator (=) to comparison operator (==) in the return statement.\"}}\n",
    "\"\"\"\n",
    "\n",
    "    def think(self, user_input: str):\n",
    "        \"\"\"LLM decides which tool to use with plan awareness.\"\"\"\n",
    "        \n",
    "        # First request: create a plan\n",
    "        if not self.plan_created:\n",
    "            plan = self.create_plan(user_query=user_input)\n",
    "            plan_summary = \"\\n\".join([\n",
    "                f\"Step {step['step']}: {step['action']}\" \n",
    "                for step in plan\n",
    "            ])\n",
    "            \n",
    "            # Return a special response indicating the plan was created\n",
    "            return json.dumps({\n",
    "                \"thought\": \"I've analyzed the task and created an execution plan.\",\n",
    "                \"plan_created\": True,\n",
    "                \"plan\": plan_summary\n",
    "            })\n",
    "\n",
    "        # Add user message to history\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        self.turns_since_summary += 1\n",
    "\n",
    "        # Check if we should summarize\n",
    "        if self.turns_since_summary >= self.summarize_after:\n",
    "            self.summarize_history()\n",
    "\n",
    "        # Get current step from plan\n",
    "        next_step = None\n",
    "        if len(self.completed_steps) < len(self.current_plan):\n",
    "            next_step = self.current_plan[len(self.completed_steps)]\n",
    "        \n",
    "        # Build context with plan information\n",
    "        plan_context = self._build_plan_context(next_step)\n",
    "\n",
    "        # Include long term memory & summary in system context\n",
    "        system_message_context = self.build_system_prompt(plan_context)\n",
    "\n",
    "        self.trim_history_to_fit(system_message_context)\n",
    "        \n",
    "        # Build prompt with system instructions\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_message_context}\n",
    "        ] + self.conversation_history\n",
    "\n",
    "        response = openai.responses.create(model=self.model, input=messages)\n",
    "        decision = response.output_text\n",
    "\n",
    "        # Add assistant's decision to conversation history\n",
    "        self.conversation_history.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": decision\n",
    "        })\n",
    "\n",
    "        return decision\n",
    "    \n",
    "    def act(self, action: dict):\n",
    "        \"\"\"Execute the chosen tool and update plan progress.\"\"\"\n",
    "        try:\n",
    "            tool_name = action.get(\"tool\")\n",
    "            args = action.get(\"args\", [])\n",
    "\n",
    "            result = self.tools.call(tool_name, *args)\n",
    "\n",
    "            # Mark current step as complete\n",
    "            if len(self.completed_steps) < len(self.current_plan):\n",
    "                current_step = self.current_plan[len(self.completed_steps)]\n",
    "                self.completed_steps.append(current_step)\n",
    "\n",
    "            self.conversation_history.append({\"role\": \"system\", \"content\": result})\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error executing tool: {e}\"\n",
    "            self.conversation_history.append({\n",
    "                \"role\": \"system\",\n",
    "                \"content\": error_msg\n",
    "            })\n",
    "            return error_msg\n",
    "\n",
    "    def run(self, user_query: str, max_iterations=10):\n",
    "        \"\"\"\n",
    "        Main execution loop with planning and ReAct pattern.\n",
    "        \n",
    "        Args:\n",
    "            user_query: The user's request\n",
    "            max_iterations: Maximum number of think-act cycles\n",
    "        \n",
    "        Returns:\n",
    "            Final response string\n",
    "        \"\"\"\n",
    "        current_input = user_query\n",
    "\n",
    "        for step in range(max_iterations):\n",
    "            print(f\"\\n{'-'*60}\")\n",
    "            print(f\"Step {step+1} of {max_iterations}\")\n",
    "            print(f\"{'-'*60}\")\n",
    "\n",
    "            llm_response = self.think(current_input)\n",
    "            print(f\"\\nAgent's LLM Response:\\n{llm_response}\")\n",
    "\n",
    "            # Parse the JSON response\n",
    "            try:\n",
    "                parsed_response = json.loads(llm_response)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"\\nJSON Parse Error: {e}\")\n",
    "                current_input = (\n",
    "                    f\"Your response was not valid JSON. Error: {e}\\n\"\n",
    "                    f\"Respond with ONLY valid JSON matching the required format.\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            # Handle plan creation response\n",
    "            if parsed_response.get(\"plan_created\"):\n",
    "                print(f\"\\nPlan Created:\")\n",
    "                print(parsed_response.get(\"plan\", \"\"))\n",
    "                current_input = \"Proceed with step 1 of the plan.\"\n",
    "                continue\n",
    "\n",
    "            # Print thought if present\n",
    "            if \"thought\" in parsed_response:\n",
    "                print(f\"\\nThought: {parsed_response['thought']}\")\n",
    "\n",
    "            # Check for final answer\n",
    "            if \"answer\" in parsed_response:\n",
    "                print(f\"\\nAnswer: {parsed_response['answer']}\")\n",
    "                print(f\"\\nProgress: {len(self.completed_steps)}/{len(self.current_plan)} steps completed\")\n",
    "                return parsed_response[\"answer\"]\n",
    "            \n",
    "            # Execute action if present\n",
    "            if \"action\" in parsed_response:\n",
    "                action = parsed_response[\"action\"]\n",
    "                tool_name = action.get(\"tool\", \"unknown\")\n",
    "                args = action.get(\"args\", [])\n",
    "\n",
    "                print(f\"\\nAction: {tool_name}({', '.join(repr(a) for a in args)})\")\n",
    "                \n",
    "                observation = self.act(action)\n",
    "                \n",
    "                # Truncate long observations for display\n",
    "                obs_display = observation[:500] + \"...\" if len(str(observation)) > 500 else observation\n",
    "                print(f\"\\nObservation: {obs_display}\")\n",
    "                \n",
    "                current_input = f\"Observation: {observation}\"\n",
    "            else:\n",
    "                # Neither action nor answer\n",
    "                print(\"\\nResponse missing both 'action' and 'answer'\")\n",
    "                current_input = (\n",
    "                    \"Your response must include either 'action' (to use a tool) \"\n",
    "                    \"or 'answer' (if the task is complete). Please try again.\"\n",
    "                )\n",
    "\n",
    "        print(f\"\\nMaximum Iterations ({max_iterations}) reached\")\n",
    "        print(f\"Progress: {len(self.completed_steps)}/{len(self.current_plan)} steps completed\")\n",
    "        return \"Task Incomplete: Max steps reached\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354950b8",
   "metadata": {},
   "source": [
    "### Usage\n",
    "Let's see our planning agent in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3f61759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 memories from agent_memory.json\n",
      "\n",
      "------------------------------------------------------------\n",
      "Step 1 of 10\n",
      "------------------------------------------------------------\n",
      "\n",
      "Agent's LLM Response:\n",
      "{\"thought\": \"I've analyzed the task and created an execution plan.\", \"plan_created\": true, \"plan\": \"Step 1: Read the contents of sample.py to review the code for issues.\\nStep 2: Fix any identified issues in sample.py and update the file.\\nStep 3: Write a new test file (e.g., test_sample.py) to cover the code in sample.py and check that issues are addressed.\\nStep 4: Run the tests in test_sample.py to verify that the issues have been fixed.\"}\n",
      "\n",
      "Plan Created:\n",
      "Step 1: Read the contents of sample.py to review the code for issues.\n",
      "Step 2: Fix any identified issues in sample.py and update the file.\n",
      "Step 3: Write a new test file (e.g., test_sample.py) to cover the code in sample.py and check that issues are addressed.\n",
      "Step 4: Run the tests in test_sample.py to verify that the issues have been fixed.\n",
      "\n",
      "------------------------------------------------------------\n",
      "Step 2 of 10\n",
      "------------------------------------------------------------\n",
      "\n",
      "Agent's LLM Response:\n",
      "{\"thought\": \"I need to read the contents of sample.py to review the code for issues.\", \"action\": {\"tool\": \"read_file\", \"args\": [\"sample.py\"]}}\n",
      "\n",
      "Thought: I need to read the contents of sample.py to review the code for issues.\n",
      "\n",
      "Action: read_file('sample.py')\n",
      "\n",
      "Observation: def divide(a, b):\n",
      "    return a / b\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Step 3 of 10\n",
      "------------------------------------------------------------\n",
      "\n",
      "Agent's LLM Response:\n",
      "{\"thought\": \"The divide function does not handle division by zero, which will raise a ZeroDivisionError. I will update the function to handle this case gracefully and return a suitable response, such as None or an error message.\", \"action\": {\"tool\": \"patch_file\", \"args\": [\"sample.py\", \"def divide(a, b):\\n    if b == 0:\\n        return None  # Avoid division by zero\\n    return a / b\\n\"]}}\n",
      "\n",
      "Thought: The divide function does not handle division by zero, which will raise a ZeroDivisionError. I will update the function to handle this case gracefully and return a suitable response, such as None or an error message.\n",
      "\n",
      "Action: patch_file('sample.py', 'def divide(a, b):\\n    if b == 0:\\n        return None  # Avoid division by zero\\n    return a / b\\n')\n",
      "\n",
      "Observation: File successfully updated: sample.py. New content written.\n",
      "\n",
      "------------------------------------------------------------\n",
      "Step 4 of 10\n",
      "------------------------------------------------------------\n",
      "\n",
      "Agent's LLM Response:\n",
      "{\"thought\": \"Now that sample.py has been updated to handle division by zero, I need to write a new test file, test_sample.py, to check the functionality and ensure that the division by zero case returns None.\", \"action\": {\"tool\": \"write_test\", \"args\": [\"test_sample.py\", \"import pytest\\nfrom sample import divide\\n\\ndef test_divide_normal():\\n    assert divide(6, 3) == 2\\n    assert divide(7, 2) == 3.5\\n\\ndef test_divide_zero():\\n    assert divide(10, 0) is None\\n\\ndef test_divide_negative():\\n    assert divide(-10, 2) == -5\\n    assert divide(10, -2) == -5\\n\"]}}\n",
      "\n",
      "Thought: Now that sample.py has been updated to handle division by zero, I need to write a new test file, test_sample.py, to check the functionality and ensure that the division by zero case returns None.\n",
      "\n",
      "Action: write_test('test_sample.py', 'import pytest\\nfrom sample import divide\\n\\ndef test_divide_normal():\\n    assert divide(6, 3) == 2\\n    assert divide(7, 2) == 3.5\\n\\ndef test_divide_zero():\\n    assert divide(10, 0) is None\\n\\ndef test_divide_negative():\\n    assert divide(-10, 2) == -5\\n    assert divide(10, -2) == -5\\n')\n",
      "\n",
      "Observation: Test file created: test_sample.py\n",
      "\n",
      "------------------------------------------------------------\n",
      "Step 5 of 10\n",
      "------------------------------------------------------------\n",
      "\n",
      "Agent's LLM Response:\n",
      "{\"thought\": \"The test file has been created. Next, I need to run the tests in test_sample.py to verify that the divide function works correctly and handles division by zero as intended.\", \"action\": {\"tool\": \"run_test\", \"args\": [\"test_sample.py\"]}}\n",
      "\n",
      "Thought: The test file has been created. Next, I need to run the tests in test_sample.py to verify that the divide function works correctly and handles division by zero as intended.\n",
      "\n",
      "Action: run_test('test_sample.py')\n",
      "\n",
      "Observation: Exit code 0\n",
      "\n",
      "Ouput:\n",
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.12.3, pytest-9.0.2, pluggy-1.6.0 -- c:\\Users\\Asa\\AppData\\Local\\Programs\\Python\\Python312\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\Repos\\writing\\ai-agents-tutorial\n",
      "plugins: anyio-4.12.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 3 items\n",
      "\n",
      "test_sample.py::test_divide_normal \u001b[32mPASSED\u001b[0m\u001b[32m                                [ 33%]\u001b[0m\n",
      "test_sample.py::test_divide_zero \u001b...\n",
      "\n",
      "------------------------------------------------------------\n",
      "Step 6 of 10\n",
      "------------------------------------------------------------\n",
      "\n",
      "Agent's LLM Response:\n",
      "{\"thought\": \"All plan steps are complete. The divide function now safely handles division by zero and passes all tests, including checks for normal, zero, and negative values.\", \"answer\": \"The divide function in sample.py was updated to handle division by zero by returning None. Comprehensive tests in test_sample.py all passed, verifying correct behavior for normal, zero, and negative cases.\"}\n",
      "\n",
      "Thought: All plan steps are complete. The divide function now safely handles division by zero and passes all tests, including checks for normal, zero, and negative values.\n",
      "\n",
      "Answer: The divide function in sample.py was updated to handle division by zero by returning None. Comprehensive tests in test_sample.py all passed, verifying correct behavior for normal, zero, and negative cases.\n",
      "\n",
      "Progress: 4/4 steps completed\n"
     ]
    }
   ],
   "source": [
    "registry = ToolRegistry()\n",
    "registry.register(\"read_file\",read_file)\n",
    "registry.register(\"print_review\",print_review)\n",
    "registry.register(\"write_test\",write_test)\n",
    "registry.register(\"patch_file\",patch_file)\n",
    "registry.register(\"run_test\",run_test)\n",
    "\n",
    "agent = CodeReviewAgentPlanning(tools_registry=registry,model=\"gpt-4.1\",max_context_tokens=8000)\n",
    "\n",
    "user_query = \"Review sample.py, fix any issues, write test and verify issues are fixed\"\n",
    "\n",
    "result = agent.run(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c358c01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
