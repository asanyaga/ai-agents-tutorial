{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66ca035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Callable, Dict\n",
    "\n",
    "def read_file(filepath: str) -> str:\n",
    "    \"\"\"Read contents of a Python file\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        return f\"File not found: {filepath}\"\n",
    "    \n",
    "    with open(filepath, \"r\") as f:\n",
    "        return f.read()\n",
    "    \n",
    "def patch_file(filepath: str, content: str) -> str:\n",
    "    \"\"\"Writes the given content to a file, completely replacing its current content.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, \"w\") as f:\n",
    "            f.write(content)\n",
    "        return f\"File successfully updated: {filepath}. New content written.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error writing to file {filepath}: {e}\"\n",
    "\n",
    "def print_review(review: str):\n",
    "    print(f\"Review: {review}\")\n",
    "    return f\"Printed review: {review}\"\n",
    "\n",
    "class ToolRegistry:\n",
    "    \"\"\"Holds available tools and dispatches them by name.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.tools: Dict[str,Callable] = {}\n",
    "    \n",
    "    def register(self, name:str, func: Callable):\n",
    "        self.tools[name] = func\n",
    "\n",
    "    def call(self, name:str, *args, **kwargs):\n",
    "        if name not in self.tools:\n",
    "            return f\"Unknown tool: {name}\"\n",
    "        return self.tools[name](*args, **kwargs)\n",
    "\n",
    "import tiktoken\n",
    "import json\n",
    "import openai\n",
    "\n",
    "class CodeReviewAgentReAct:\n",
    "    def __init__(self,tools_registry: ToolRegistry, model=\"gpt-4.1\",memory_file=\"agent_memory.json\",summarize_after=10,max_context_tokens=6000):\n",
    "        self.tools = tools_registry\n",
    "        self.model = model\n",
    "        self.conversation_history = [] # Short-term memory\n",
    "        self.memory_file = memory_file\n",
    "        self.load_long_term_memory() # Long-term memory (key-value store)\n",
    "        self.conversation_summary = \"\" # Summarized conversation history\n",
    "        self.summarize_after = summarize_after\n",
    "        self.turns_since_summary = 0\n",
    "        self.max_context_tokens = max_context_tokens\n",
    "        \n",
    "\n",
    "        # Initialize tokenizer for the model\n",
    "        try:\n",
    "            self.tokenizer = tiktoken.encoding_for_model(model)\n",
    "        except:\n",
    "            self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    def count_tokens(self, text:str) -> int:\n",
    "        \"\"\"Count tokens in a string\"\"\"\n",
    "        return len(self.tokenizer.encode(text))\n",
    "    \n",
    "    def trim_history_to_fit(self, system_message:str):\n",
    "        \"\"\"Remove old messages until we fit within the token budget\"\"\"\n",
    "\n",
    "        # Count tokens in system message\n",
    "        fixed_tokens = self.count_tokens(system_message)\n",
    "\n",
    "        # Count tokens in conversation history\n",
    "        history_tokens = sum([self.count_tokens(msg[\"content\"]) for msg in self.conversation_history])\n",
    "\n",
    "        total_tokens = fixed_tokens + history_tokens\n",
    "\n",
    "        while total_tokens > self.max_context_tokens and len(self.conversation_history) > 2:\n",
    "            removed_msg = self.conversation_history.pop(0)\n",
    "            total_tokens -= self.count_tokens(removed_msg[\"content\"])\n",
    "\n",
    "        return total_tokens\n",
    "\n",
    "    def summarize_history(self):\n",
    "        \"\"\"Use LLM to summarize the conversation so far.\"\"\"\n",
    "        if len(self.conversation_history) < 3:\n",
    "            return\n",
    "        \n",
    "        history_text = \"\\n\".join([f\"{msg[\"role\"]}:{msg[\"content\"]}\" for msg in self.conversation_history])\n",
    "\n",
    "        summary_prompt = f\"\"\"Summarize this conversation in 3-4 sentences,\n",
    "        preserving key fact, decisions, and actions taken:\n",
    "        {history_text}\n",
    "\n",
    "        Previous Summary: {self.conversation_summary or 'None'}\n",
    "        \"\"\"\n",
    "\n",
    "        response = openai.responses.create(model=self.model, input=[{\"role\":\"user\",\"content\":summary_prompt}])\n",
    "\n",
    "        self.conversation_summary = response.output_text\n",
    "\n",
    "        # Keep only the last few turns + the summary\n",
    "        recent_turns = self.conversation_history[-4:] # Keep the last 4 messages (2 user/assistant exchanges)\n",
    "\n",
    "        self.conversation_history = recent_turns\n",
    "        self.turns_since_summary = 0\n",
    "\n",
    "\n",
    "    def remember(self, key:str, value: str):\n",
    "        \"\"\"Retrieve information from long term memory.\"\"\"\n",
    "        self.long_term_memory[key] = value\n",
    "        self.save_long_term_memory()\n",
    "    \n",
    "    def recall(self,key:str) -> str:\n",
    "        \"\"\"Retrieve information from long term memory\"\"\"\n",
    "        return self.long_term_memory.get(key,\"No memory found for this key.\")\n",
    "    \n",
    "    def get_relevant_memories(self) -> str:\n",
    "        \"\"\"Format long term memories for inclusion in prompts.\"\"\"\n",
    "        if not self.long_term_memory:\n",
    "            return \"No stored memories\"\n",
    "        \n",
    "        memories = \"\\n\".join([f\"- {k}:{v}\" for k, v in self.long_term_memory.items()])\n",
    "        return f\"Relevant memories:\\n{memories}\"\n",
    "    \n",
    "    def save_long_term_memory(self):\n",
    "        \"\"\"Persist long term memory to JSON file\"\"\"\n",
    "        try:\n",
    "            with open(self.memory_file,\"w\") as f:\n",
    "                json.dump(self.long_term_memory,f,indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save memory to {self.memory_file}:  {e}\")\n",
    "\n",
    "    def load_long_term_memory(self):\n",
    "        \"\"\"Load long term memory from JSON file\"\"\"\n",
    "        if os.path.exists(self.memory_file):\n",
    "            try:\n",
    "                with open(self.memory_file, 'r') as f:\n",
    "                    self.long_term_memory = json.load(f)\n",
    "                print(f\"Loaded {len(self.long_term_memory)} memories from {self.memory_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load memory from {self.memory_file}: {e}\")\n",
    "        else:\n",
    "            self.long_term_memory = {}\n",
    "\n",
    "    \n",
    "    def build_system_prompt(self) -> str:\n",
    "        \"\"\"Construct the ReAct system prompt with current context.\"\"\"\n",
    "        return f\"\"\"You are a code review assistant using the ReAct pattern.\n",
    "\n",
    "        ## Available Tools\n",
    "        - read_file(filepath): Read contents of a file\n",
    "        - analyze_code(code): Get LLM analysis of code  \n",
    "        - patch_file(filepath, content): Replace file contents entirely\n",
    "\n",
    "        ## Context\n",
    "        {self.get_relevant_memories()}\n",
    "\n",
    "        Conversation summary: {self.conversation_summary or 'This is the start of the conversation.'}\n",
    "\n",
    "        ## Response Format\n",
    "\n",
    "        You MUST respond with valid JSON in one of these two formats:\n",
    "\n",
    "        ### Format 1: When you need to use a tool\n",
    "        {{\n",
    "        \"thought\": \"Your reasoning about what to do and why\",\n",
    "        \"action\": {{\n",
    "            \"tool\": \"tool_name\",\n",
    "            \"args\": [\"arg1\", \"arg2\"]\n",
    "        }}\n",
    "        }}\n",
    "\n",
    "        ### Format 2: When the task is complete\n",
    "        {{\n",
    "            \"thought\": \"Your reasoning about why the task is complete\",\n",
    "            \"answer\": \"Your final response to the user\"\n",
    "        }}\n",
    "\n",
    "        ## Rules\n",
    "        1. Always include \"thought\" explaining your reasoning\n",
    "        2. Include \"action\" when you need to call a tool\n",
    "        3. Include \"answer\" only when the task is fully complete\n",
    "        4. Never include both \"action\" and \"answer\"\n",
    "        5. Respond with ONLY valid JSONâ€”no markdown, no extra text\n",
    "\n",
    "        ## Example\n",
    "\n",
    "        User: Review auth.py and fix any bugs\n",
    "\n",
    "        Response 1:\n",
    "        {{\"thought\": \"I need to read the file first to see its contents.\", \"action\": {{\"tool\": \"read_file\", \"args\": [\"auth.py\"]}}}}\n",
    "\n",
    "        Observation: def check(u): return db.user = u\n",
    "\n",
    "        Response 2:\n",
    "        {{\"thought\": \"There's a bug: using = (assignment) instead of == (comparison). I'll fix it.\", \"action\": {{\"tool\": \"patch_file\", \"args\": [\"auth.py\", \"def check(u): return db.user == u\"]}}}}\n",
    "\n",
    "        Observation: File successfully updated: auth.py\n",
    "\n",
    "        Response 3:\n",
    "        {{\"thought\": \"The bug is fixed. The comparison operator is now correct.\", \"answer\": \"Fixed auth.py: changed assignment operator (=) to comparison operator (==) in the return statement.\"}}\n",
    "        \"\"\"\n",
    "\n",
    "    def think(self, user_input:str):\n",
    "        \"\"\"LLM decides which tool to use with both short term and long term context.\"\"\"\n",
    "        # Add user message to history\n",
    "        self.conversation_history.append({\"role\":\"user\",\"content\":user_input})\n",
    "\n",
    "        self.turns_since_summary += 1\n",
    "\n",
    "        # Check if we should summarize\n",
    "        if self.turns_since_summary >= self.summarize_after:\n",
    "            self.summarize_history()\n",
    "\n",
    "        #Include long term memory & summary in system context\n",
    "        system_message_context = self.build_system_prompt()\n",
    "\n",
    "        self.trim_history_to_fit(system_message_context)\n",
    "        \n",
    "        # Build prompt with system instructions\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\":\"system\",\n",
    "                \"content\":system_message_context\n",
    "            }\n",
    "        ] + self.conversation_history\n",
    "\n",
    "        response = openai.responses.create(model=self.model, input=messages)\n",
    "\n",
    "        decision = response.output_text\n",
    "\n",
    "        # Add assistant's decision to conversation history\n",
    "        self.conversation_history.append({\n",
    "            \"role\":\"assistant\",\n",
    "            \"content\": decision\n",
    "        })\n",
    "\n",
    "        return decision\n",
    "    \n",
    "    def act(self, action:str):\n",
    "        \"\"\"Execute the chosen tool and return the result.\"\"\"\n",
    "        try:\n",
    "            \n",
    "            tool_name = action.get(\"tool\")\n",
    "            args = action.get(\"args\",[])\n",
    "\n",
    "            result = self.tools.call(tool_name,*args)\n",
    "            self.conversation_history.append({\"role\":\"system\",\"content\":result})\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error executing tool: {e}\"\n",
    "            self.conversation_history.append({\n",
    "                \"role\":\"system\",\n",
    "                \"content\": error_msg\n",
    "            })\n",
    "            \n",
    "            return error_msg\n",
    "\n",
    "    def run(self, user_query:str, max_iterations=3):\n",
    "        \"\"\"\n",
    "        Main execution loop with reflection.\n",
    "        Args:\n",
    "            user_query: The user's request\n",
    "            max_iterations: Maxumum number of think-act-reflect cycles. this is to avoid the agent getting stuck in a loop.\n",
    "        \n",
    "        Returns:\n",
    "            Final response string\n",
    "        \"\"\"\n",
    "        step = 0\n",
    "\n",
    "        current_input = user_query\n",
    "\n",
    "        for step in range(max_iterations):\n",
    "            \n",
    "            print(f\"\\n{'-'*60}\")\n",
    "            print(f\"\\nStep {step+1} of {max_iterations}\")\n",
    "            print(f\"\\n{'-'*60}\")\n",
    "\n",
    "            llm_response = self.think(current_input)\n",
    "\n",
    "            print(f\"Agent's LLM Response:\\n{llm_response}\")\n",
    "\n",
    "            try:\n",
    "                parsed_reponse = json.loads(llm_response)\n",
    "            except json.JSONDecodeError as e:\n",
    "                current_input = (\n",
    "                    f\"Your response was not valid Json. Error: {e}\\n\"\n",
    "                    f\"Respond with ONLY valid JSON matching the required format.\"\n",
    "                )\n",
    "            if \"thought\" in parsed_reponse:\n",
    "                print(f\"\\nThought: {parsed_reponse[\"thought\"]}\")\n",
    "\n",
    "            if \"answer\" in parsed_reponse:\n",
    "                print(f\"\\n Answer: {parsed_reponse[\"answer\"]}\")\n",
    "                return parsed_reponse[\"answer\"]\n",
    "            \n",
    "            if \"action\" in parsed_reponse:\n",
    "                action = parsed_reponse[\"action\"]\n",
    "                tool_name = action.get(\"tool\",\"unknown\")\n",
    "                args = action.get(\"args\", [])\n",
    "\n",
    "                observation = self.act(action)\n",
    "                print(f\"Action: {tool_name}({','.join(repr(a) for a in args)})\")\n",
    "                current_input = f\"Observation: {observation}\"\n",
    "            else:\n",
    "                # Neither action nor answer\n",
    "                print(\"\\nResponse missing both 'action' and 'answer'\")\n",
    "                current_input = (\n",
    "                    \"Your response must include either 'action' (to use a tool) \"\n",
    "                    \"or 'answer' (if the task is complete). Please try again.\"\n",
    "                )\n",
    "\n",
    "        return \"Max steps reached without a final answer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e8841d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
