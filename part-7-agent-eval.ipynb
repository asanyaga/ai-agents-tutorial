{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a893a74",
   "metadata": {},
   "source": [
    "# Part 7: Evaluating AI Agents\n",
    "In [Part-6-Observability](https://github.com/asanyaga/ai-agents-tutorial/blob/main/part-6-agent-observability.ipynb) we built logging, tracing, metrics, and token tracking into our code review agent.  \n",
    "We can now see *what* our agent does. But, observability alone does not tell you whether the agent is doing things correctly.\n",
    "\n",
    "This tutorial addresses the question; *How do we systematically measure whether an agent works*\n",
    "\n",
    "In this tutorial we will be building very simple evaluations for the code review agent. We are building from scratch and keeping it simple to demostrate key concepts rather than full implementations.\n",
    "\n",
    "## From Observability to Evaluation\n",
    "Observability and evaluation are complementary\n",
    "- **Observability** answers: \"What happened during this execution?\"\n",
    "- **Evaluation** answers: \"Was that execution correct?\"\n",
    "\n",
    "Observability gives you the data. Evaluation gives you the judgement.\n",
    "\n",
    "Traditional software testing is straightforward; given input X, expect output Y. The vast majority of traditional software is deterministic, meaning that given the same input and in the same conditions we can expect the output is always going to be the same.\n",
    "AI Agents rely on outputs from large language models which are non deterministic which means the output from an AI agent is also not deterministic;\n",
    "- The same query might produce different tool sequences\n",
    "- Valid outputs can vary in phrasing while meaning the same thing\n",
    "- Correct output and behaviour from agents often requires judgement not just exact output matching\n",
    "- The path that the agent takes to get to the result matthers, not just the final output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b835c00",
   "metadata": {},
   "source": [
    "## Simple Eval: End to End Task Success\n",
    "End to end evaluations are simple; run the agent, check if the final output meetis expectation.\n",
    "\n",
    "### Setting up the test environment\n",
    "We will create a simple test case which will be a python file with a known bug and a way to verify the fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5774372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case: division_zero_check\n",
      "Description: Fix missing division by zero check\n",
      "Input Query: Review and fix sample.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "@dataclass\n",
    "class SimpleTestCase:\n",
    "    \"\"\"A single end to end test case\"\"\"\n",
    "    name: str\n",
    "    description: str\n",
    "    input_query: str\n",
    "    file_content: str\n",
    "    file_name: str\n",
    "    expected_behaviour: str\n",
    "\n",
    "def create_test_file(directory: str, filename: str, content: str) -> str:\n",
    "    \"\"\"Create a test file and return its path\"\"\"\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    with open(file_path,\"w\") as f:\n",
    "        f.write(content)\n",
    "    return file_path\n",
    "\n",
    "# Test Case: division function missing zero check\n",
    "first_test = SimpleTestCase(\n",
    "    name=\"division_zero_check\",\n",
    "    description=\"Fix missing division by zero check\",\n",
    "    input_query=\"Review and fix sample.py\",\n",
    "    file_content= \"\"\"\n",
    "def divide(a,b):\n",
    "    return a/b\n",
    "\"\"\",\n",
    "    file_name=\"sample.py\",\n",
    "    expected_behaviour=\"Should add a check for division by zero\"\n",
    ")\n",
    "\n",
    "print(f\"Test Case: {first_test.name}\")\n",
    "print(f\"Description: {first_test.description}\")\n",
    "print(f\"Input Query: {first_test.input_query}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e9be8d",
   "metadata": {},
   "source": [
    "# A Simple Pass/Fail Checker\n",
    "Let us write a function that checks whether the agent's fix is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa1c7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_division_fix(filepath: str) -> tuple[bool, str]:\n",
    "    \"\"\" \n",
    "    Check if the division function now handles zero division.\n",
    "\n",
    "    Returns:\n",
    "        (passed, reason): Whether the test passed and why\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with open(filepath,\"r\") as f:\n",
    "            code = f.read()\n",
    "\n",
    "            # Check for zero-handling patterns\n",
    "\n",
    "            zero_checks = [\n",
    "                \"b == 0\",\n",
    "                \"b != 0\",\n",
    "                \"if not b\",\n",
    "                \"if b:\",\n",
    "                \"ZeroDivisionError\",\n",
    "                \"division by zero\"\n",
    "            ]\n",
    "\n",
    "            has_zero_check = any(check in code for check in zero_checks)\n",
    "\n",
    "            if has_zero_check:\n",
    "                return True, \"Code now handles division by zero\"\n",
    "            else:\n",
    "                return False, f\"No zero division handling found. Current code:\\n{code}\"\n",
    "    except FileNotFoundError:\n",
    "        return False,f\"File not found {filepath}\"\n",
    "    except Exception as e:\n",
    "        return False, f\"Error checking file {e}\"\n",
    "    \n",
    "test_code_good = \"\"\"def divide(a, b):\n",
    "    if b == 0:\n",
    "        raise ValueError(\"Cannot divide by zero)\n",
    "    return a / b\n",
    "\"\"\"\n",
    "\n",
    "test_code_bad = \"\"\"def divide(a, b):\n",
    "    return a / b\n",
    "\"\"\"\n",
    "\n",
    "# Verify checker works\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    good_path = create_test_file(tmpdir,\"good.py\", test_code_good)\n",
    "    bad_path = create_test_file(tmpdir, \"bad.py\",test_code_bad)\n",
    "\n",
    "    good_result = check_division_fix(good_path)\n",
    "    bad_result = check_division_fix(bad_path)\n",
    "\n",
    "    print(f\"Good code check: passed={good_result[0]}, reason={good_result[1][:50]}...\")\n",
    "    print(f\"Bad code check: passed={bad_result[0]}, reasong= {bad_result[1][:50]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fbfae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_review_agent_observable import CodeReviewAgentObservable,ToolRegistry,read_file,patch_file,print_review,run_test,write_test\n",
    "\n",
    "import time\n",
    "\n",
    "def run_single_evaluation(agent_class, tools_registry, test_case: SimpleTestCase,checker_func) -> dict:\n",
    "    result = {\n",
    "        \"test_name\": test_case.name,\n",
    "        \"passed\": False,\n",
    "        \"reason\": \"\",\n",
    "        \"duration_seconds\":0,\n",
    "        \"agent_output\": \"\",\n",
    "        \"error\": None\n",
    "    }\n",
    "\n",
    "    # Create isolated test environment\n",
    "    with tempfile.TemporaryDirectory() as test_dir:\n",
    "        origininal_cwd = os.getcwd()\n",
    "        os.chdir(test_dir)\n",
    "\n",
    "        try:\n",
    "\n",
    "            agent = agent_class(tools_registry=tools_registry,model=\"gpt-4.1\")\n",
    "\n",
    "            file_path = create_test_file(test_dir, test_case.file_name, test_case.file_content)\n",
    "            # Run Agent\n",
    "            start_time = time.time()\n",
    "            agent_output = agent.run(test_case.input_query,max_iterations=10)\n",
    "            result[\"duration_seconds\"] = time.time() - start_time\n",
    "            result[\"agent_output\"] = agent_output\n",
    "\n",
    "            passed, reason = checker_func(file_path)\n",
    "            result[\"passed\"] = passed\n",
    "            result[\"reason\"] = reason\n",
    "        \n",
    "        except Exception as e:\n",
    "            result[\"error\"] = str(e)\n",
    "            result[\"reason\"] = f\"Exception during execution: {e}\"\n",
    "        \n",
    "        finally:\n",
    "            os.chdir(origininal_cwd)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "767132c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] AGENT_INIT: Agent initialized\n",
      "\n",
      "------------------------------------------------------------\n",
      "Step 1 of 10\n",
      "------------------------------------------------------------\n",
      "[INFO] THINK_START: Starting Reasoning\n",
      "\n",
      "Agent's LLM Response:\n",
      "{\"thought\": \"I've analyzed the task and created an execution plan.\", \"plan_created\": true, \"plan\": \"Step 1: Read the contents of sample.py to review its current implementation and identify issues.\\nStep 2: Patch sample.py to fix any identified errors or improve the code as necessary.\\nStep 3: Write or update a test file named test_sample.py to cover the functionality of sample.py.\\nStep 4: Run tests in test_sample.py to verify that sample.py is working correctly after modifications.\"}\n",
      "\n",
      "Plan Created:\n",
      "Step 1: Read the contents of sample.py to review its current implementation and identify issues.\n",
      "Step 2: Patch sample.py to fix any identified errors or improve the code as necessary.\n",
      "Step 3: Write or update a test file named test_sample.py to cover the functionality of sample.py.\n",
      "Step 4: Run tests in test_sample.py to verify that sample.py is working correctly after modifications.\n",
      "\n",
      "------------------------------------------------------------\n",
      "Step 2 of 10\n",
      "------------------------------------------------------------\n",
      "[INFO] THINK_START: Starting Reasoning\n",
      "[INFO] THINK_COMPLETE: Reasoning Complete\n",
      "\n",
      "Agent's LLM Response:\n",
      "{\"thought\": \"I need to read the contents of sample.py to review its current implementation and identify any issues.\", \"action\": {\"tool\": \"read_file\", \"args\": [\"sample.py\"]}}\n",
      "\n",
      "Thought: I need to read the contents of sample.py to review its current implementation and identify any issues.\n",
      "\n",
      "Action: read_file('sample.py')\n",
      "[INFO] ACT_START: Executing action\n",
      "[DEBUG] TOOL_CALL: Calling read_file\n",
      "[INFO] TOOL_COMPLETE: read_file completed\n",
      "\n",
      "Observation: \n",
      "def divide(a,b):\n",
      "    return a/b\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Step 3 of 10\n",
      "------------------------------------------------------------\n",
      "[INFO] THINK_START: Starting Reasoning\n",
      "[INFO] THINK_COMPLETE: Reasoning Complete\n",
      "\n",
      "Agent's LLM Response:\n",
      "{\"thought\": \"The function divides 'a' by 'b', but there's no check for division by zero. I should add error handling to avoid a ZeroDivisionError.\", \"action\": {\"tool\": \"patch_file\", \"args\": [\"sample.py\", \"def divide(a, b):\\n    if b == 0:\\n        raise ValueError('Division by zero is not allowed')\\n    return a / b\\n\"]}}\n",
      "\n",
      "Thought: The function divides 'a' by 'b', but there's no check for division by zero. I should add error handling to avoid a ZeroDivisionError.\n",
      "\n",
      "Action: patch_file('sample.py', \"def divide(a, b):\\n    if b == 0:\\n        raise ValueError('Division by zero is not allowed')\\n    return a / b\\n\")\n",
      "[INFO] ACT_START: Executing action\n",
      "[DEBUG] TOOL_CALL: Calling patch_file\n",
      "[INFO] TOOL_COMPLETE: patch_file completed\n",
      "\n",
      "Observation: File successfully updated: sample.py. New content written.\n",
      "\n",
      "------------------------------------------------------------\n",
      "Step 4 of 10\n",
      "------------------------------------------------------------\n",
      "[INFO] THINK_START: Starting Reasoning\n",
      "[INFO] THINK_COMPLETE: Reasoning Complete\n",
      "\n",
      "Agent's LLM Response:\n",
      "{\"thought\": \"The code in sample.py is now improved with error handling. Next, I need to write or update a test file named test_sample.py to cover divide's functionality, including division by zero.\", \"action\": {\"tool\": \"write_test\", \"args\": [\"test_sample.py\", \"import pytest\\nfrom sample import divide\\n\\ndef test_divide_normal_cases():\\n    assert divide(6, 3) == 2\\n    assert divide(-10, 2) == -5\\n    assert divide(0, 5) == 0\\n\\ndef test_divide_by_zero():\\n    with pytest.raises(ValueError, match='Division by zero is not allowed'):\\n        divide(1, 0)\\n\"]}}\n",
      "\n",
      "Thought: The code in sample.py is now improved with error handling. Next, I need to write or update a test file named test_sample.py to cover divide's functionality, including division by zero.\n",
      "\n",
      "Action: write_test('test_sample.py', \"import pytest\\nfrom sample import divide\\n\\ndef test_divide_normal_cases():\\n    assert divide(6, 3) == 2\\n    assert divide(-10, 2) == -5\\n    assert divide(0, 5) == 0\\n\\ndef test_divide_by_zero():\\n    with pytest.raises(ValueError, match='Division by zero is not allowed'):\\n        divide(1, 0)\\n\")\n",
      "[INFO] ACT_START: Executing action\n",
      "[DEBUG] TOOL_CALL: Calling write_test\n",
      "[INFO] TOOL_COMPLETE: write_test completed\n",
      "\n",
      "Observation: Test file created: test_sample.py\n",
      "\n",
      "------------------------------------------------------------\n",
      "Step 5 of 10\n",
      "------------------------------------------------------------\n",
      "[INFO] THINK_START: Starting Reasoning\n",
      "[INFO] THINK_COMPLETE: Reasoning Complete\n",
      "\n",
      "Agent's LLM Response:\n",
      "{\"thought\": \"The tests for sample.py have been created. Now I need to run the tests in test_sample.py to verify that the divide function works correctly and handles division by zero as expected.\", \"action\": {\"tool\": \"run_test\", \"args\": [\"test_sample.py\"]}}\n",
      "\n",
      "Thought: The tests for sample.py have been created. Now I need to run the tests in test_sample.py to verify that the divide function works correctly and handles division by zero as expected.\n",
      "\n",
      "Action: run_test('test_sample.py')\n",
      "[INFO] ACT_START: Executing action\n",
      "[DEBUG] TOOL_CALL: Calling run_test\n",
      "[INFO] TOOL_COMPLETE: run_test completed\n",
      "\n",
      "Observation: Exit code 0\n",
      "\n",
      "Ouput:\n",
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.12.3, pytest-9.0.2, pluggy-1.6.0 -- c:\\Users\\Asa\\AppData\\Local\\Programs\\Python\\Python312\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: C:\\Users\\Asa\\AppData\\Local\\Temp\\tmp_93aqdwe\n",
      "plugins: anyio-4.12.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "test_sample.py::test_divide_normal_cases \u001b[32mPASSED\u001b[0m\u001b[32m                          [ 50%]\u001b[0m\n",
      "test_sample.py::test_divid...\n",
      "\n",
      "------------------------------------------------------------\n",
      "Step 6 of 10\n",
      "------------------------------------------------------------\n",
      "[INFO] THINK_START: Starting Reasoning\n",
      "[INFO] THINK_COMPLETE: Reasoning Complete\n",
      "\n",
      "Agent's LLM Response:\n",
      "{\"thought\": \"All steps in the plan are complete and both tests have passed, verifying that divide works correctly and handles division by zero. No further action is necessary.\", \"answer\": \"Reviewed and improved sample.py to handle division by zero. Added tests in test_sample.py; all tests passed successfully.\"}\n",
      "\n",
      "Thought: All steps in the plan are complete and both tests have passed, verifying that divide works correctly and handles division by zero. No further action is necessary.\n",
      "\n",
      "Answer: Reviewed and improved sample.py to handle division by zero. Added tests in test_sample.py; all tests passed successfully.\n",
      "\n",
      "Progress: 4/4 steps completed\n",
      "\n",
      "ðŸ“Š Instrumentation saved:\n",
      "  - Traces: traces.json\n",
      "  - Logs: log.json\n",
      "  - Tokens: tokens.json\n",
      "  - Metrics: metrics.json\n",
      "\n",
      "ðŸ“ˆ Execution Summary\n",
      "  Cost: $0.0169\n",
      "  Tokens: 5,539\n",
      "  Tool calls: 4\n",
      "  Iterations: 6\n",
      "Test: division_zero_check\n",
      "Passed: True\n",
      "Duration: 14.94s\n",
      "Reason: Code now handles division by zero\n"
     ]
    }
   ],
   "source": [
    "registry = ToolRegistry()\n",
    "registry.register(\"read_file\",read_file)\n",
    "registry.register(\"patch_file\",patch_file)\n",
    "registry.register(\"print_review\",print_review)\n",
    "registry.register(\"write_test\",write_test)\n",
    "registry.register(\"run_test\",run_test)\n",
    "\n",
    "result = run_single_evaluation(CodeReviewAgentObservable,registry,first_test,check_division_fix)\n",
    "\n",
    "print(f\"Test: {result[\"test_name\"]}\")\n",
    "print(f\"Passed: {result[\"passed\"]}\")\n",
    "print(f\"Duration: {result[\"duration_seconds\"]:.2f}s\")\n",
    "print(f\"Reason: {result[\"reason\"]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb53532",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
