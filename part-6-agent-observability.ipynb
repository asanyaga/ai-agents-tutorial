{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b65e5bd1",
   "metadata": {},
   "source": [
    "# Observability and Debugging AI Agents\n",
    "As your AI Agent grows more sophisticated with components handling multi step plans, maintaining memory and using multiple tools, understanding what it's doing and why becomes important.\n",
    "Without prooper observability, debugging agent failures feels like operating in the dark\n",
    "\n",
    "I this tutorial, we will add instrumentation to our code review agent, covering:\n",
    "* **Structured logging** for every agent action\n",
    "* **Trace visualization** to understand thought -> tool -> result chains\n",
    "* **Token usage and cost tracking** for budget management\n",
    "* **Performance metrics** to identify bottlenecks\n",
    "* **Error detection** for loops and excessive tool usage\n",
    "\n",
    "By the end, you will have patterns for instrumenting any agent system - patterns that are similar to professional observability tools.\n",
    "\n",
    "## Why Observability Matters\n",
    "Consider this scenario: Your agent runs for 30 seconds, makes 8 tool calls and returns an incorrect answer or does not complete the task. *What went wrong?* \n",
    "With proper observability you can:\n",
    "* **Trace execution flow:** See the exact sequence of thoughts and actions\n",
    "* **Identify performance issues:** Find which tools are slow\n",
    "* **Track costs:** Know how many tokens each operation consumed\n",
    "* **Detect anomalies:** Catch infinite loops or repeated tool calls\n",
    "* **Debug in production** Understand real user interactions\n",
    "\n",
    "Let's build this starting with structured logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d287a08a",
   "metadata": {},
   "source": [
    "### Step 1: Structured Logging\n",
    "We want to replace `print()` statements with structured logs that capture rich metadata about every agent action.\n",
    "\n",
    "### Add a logging layer\n",
    "* **Timestamps:** Every log gets a UTC timestamp for analysis\n",
    "* **Event types:** Lets us categorize logs (e.g. \"TOOL_CALL\",\"LLM_REQUEST\") for filtering\n",
    "* **Metadata:** For context specific information\n",
    "* **Agent ID:** Identify agent instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af7d0882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "from typing import Any, Optional\n",
    "\n",
    "class LogLevel(Enum):\n",
    "    DEBUG = \"DEBUG\"\n",
    "    INFO = \"INFO\"\n",
    "    WARNING = \"WARNING\"\n",
    "    ERROR = \"ERROR\"\n",
    "\n",
    "class AgentLogger:\n",
    "    \"\"\"Structured logging for agent actions\"\"\"\n",
    "    def __init__(self, agent_id:str=\"agent-1\"):\n",
    "        self.agent_id = agent_id\n",
    "        self.logs = []\n",
    "\n",
    "    def log(self, level:LogLevel,event_type:str,message:str, metadata: Optional[dict[str,Any]]):\n",
    "        \"\"\"Create a structured log entry\"\"\"\n",
    "        log_entry = {\n",
    "            \"timestamp\": datetime.utcnow().isoformat(),\n",
    "            \"agent_id\": self.agent_id,\n",
    "            \"level\": level.value,\n",
    "            \"event_type\": event_type,\n",
    "            \"message\":message,\n",
    "            \"metadata\": metadata or {}\n",
    "        }\n",
    "        self.logs.append(log_entry)\n",
    "\n",
    "        # Also print for real-time feedback\n",
    "        print(f\"[{level.value}] {event_type}:{message}\")\n",
    "    \n",
    "    def get_logs(self, event_type:Optional[str] = None) -> list:\n",
    "        \"\"\"Retrieve logs optionally filtered by event type\"\"\"\n",
    "        if event_type:\n",
    "            return [log for log in self.logs if log[\"event_type\"]==event_type]\n",
    "        \n",
    "        return self.logs\n",
    "\n",
    "    def save_logs(self,file_path:str):\n",
    "        \"\"\"Persist logs to a JSON file\"\"\"\n",
    "        with open(file_path,\"w\") as f:\n",
    "            json.dump(self.logs,f,indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0567b160",
   "metadata": {},
   "source": [
    "### Integrating logging into the Agent\n",
    "* Add logging to key methods\n",
    "    - `__init__()`\n",
    "    - `think()`\n",
    "    - `act()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedd4cfe",
   "metadata": {},
   "source": [
    "## Step 2: Trace Hierarchies\n",
    "Logs are flat, they dont show *relationships* between operation. A trace captures the nested structure of agent execution.\n",
    "\n",
    "### Building a Trace Structure\n",
    "* **Spans:** Individual units of work\n",
    "* **Hierarchy:** Child spans nest under parents to show causality\n",
    "* **Context propagation:** `current_span_id` tracks where we are in the call stack\n",
    "* **Lazy evaluation:** Only root spans are saved to `traces`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec865034",
   "metadata": {},
   "source": [
    "### Integrating tracing into the Agent\n",
    "* Add create tracer during initialization\n",
    "* Add tracing to key methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7feededf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import uuid\n",
    "\n",
    "class Span:\n",
    "    \"\"\"Represents a single unit of work in a trace\"\"\"\n",
    "\n",
    "    def __init__(self, name:str,span_type:str,parent_id:Optional[str]=None):\n",
    "        self.span_id = str(uuid.uuid4())[:8]\n",
    "        self.parent_id = parent_id\n",
    "        self.name = name\n",
    "        self.span_type = span_type\n",
    "        self.start_time = time.time()\n",
    "        self.end_time = None\n",
    "        self.status = \"running\"\n",
    "        self.metadata = {}\n",
    "        self.children : list[Span] =[]\n",
    "\n",
    "    def end(self,status:str=\"success\",metadata:Optional[dict]=None):\n",
    "        \"\"\"Mark span as complete\"\"\"\n",
    "        self.end_time= time.time()\n",
    "        self.status = status\n",
    "        if metadata:\n",
    "            self.metadata.update(metadata)\n",
    "    \n",
    "    def duration_ms(self) -> float:\n",
    "        \"\"\"Calculate span duration in milliseconds\"\"\"\n",
    "        if self.end_time:\n",
    "            return (self.end_time - self.start_time) * 1000\n",
    "        return (time.time() - self.start_time) * 1000\n",
    "    \n",
    "    def add_child(self, child:'Span'):\n",
    "        \"\"\"Add child span\"\"\"\n",
    "        self.children.append(child)\n",
    "\n",
    "    def to_dict(self) -> dict:\n",
    "        \"\"\"Convert span to dict for serializatioin\"\"\"\n",
    "        return {\n",
    "            \"span_id\": self.span_id,\n",
    "            \"parent_id\": self.parent_id,\n",
    "            \"name\": self.name,\n",
    "            \"type\": self.span_type,\n",
    "            \"start_time\": self.start_time,\n",
    "            \"end_time\":self.end_time,\n",
    "            \"duration_ms\": self.duration_ms(),\n",
    "            \"Status\":self.status,\n",
    "            \"metadata\": self.metadata,\n",
    "            \"children\":[child.to_dict() for child in self.children]\n",
    "        }\n",
    "\n",
    "class TraceManager:\n",
    "    \"\"\"Manages execution traces\"\"\"\n",
    "    def __init__(self):\n",
    "        self.traces = []\n",
    "        self.active_spans = {}\n",
    "        self.current_span_id = None\n",
    "    \n",
    "    def start_span(self, name:str, span_type: str) -> str:\n",
    "        \"\"\"Create and activate a new span\"\"\"\n",
    "        parent_id = self.current_span_id\n",
    "        span = Span(name, span_type,parent_id)\n",
    "        self.active_spans[span.span_id] = span\n",
    "\n",
    "        if parent_id and parent_id in self.active_spans:\n",
    "            self.active_spans[parent_id].add_child(span)\n",
    "        \n",
    "        self.current_span_id = span.span_id\n",
    "        return span.span_id\n",
    "    \n",
    "    def end_span(self, span_id:str, status: str = \"success\",metadata: Optional[dict] = None):\n",
    "        \"\"\"Complete a span and update current span\"\"\"\n",
    "        if span_id in self.active_spans:\n",
    "            span = self.active_spans[span_id]\n",
    "            span.end(status, metadata)\n",
    "\n",
    "            # Move current span to parent\n",
    "            if span.parent_id:\n",
    "                self.current_span_id = span.parent_id\n",
    "            else:\n",
    "                # Root span completed - save trace\n",
    "                self.traces.append(span)\n",
    "                self.current_span_id = None\n",
    "\n",
    "    def get_current_span(self) -> Optional[Span]:\n",
    "        \"\"\"Get the currently active span\"\"\"\n",
    "        if self.current_span_id:\n",
    "            return self.active_spans.get(self.current_span_id)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def save_traces(self,file_path:str):\n",
    "        \"\"\"Save all traces to a file\"\"\"\n",
    "        traces_data = [trace.to_dict() for trace in self.traces]\n",
    "        with open(file_path,\"w\") as f:\n",
    "            json.dump(traces_data,f,indent=2)\n",
    "            \n",
    "class TraceVisualizer:\n",
    "    \"\"\"Generate human readable trace vizualizations\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def format_trace(span: dict, indent: int = 0) -> str:\n",
    "        \"\"\"receursively format a trace and it's children\"\"\"\n",
    "        prefix = \"  \" * indent\n",
    "        \n",
    "        duration = span[\"duration_ms\"]\n",
    "        duration_str = f\"{duration:.0f}ms\"\n",
    "\n",
    "        status_icon = \"☑️\" if span[\"staus\"] == \"SUCCESS\" else \"❌\"\n",
    "\n",
    "        # Build line\n",
    "        line = f\"{prefix}{status_icon} {span[\"name\"] ({span[\"type\"]}) - {duration_str}}\"\n",
    "\n",
    "        if span.get(\"metadata\"):\n",
    "            metadata = span[\"metadata\"]\n",
    "            if \"cost_usd\" in metadata:\n",
    "                line += f\"[${metadata[\"cost_usd\"]:.4f}]\"\n",
    "            if \"error\" in metadata:\n",
    "                line += f\" [ERROR: {metadata[\"error\"]}]\"\n",
    "\n",
    "        lines = [line]\n",
    "\n",
    "        # Recusrively format children\n",
    "        for child in span.get(\"children\", []):\n",
    "            lines.append(TraceVisualizer.format_trace(child,indent + 1))\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "    \n",
    "    @staticmethod\n",
    "    def print_all_traces(traces: List[dict]):\n",
    "        \"\"\"Print all traces in a printable format\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"EXECUTION TRACES\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        for i, trace in enumerate(traces, 1):\n",
    "            print(f\"Trace {i}:\")\n",
    "            print(TraceVisualizer.format_trace(trace))\n",
    "            \n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9456c4b6",
   "metadata": {},
   "source": [
    "## Token Usage and Cost Tracking\n",
    "LLM costs can add up quickly, Let's track token usage and estimate costs per operation\n",
    "\n",
    "### Token Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35aff225",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenTracker:\n",
    "    \"\"\"Track token usage and estimate costs\"\"\"\n",
    "\n",
    "    # Pricing per 1M tokens \n",
    "    PRICING = {\n",
    "        \"gpt-4.1\": {\"input\":2.50,\"output\":10.00},\n",
    "        \"gpt-4.1-mini\":{\"input\":0.15,\"output\":0.60}\n",
    "    }\n",
    "\n",
    "    def __init__(self,model:str):\n",
    "        self.model = model\n",
    "        self.total_input_tokens = 0\n",
    "        self.total_output_tokens = 0\n",
    "        self.call_count = 0\n",
    "        self.token_log = []\n",
    "    \n",
    "    def track_usage(self, input_tokens: int, output_tokens: int,operation:str = \"llm_call\"):\n",
    "        self.total_input_tokens += input_tokens\n",
    "        self.total_output_tokens += output_tokens\n",
    "        self.call_count += 1\n",
    "\n",
    "        entry = {\n",
    "            \"timestamp\": datetime.utcnow().isoformat(),\n",
    "            \"operation\": operation,\n",
    "            \"input_tokens\": input_tokens,\n",
    "            \"output_tokens\": output_tokens,\n",
    "            \"cost_usd\": self._calculate_cost(input_tokens,output_tokens)\n",
    "        }\n",
    "        self.token_log.append(entry)\n",
    "    \n",
    "    def _calculate_cost(self,input_tokens:int, output_tokens:int) -> float:\n",
    "        \"\"\"Calculate cost in USD\"\"\"\n",
    "        if self.model not in self.PRICING:\n",
    "            return 0.0\n",
    "        pricing = self.PRICING[self.model]\n",
    "        input_cost = (input_tokens/1000000) * pricing[\"input\"]\n",
    "        output_cost = (output_tokens/1000000) * pricing[\"output\"]\n",
    "        return input_cost + output_cost\n",
    "    \n",
    "    def get_summary(self) -> dict:\n",
    "        \"\"\"Get usage summary\"\"\"\n",
    "        return {\n",
    "            \"model\":self.model,\n",
    "            \"total_call\": self.call_count,\n",
    "            \"total_input_tokens\": self.total_input_tokens,\n",
    "            \"total_output_tokens\": self.total_output_tokens,\n",
    "            \"estimated_cost_usd\": self._calculate_cost(self.total_input_tokens,self.total_output_tokens)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f332eb",
   "metadata": {},
   "source": [
    "### Performance Metrics and Anomaly Detection\n",
    "Track metrics to identify performance issues and agent misbehaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdbbc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "class MetricsCollector:\n",
    "    \"\"\"Collect and analyze performance metrics\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            \"iteration_count\": 0,\n",
    "            \"tool_calls\": {}, # tool_name: count\n",
    "            \"tool_latencies\":{}, # tool_name: durations\n",
    "            \"llm_latencies\": [],\n",
    "            \"errors\": [],\n",
    "            \"loop_detection\": [] # Track repeated tool calls\n",
    "        }\n",
    "        self.last_n_tools = [] # Sliding window for loop detection\n",
    "    \n",
    "    def record_iteration(self):\n",
    "        \"\"\"Increment iteration counter\"\"\"\n",
    "        self.metrics[\"iteration_count\"] += 1\n",
    "    \n",
    "    def record_tool_call(self, tool_name: str, duration_ms: float):\n",
    "        \"\"\"Record a tool invocation\"\"\"\n",
    "        if tool_name not in self.metrics[\"tool_calls\"]:\n",
    "            self.metrics[\"tool_calls\"][tool_name] = 0\n",
    "            self.metrics[\"tool_latencies\"][tool_name] = []\n",
    "        \n",
    "        self.metrics[\"tool_calls\"][tool_name] += 1\n",
    "        self.metrics[\"tool_latencies\"][tool_name].append(duration_ms)\n",
    "\n",
    "        # Loop detection: track last 5 tool calls\n",
    "        self.last_n_tools.append(tool_name)\n",
    "        if len(self.last_n_tools) > 5:\n",
    "            self.last_n_tools.pop(0)\n",
    "        \n",
    "        #Check for repeated patterns\n",
    "        if len(self.last_n_tools) == 5:\n",
    "            if len(set(self.last_n_tools)) >=2: # 1 or 2 two unique tool calls in last 5\n",
    "                self.metrics[\"loop_detection\"].append({\n",
    "                    \"iteration\": self.metrics[\"iteration_count\"],\n",
    "                    \"pattern\": self.last_n_tools.copy()\n",
    "                })\n",
    "    \n",
    "    def record_llm_latency(self, duration_ms: float):\n",
    "        \"\"\"Record LLM call duration\"\"\"\n",
    "        self.metrics[\"llm_latencies\"].append(duration_ms)\n",
    "    \n",
    "    def record_error(self, error_type:str, details:str):\n",
    "        \"\"\"Record an error\"\"\"\n",
    "        self.metrics[\"errors\"].append(\n",
    "            {\n",
    "                \"timestamp\": datetime.utcnow().isoformat(),\n",
    "                \"type\": error_type,\n",
    "                \"details\": details\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def get_summary(self) -> dict:\n",
    "        \"\"\"Generate metrics summary\"\"\"\n",
    "\n",
    "        summary = {\n",
    "            \"total_iterations\": self.metrics[\"iteration_count\"],\n",
    "            \"total_tool_calls\": sum(self.metrics[\"tool_calls\"].values()),\n",
    "            \"tool_usage\": self.metrics[\"tool_calls\"],\n",
    "            \"error_count\": len(self.metrics[\"errors\"]),\n",
    "            \"potential_loops\": len(self.metrics[\"loop_detection\"])\n",
    "        }\n",
    "\n",
    "        # Calculate average latencies\n",
    "        if self.metrics[\"llm_latencies\"]:\n",
    "            summary[\"avg_llm_latency_ms\"] = sum(self.metrics[\"llm_latencies\"]) /len(self.metrics[\"llm_latencies\"])\n",
    "\n",
    "        summary[\"tool_avg_latencies\"] = {}\n",
    "        for tool, latencies in self.metrics[\"tool_latencies\"].items():\n",
    "            if latencies:\n",
    "                summary[\"tool_avg_latencies\"][tool] =sum(latencies) / len(latencies)\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def check_anomalies(self) -> List[str]  :\n",
    "        \"\"\"Detect anomalous behaviour\"\"\"\n",
    "\n",
    "        wanrings = []\n",
    "        # Check for excessive iterations\n",
    "        if self.metrics[\"iteration_count\"] > 15:\n",
    "            wanrings.append(f\" High iteration count: {self.metrics[\"iteration_count\"]}\")\n",
    "        \n",
    "        # Check for tool call loops\n",
    "        if self.metrics[\"loop_detection\"]:\n",
    "            wanrings.append(f\"Possible loop detected: {len(self.metrics[\"loop_detection\"])} instances\")\n",
    "        # Check for excessive errors\n",
    "        if len(self.metrics[\"errors\"]) > 3:\n",
    "            wanrings.append(f\"Muliple errors: {len(self.metrics[\"errors\"])}\")\n",
    "        \n",
    "        # Check for slow operations\n",
    "        if self.metrics[\"llm_latencies\"]:\n",
    "            avg_llm = sum(self.metrics[\"llm_latencies\"])/len(self.metrics[\"llm_latencies\"])\n",
    "            if avg_llm > 2000:\n",
    "                wanrings.append(f\"Slow LLM calls: avg {avg_llm:.0f}ms\")\n",
    "        \n",
    "        return wanrings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109a01cd",
   "metadata": {},
   "source": [
    "### Integrating Metrics\n",
    "Add the metrics collector and update instrumented methods\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa7a5c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict\n",
    "import openai\n",
    "import os\n",
    "\n",
    "\n",
    "## Set up the tools and tools registry\n",
    "def write_test(file_path:str, test_code: str) -> str:\n",
    "    \"\"\"Write test code to a test file\"\"\"\n",
    "    try:\n",
    "        test_dir = os.path.dirname(file_path) or \"tests\"\n",
    "        if not os.path.exists(test_dir):\n",
    "            os.makedirs(test_dir)\n",
    "\n",
    "        with open(file_path, \"w\") as f:\n",
    "            f.write(test_code)\n",
    "        return f\"Test file created: {file_path}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error writing test file {file_path: {e}}\"\n",
    "\n",
    "def run_test(file_path: str) -> str:\n",
    "    \"\"\"Run a Python test file and return results\"\"\"\n",
    "    try:\n",
    "        import subprocess\n",
    "        result = subprocess.run(\n",
    "            [\"python\",\"-m\",\"pytest\", file_path,\"-v\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=30\n",
    "        )\n",
    "        return f\"Exit code {result.returncode}\\n\\nOuput:\\n{result.stdout}\\n\\nErrors:\\n{result.stderr}\"\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return \"Test execution timed out after 30 seconds\"\n",
    "    except Exception as e:\n",
    "        return f\"Error running tests: {e}\"\n",
    "\n",
    "def read_file(file_path: str) -> str:\n",
    "    \"\"\"Read contents of a Python file\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        return f\"File not found: {file_path}\"\n",
    "    \n",
    "    with open(file_path, \"r\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def analyze_code(code: str) -> str:\n",
    "    \"\"\"Ask an LLM to analyze the provided code.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful code review assistant.\n",
    "    Analyze the following Python code and suggest one improvement.\n",
    "\n",
    "    Code:\n",
    "    {code}\n",
    "    \"\"\"\n",
    "\n",
    "    response = openai.responses.create(model=\"gpt-4.1-mini\",input=[{\"role\":\"user\",\"content\":prompt}])\n",
    "\n",
    "    return response.output_text\n",
    "\n",
    "def patch_file(filepath: str, content: str) -> str:\n",
    "    \"\"\"Writes the given content to a file, completely replacing its current content.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, \"w\") as f:\n",
    "            f.write(content)\n",
    "        return f\"File successfully updated: {filepath}. New content written.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error writing to file {filepath}: {e}\"\n",
    "        \n",
    "class ToolRegistry:\n",
    "    \"\"\"Holds available tools and dispatches them by name.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.tools: Dict[str,Callable] = {}\n",
    "    \n",
    "    def register(self, name:str, func: Callable):\n",
    "        self.tools[name] = func\n",
    "\n",
    "    def call(self, name:str, *args, **kwargs):\n",
    "        if name not in self.tools:\n",
    "            return f\"Unknown tool: {name}\"\n",
    "        return self.tools[name](*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14da14c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import json\n",
    "\n",
    "class CodeReviewAgentPlanning:\n",
    "    def __init__(self,tools_registry: ToolRegistry, model=\"gpt-4.1\",memory_file=\"agent_memory.json\",summarize_after=10,max_context_tokens=6000):\n",
    "        self.tools = tools_registry\n",
    "        self.model = model\n",
    "        self.conversation_history = [] # Short-term memory\n",
    "        self.memory_file = memory_file\n",
    "        self.load_long_term_memory() # Long-term memory (key-value store)\n",
    "        self.conversation_summary = \"\" # Summarized conversation history\n",
    "        self.summarize_after = summarize_after\n",
    "        self.turns_since_summary = 0\n",
    "        self.max_context_tokens = max_context_tokens\n",
    "        self.current_plan = [] #List of planned steps\n",
    "        self.completed_steps = [] # Track what has been done\n",
    "        self.plan_created = False\n",
    "\n",
    "        # Add logger\n",
    "        self.logger = AgentLogger(agent_id=f\"code-reivew-{int(time.time())}\")\n",
    "        self.logger.log(LogLevel.INFO,\"AGENT_INIT\",\"Agent initialized\",{\"model\":model,\"max_token\":max_context_tokens})\n",
    "        self.tracer = TraceManager()\n",
    "        self.token_tracker = TokenTracker(model=model)\n",
    "        self.metrics = MetricsCollector()\n",
    "\n",
    "        # Initialize tokenizer for the model\n",
    "        try:\n",
    "            self.tokenizer = tiktoken.encoding_for_model(model)\n",
    "        except:\n",
    "            self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    def count_tokens(self, text:str) -> int:\n",
    "        \"\"\"Count tokens in a string\"\"\"\n",
    "        return len(self.tokenizer.encode(text))\n",
    "    \n",
    "    def trim_history_to_fit(self, system_message:str):\n",
    "        \"\"\"Remove old messages until we fit within the token budget\"\"\"\n",
    "\n",
    "        # Count tokens in system message\n",
    "        fixed_tokens = self.count_tokens(system_message)\n",
    "\n",
    "        # Count tokens in conversation history\n",
    "        history_tokens = sum([self.count_tokens(msg[\"content\"]) for msg in self.conversation_history])\n",
    "\n",
    "        total_tokens = fixed_tokens + history_tokens\n",
    "\n",
    "        while total_tokens > self.max_context_tokens and len(self.conversation_history) > 2:\n",
    "            removed_msg = self.conversation_history.pop(0)\n",
    "            total_tokens -= self.count_tokens(removed_msg[\"content\"])\n",
    "\n",
    "        return total_tokens\n",
    "\n",
    "\n",
    "    def summarize_history(self):\n",
    "        \"\"\"Use LLM to summarize the conversation so far.\"\"\"\n",
    "        if len(self.conversation_history) < 3:\n",
    "            return\n",
    "        \n",
    "        history_text = \"\\n\".join([f\"{msg[\"role\"]}:{msg[\"content\"]}\" for msg in self.conversation_history])\n",
    "\n",
    "        summary_prompt = f\"\"\"Summarize this conversation in 3-4 sentences,\n",
    "        preserving key fact, decisions, and actions taken:\n",
    "        {history_text}\n",
    "\n",
    "        Previous Summary: {self.conversation_summary or 'None'}\n",
    "        \"\"\"\n",
    "\n",
    "        response = openai.responses.create(model=self.model, input=[{\"role\":\"user\",\"content\":summary_prompt}])\n",
    "\n",
    "        self.conversation_summary = response.output_text\n",
    "\n",
    "        # Keep only the last few turns + the summary\n",
    "        recent_turns = self.conversation_history[-4:] # Keep the last 4 messages (2 user/assistant exchanges)\n",
    "\n",
    "        self.conversation_history = recent_turns\n",
    "        self.turns_since_summary = 0\n",
    "\n",
    "\n",
    "    def remember(self, key:str, value: str):\n",
    "        \"\"\"Retrieve information from long term memory.\"\"\"\n",
    "        self.long_term_memory[key] = value\n",
    "        self.save_long_term_memory()\n",
    "    \n",
    "    def recall(self,key:str) -> str:\n",
    "        \"\"\"Retrieve information from long term memory\"\"\"\n",
    "        return self.long_term_memory.get(key,\"No memory found for this key.\")\n",
    "    \n",
    "    def get_relevant_memories(self) -> str:\n",
    "        \"\"\"Format long term memories for inclusion in prompts.\"\"\"\n",
    "        if not self.long_term_memory:\n",
    "            return \"No stored memories\"\n",
    "        \n",
    "        memories = \"\\n\".join([f\"- {k}:{v}\" for k, v in self.long_term_memory.items()])\n",
    "        return f\"Relevant memories:\\n{memories}\"\n",
    "    \n",
    "    def save_long_term_memory(self):\n",
    "        \"\"\"Persist long term memory to JSON file\"\"\"\n",
    "        try:\n",
    "            with open(self.memory_file,\"w\") as f:\n",
    "                json.dump(self.long_term_memory,f,indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save memory to {self.memory_file}:  {e}\")\n",
    "\n",
    "    def load_long_term_memory(self):\n",
    "        \"\"\"Load long term memory from JSON file\"\"\"\n",
    "        if os.path.exists(self.memory_file):\n",
    "            try:\n",
    "                with open(self.memory_file, 'r') as f:\n",
    "                    self.long_term_memory = json.load(f)\n",
    "                print(f\"Loaded {len(self.long_term_memory)} memories from {self.memory_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load memory from {self.memory_file}: {e}\")\n",
    "        else:\n",
    "            self.long_term_memory = {}\n",
    "    \n",
    "    def create_plan(self, user_query:str) -> list:\n",
    "        \"\"\"Generate a step by step plan for the user's request\"\"\"\n",
    "        planning_prompt = f\"\"\"\n",
    "        Given this task:\"\"{user_query}\"\"\n",
    "        Create a detailed execution plan with numbered steps. Each step should be a specific action\n",
    "\n",
    "        Available tools:\n",
    "        - read_file(file): Read a file's contents\n",
    "        - analyze_code(code): Get code analysis and suggestions\n",
    "        - patch_file(file_path, content): Update a file\n",
    "        - write_test(file_path, text_code): Create a test file\n",
    "        - run_test(file_path): Execute tests\n",
    "\n",
    "        Format your response as a JSON list of steps\n",
    "        [\n",
    "        {{\"step\":1,\"action\":\"description\",\"tool\":\"tool_name\"}},\n",
    "        {{\"step\":1,\"action\":\"description\",\"tool\":\"tool_name\"}}\n",
    "        ]\n",
    "\n",
    "        Only include necessary steps. Be specific about which files to work with.\n",
    "        \"\"\"\n",
    "\n",
    "        resposnse = openai.responses.create(model=self.model,\n",
    "                                            input=[{\"role\":\"user\",\"content\":planning_prompt}])\n",
    "        \n",
    "        try:\n",
    "            plan = json.loads(resposnse.output_text)\n",
    "            self.current_plan = plan\n",
    "            self.plan_created = True\n",
    "            return plan\n",
    "        except json.JSONDecodeError:\n",
    "            self.current_plan = [{\"step\":1,\"action\":\"Proceed step by step\",\"tool\":\"analyze_code\"}]\n",
    "            self.plan_created= True\n",
    "            return self.current_plan\n",
    "    \n",
    "    def _build_plan_context(self,next_step) -> str:\n",
    "        \"\"\"Format plan information for the prompt\"\"\"\n",
    "        completed = \"\\n\".join([f\"Step {step[\"step\"]}:{step[\"action\"]}\" for step in self.completed_steps])\n",
    "\n",
    "        if next_step:\n",
    "            current = f\"\\nCURRENT: Step {next_step[\"step\"]}: {next_step[\"action\"]}\"\n",
    "        else:\n",
    "            current = \"\\n All steps completed\"\n",
    "        \n",
    "        remaining = \"\\n\".join([f\" Step {step[\"step\"]}: {step[\"action\"]}\" for step in self.current_plan[len(self.completed_steps)+1:]])\n",
    "\n",
    "        execution_plan = f\"\"\"\n",
    "        Completed:\n",
    "        {completed if completed else \"None\"}\n",
    "        {current}\n",
    "        Remaining:\n",
    "        {remaining if remaining else \"None\"}\n",
    "        \"\"\"\n",
    "\n",
    "        return execution_plan\n",
    "\n",
    "    def think(self, user_input:str):\n",
    "        \"\"\"LLM enhanced thinking with logging and tracing\"\"\"\n",
    "        self.logger.log(LogLevel.INFO,\"THINK_START\",\"Starting Reasoning\",{\"user_input\":user_input[:100]})\n",
    "        think_span_id = self.tracer.start_span(\"Think\",\"LLM_CALL\")\n",
    "\n",
    "        \n",
    "        # First request: create a plan\n",
    "        if not self.plan_created:\n",
    "            plan = self.create_plan(user_query=user_input)\n",
    "\n",
    "            plan_summary = \"\\n\".join([f\"Step {step[\"step\"]}:{step[\"action\"]}\" for step in plan])\n",
    "\n",
    "            response = f\"\"\"\n",
    "            I have created this execution plan:\n",
    "            {plan_summary}\n",
    "            \n",
    "            I will now begin executing these steps\n",
    "            \"\"\"\n",
    "\n",
    "            return response\n",
    "\n",
    "        # Add user message to history\n",
    "        self.conversation_history.append({\"role\":\"user\",\"content\":user_input})\n",
    "\n",
    "        self.turns_since_summary += 1\n",
    "\n",
    "        # Check if we should summarize\n",
    "        if self.turns_since_summary >= self.summarize_after:\n",
    "            self.summarize_history()\n",
    "\n",
    "        # Get current step from plan\n",
    "        next_step = None\n",
    "        if len(self.completed_steps)<len(self.current_plan):\n",
    "            next_step = self.current_plan[len(self.completed_steps)]\n",
    "        \n",
    "        # Build context with plan information\n",
    "        plan_context = self._build_plan_context(next_step)\n",
    "\n",
    "\n",
    "        #Include long term memory & summary in system context\n",
    "        system_message_context = f\"\"\"You are a code assistant with access to these tools:\n",
    "                - read_file(filepath)\n",
    "                - analyze_code(code)\n",
    "                - patch_file(filepath,content)\n",
    "                - write_test(file_path,test_code)\n",
    "                - run_test(file_path)\n",
    "\n",
    "                {self.get_relevant_memories()}\n",
    "\n",
    "                Conversation Summary: {self.conversation_summary or 'This is the start of the conversation'}\n",
    "\n",
    "                {plan_context}\n",
    "\n",
    "                Follow the ReAct pattern: **Thought**, then **Action** or a final **Answer**\n",
    "                **Format your response STRICTLY as follows:**\n",
    "\n",
    "                1. Thought:Your internal reasoning and plan.\n",
    "                2. Action:The tool call to make in JSON format {{\"tool\": \"tool_name\", \"args\": [\"arg1\", \"arg2\"]}} (e.g., {{\"tool\":\"patch_file\", \"args\":[\"file_path\",\"content\"]}}. **OR**\n",
    "                3. Answer:Your final response when all steps are complete.\n",
    "\n",
    "\n",
    "\n",
    "                After each successful action I'll mark that step as complete and move to the next one\n",
    "\n",
    "                \"\"\"\n",
    "\n",
    "        self.trim_history_to_fit(system_message_context)\n",
    "        \n",
    "        # Build prompt with system instructions\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\":\"system\",\n",
    "                \"content\":system_message_context\n",
    "            }\n",
    "        ] + self.conversation_history\n",
    "\n",
    "\n",
    "        input_text = json.dumps([msg[\"content\"] for msg in messages])\n",
    "        input_tokens = self.count_tokens(input_text)\n",
    "\n",
    "        start_time = time.time()\n",
    "        response = openai.responses.create(model=self.model, input=messages)\n",
    "\n",
    "        duration_ms = (time.time() - start_time) * 1000\n",
    "\n",
    "        decision = response.output_text\n",
    "\n",
    "        # Add assistant's decision to conversation history\n",
    "        self.conversation_history.append({\n",
    "            \"role\":\"assistant\",\n",
    "            \"content\": decision\n",
    "        })\n",
    "\n",
    "        self.tracer.end_span(think_span_id,\"SUCCESS\",{\"messages\":messages,\"decision\":decision})\n",
    "        self.logger.log(LogLevel.INFO,\"THINK_COMPLETE\",\"Reasoning Complete\",{\"decision\":decision})\n",
    "        output_tokens = self.count_tokens(decision)\n",
    "        self.token_tracker.track_usage(input_tokens,output_tokens,\"think\")\n",
    "        self.metrics.record_llm_latency(duration_ms)\n",
    "\n",
    "        return decision\n",
    "    \n",
    "    def act(self, decision:str):\n",
    "        \"\"\"Execute the chosen tool and update plan progress with logging\"\"\"\n",
    "\n",
    "        self.logger.log(LogLevel.INFO,\"ACT_START\",\"Executing action\",{\"decision\":decision})\n",
    "        act_span_id = self.tracer.start_span(\"Act\",\"TOOL_EXECUTION\")\n",
    "\n",
    "        try:\n",
    "            parsed = json.loads(decision)\n",
    "            tool_name = parsed[\"tool\"]\n",
    "            args = parsed.get(\"args\",[])\n",
    "\n",
    "\n",
    "            self.logger.log(LogLevel.DEBUG,\"TOOL_CALL\",f\"Calling {tool_name}\",{\"tool\":tool_name,\"args\":args})\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            result = self.tools.call(tool_name,*args)\n",
    "\n",
    "            duration = time.time() - start_time\n",
    "\n",
    "            self.logger.log(LogLevel.INFO,\"TOOL_COMPLETE\",f\"{tool_name} completed\", {\"tool\":tool_name,\"duration_ms\":duration*1000})\n",
    "            self.tracer.end_span(act_span_id,\"SUCCESS\")\n",
    "            self.metrics.record_tool_call(tool_name,duration_ms=duration*1000)\n",
    "\n",
    "            #Mark current step as complete\n",
    "            if len(self.completed_steps) < len(self.current_plan):\n",
    "                current_step = self.current_plan[len(self.completed_steps)]\n",
    "                self.completed_steps.append(current_step)\n",
    "\n",
    "            self.conversation_history.append({\"role\":\"system\",\"content\":result})\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error executing tool: {e}\"\n",
    "            self.logger.log(LogLevel.ERROR,\"ACT_ERROR\",error_msg,{\"decision:\":decision,\"error\":str(e)})\n",
    "            self.tracer.end_span(act_span_id,\"ERROR\",{\"error\":str(e)})\n",
    "            self.conversation_history.append({\n",
    "                \"role\":\"system\",\n",
    "                \"content\": error_msg\n",
    "            })\n",
    "            return error_msg\n",
    "\n",
    "    def run(self, user_query:str, max_iterations=10):\n",
    "        \"\"\"\n",
    "        Main execution loop with reflection.\n",
    "        Args:\n",
    "            user_query: The user's request\n",
    "            max_iterations: Maxumum number of think-act-reflect cycles\n",
    "        \n",
    "        Returns:\n",
    "            Final response string\n",
    "        \"\"\"\n",
    "        run_span_id = self.tracer.start_span(f\"Agent Run: {user_query[:50]}\",span_type=\"AGENT_RUN\")\n",
    "\n",
    "        try:\n",
    "            step = 0\n",
    "\n",
    "            current_input = user_query\n",
    "\n",
    "            while step < max_iterations:\n",
    "                #Create a span for each iteration\n",
    "                iter_span_id = self.tracer.start_span(name=f\"Iteration {step + 1}\",span_type=\"ITERATION\")\n",
    "                self.metrics.record_iteration()\n",
    "\n",
    "                print(f\"\\n--- Step {step+1} ---\")\n",
    "\n",
    "                llm_response = self.think(current_input)\n",
    "\n",
    "                print(f\"Agent's LLM Response:\\n{llm_response}\")\n",
    "\n",
    "                #Check if the response is the plan. If it is go to the first step\n",
    "                if \"I have created this execution plan\" in llm_response:\n",
    "                    current_input = \"Proceed with step 1\"\n",
    "                    step +=1\n",
    "                    continue\n",
    "\n",
    "                if \"Answer:\" in llm_response:\n",
    "                    final_answer = llm_response.split(\"Answer:\",1)[1].strip()\n",
    "\n",
    "                    self.tracer.end_span(iter_span_id,\"SUCCESS\",{\"final_answer\":final_answer[:100]})\n",
    "                    self.tracer.end_span(run_span_id,\"SUCCESS\",{\"total_iterations\": step +1})\n",
    "\n",
    "                    # print(f\"\\n Agent Finished: \\n {final_answer}\")\n",
    "                    return final_answer\n",
    "                if \"Action:\" in llm_response:\n",
    "                    action_line = llm_response.split(\"Action:\",1)[1].split(\"\\n\")[0].strip()\n",
    "                    print(f\"Acting: {action_line}\")\n",
    "\n",
    "                    tool_result = self.act(action_line)\n",
    "\n",
    "                    print(f\"\\nTool Result:\\n{tool_result}\")\n",
    "                    current_input = f\"Observation:{tool_result}\"\n",
    "                else:\n",
    "                    error_msg = f\"LLM did not provide valid Action or Answer: LLM Respose:: {llm_response}\"\n",
    "                    print(f\"\\n Error: {error_msg}\")\n",
    "                    return error_msg\n",
    "                \n",
    "                self.tracer.end_span(iter_span_id,\"SUCCESS\")\n",
    "                step +=1\n",
    "\n",
    "            self.tracer.end_span(run_span_id,\"MAX_ITERATIONS\",{\"completed_steps\":len(self.completed_steps),\"total_steps\":len(self.current_plan)})\n",
    "            # Check for anomalies at the end\n",
    "            warnings = self.metrics.check_anomalies()\n",
    "            if warnings:\n",
    "                print(\"\\n Performance Warnings:\") \n",
    "                for warning in warnings:\n",
    "                    print(f\"    {warning}\")\n",
    "\n",
    "            return \"Task Incomplete: Max steps reached\"\n",
    "        except Exception as e:\n",
    "            self.tracer.end_span(run_span_id,\"ERROR\",{\"error\":str(e)})\n",
    "            raise\n",
    "\n",
    "    def save_instrumentation(self, trace_file=\"traces.json\",log_file=\"log.json\",token_file=\"tokens.json\",metrics_file=\"metrics.json\"):\n",
    "        self.tracer.save_traces(trace_file)\n",
    "        self.logger.save_logs(log_file)\n",
    "\n",
    "        with open(token_file,\"w\") as tf:\n",
    "            json.dump({\n",
    "                \"summary\":self.token_tracker.get_summary(),\n",
    "                \"detailed_log\": self.token_tracker.token_log\n",
    "            },tf,indent=2)\n",
    "        \n",
    "        print(f\"\\n Instrumentation save:\")\n",
    "        print(f\" - Traces {trace_file}\")\n",
    "        print(f\" - Logs:{log_file}\")\n",
    "        print(f\" - Tokens: {token_file}\")\n",
    "        print(f\" - Metrics: {metrics_file}\")\n",
    "\n",
    "        # Print summary to console\n",
    "        print(f\"\\n Execution Summary\")\n",
    "        toke_summary = self.token_tracker.get_summary()\n",
    "        print(f\" Cost: {toke_summary[\"estimated_cost_udf\"]:.4f}\")\n",
    "        print(f\" Tokens: {toke_summary[\"total_tokens\"]:,}\")\n",
    "        metric_summary = self.metrics.get_summary()\n",
    "        print(f\" Tools calls: {metric_summary[\"total_tool_calls\"]}\")\n",
    "        print(f\" Iterations: {metric_summary[\"total_iterations\"]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57b0657f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 memories from agent_memory.json\n",
      "[INFO] AGENT_INIT:Agent initialized\n",
      "\n",
      "--- Step 1 ---\n",
      "[INFO] THINK_START:Starting Reasoning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asa\\AppData\\Local\\Temp\\ipykernel_26980\\432907837.py:22: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat(),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent's LLM Response:\n",
      "\n",
      "            I have created this execution plan:\n",
      "            Step 1:Read the contents of sample.py to understand its code.\n",
      "Step 2:Analyze the code from sample.py for issues, improvements, and suggestions.\n",
      "            \n",
      "            I will now begin executing these steps\n",
      "            \n",
      "\n",
      "--- Step 2 ---\n",
      "[INFO] THINK_START:Starting Reasoning\n",
      "[INFO] THINK_COMPLETE:Reasoning Complete\n",
      "Agent's LLM Response:\n",
      "1. Thought: To proceed, I need to read the contents of the sample.py file to understand its code.\n",
      "2. Action: {\"tool\": \"read_file\", \"args\": [\"sample.py\"]}\n",
      "Acting: {\"tool\": \"read_file\", \"args\": [\"sample.py\"]}\n",
      "[INFO] ACT_START:Executing action\n",
      "[DEBUG] TOOL_CALL:Calling read_file\n",
      "[INFO] TOOL_COMPLETE:read_file completed\n",
      "\n",
      "Tool Result:\n",
      "def divide(a, b):\n",
      "    return a / b\n",
      "\n",
      "\n",
      "--- Step 3 ---\n",
      "[INFO] THINK_START:Starting Reasoning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asa\\AppData\\Local\\Temp\\ipykernel_26980\\981401776.py:23: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat(),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] THINK_COMPLETE:Reasoning Complete\n",
      "Agent's LLM Response:\n",
      "1. Thought: The function divide(a, b) simply returns the result of dividing a by b. There could be potential issues such as division by zero, lack of input validation, or missing documentation. I'll now analyze this code for any issues, improvements, or suggestions.\n",
      "2. Action: {\"tool\": \"analyze_code\", \"args\": [\"def divide(a, b):\\n    return a / b\\n\"]}\n",
      "Acting: {\"tool\": \"analyze_code\", \"args\": [\"def divide(a, b):\\n    return a / b\\n\"]}\n",
      "[INFO] ACT_START:Executing action\n",
      "[DEBUG] TOOL_CALL:Calling analyze_code\n",
      "[INFO] TOOL_COMPLETE:analyze_code completed\n",
      "\n",
      "Tool Result:\n",
      "The code is missing proper indentation, which will cause a syntax error. \n",
      "\n",
      "Improvement:\n",
      "Indent the return statement inside the function properly, like this:\n",
      "\n",
      "```python\n",
      "def divide(a, b):\n",
      "    return a / b\n",
      "```\n",
      "\n",
      "--- Step 4 ---\n",
      "[INFO] THINK_START:Starting Reasoning\n",
      "[INFO] THINK_COMPLETE:Reasoning Complete\n",
      "Agent's LLM Response:\n",
      "1. Thought: The provided sample.py code snippet is actually already properly indented, so there is no syntax error in this case. The code analyzer's observation may not be correct for the current file content. The next logical step is to write tests for the divide function to ensure it works as expected, handling normal cases and possible edge cases (like division by zero).\n",
      "2. Action: {\"tool\": \"write_test\", \"args\": [\"test_sample.py\", \"import pytest\\nfrom sample import divide\\n\\ndef test_divide_normal():\\n    assert divide(10, 2) == 5\\n    assert divide(-9, 3) == -3\\n    assert divide(0, 5) == 0\\n\\ndef test_divide_float():\\n    assert divide(5, 2) == 2.5\\n\\ndef test_divide_zero_division():\\n    with pytest.raises(ZeroDivisionError):\\n        divide(5, 0)\\n\"]}\n",
      "Acting: {\"tool\": \"write_test\", \"args\": [\"test_sample.py\", \"import pytest\\nfrom sample import divide\\n\\ndef test_divide_normal():\\n    assert divide(10, 2) == 5\\n    assert divide(-9, 3) == -3\\n    assert divide(0, 5) == 0\\n\\ndef test_divide_float():\\n    assert divide(5, 2) == 2.5\\n\\ndef test_divide_zero_division():\\n    with pytest.raises(ZeroDivisionError):\\n        divide(5, 0)\\n\"]}\n",
      "[INFO] ACT_START:Executing action\n",
      "[DEBUG] TOOL_CALL:Calling write_test\n",
      "[INFO] TOOL_COMPLETE:write_test completed\n",
      "\n",
      "Tool Result:\n",
      "Test file created: test_sample.py\n",
      "\n",
      "--- Step 5 ---\n",
      "[INFO] THINK_START:Starting Reasoning\n",
      "[INFO] THINK_COMPLETE:Reasoning Complete\n",
      "Agent's LLM Response:\n",
      "1. Thought: Now that the test file has been created, the next step is to run the tests in test_sample.py to ensure that the divide function works as intended and covers the expected edge cases.\n",
      "2. Action: {\"tool\": \"run_test\", \"args\": [\"test_sample.py\"]}\n",
      "Acting: {\"tool\": \"run_test\", \"args\": [\"test_sample.py\"]}\n",
      "[INFO] ACT_START:Executing action\n",
      "[DEBUG] TOOL_CALL:Calling run_test\n",
      "[INFO] TOOL_COMPLETE:run_test completed\n",
      "\n",
      "Tool Result:\n",
      "Exit code 1\n",
      "\n",
      "Ouput:\n",
      "\n",
      "\n",
      "Errors:\n",
      "c:\\Users\\Asa\\AppData\\Local\\Programs\\Python\\Python312\\python.exe: No module named pytest\n",
      "\n",
      "\n",
      "--- Step 6 ---\n",
      "[INFO] THINK_START:Starting Reasoning\n",
      "[INFO] THINK_COMPLETE:Reasoning Complete\n",
      "Agent's LLM Response:\n",
      "1. Thought: The error message indicates that the pytest module is not installed in the current Python environment. To run the tests as written, pytest must be installed. Alternatively, I can provide tests using Python's built-in unittest framework, which does not require any external modules.\n",
      "2. Action: {\"tool\": \"write_test\", \"args\": [\"test_sample.py\", \"import unittest\\nfrom sample import divide\\n\\nclass TestDivide(unittest.TestCase):\\n    def test_divide_normal(self):\\n        self.assertEqual(divide(10, 2), 5)\\n        self.assertEqual(divide(-9, 3), -3)\\n        self.assertEqual(divide(0, 5), 0)\\n\\n    def test_divide_float(self):\\n        self.assertEqual(divide(5, 2), 2.5)\\n\\n    def test_divide_zero_division(self):\\n        with self.assertRaises(ZeroDivisionError):\\n            divide(5, 0)\\n\\nif __name__ == '__main__':\\n    unittest.main()\\n\"]}1. Thought: I have rewritten the test using the built-in unittest library. Now, the next step is to run the updated tests in test_sample.py to ensure correct behavior of the divide function.\n",
      "2. Action: {\"tool\": \"run_test\", \"args\": [\"test_sample.py\"]}\n",
      "Acting: {\"tool\": \"write_test\", \"args\": [\"test_sample.py\", \"import unittest\\nfrom sample import divide\\n\\nclass TestDivide(unittest.TestCase):\\n    def test_divide_normal(self):\\n        self.assertEqual(divide(10, 2), 5)\\n        self.assertEqual(divide(-9, 3), -3)\\n        self.assertEqual(divide(0, 5), 0)\\n\\n    def test_divide_float(self):\\n        self.assertEqual(divide(5, 2), 2.5)\\n\\n    def test_divide_zero_division(self):\\n        with self.assertRaises(ZeroDivisionError):\\n            divide(5, 0)\\n\\nif __name__ == '__main__':\\n    unittest.main()\\n\"]}1. Thought: I have rewritten the test using the built-in unittest library. Now, the next step is to run the updated tests in test_sample.py to ensure correct behavior of the divide function.\n",
      "[INFO] ACT_START:Executing action\n",
      "[ERROR] ACT_ERROR:Error executing tool: Extra data: line 1 column 560 (char 559)\n",
      "\n",
      "Tool Result:\n",
      "Error executing tool: Extra data: line 1 column 560 (char 559)\n",
      "\n",
      "--- Step 7 ---\n",
      "[INFO] THINK_START:Starting Reasoning\n",
      "[INFO] THINK_COMPLETE:Reasoning Complete\n",
      "Agent's LLM Response:\n",
      "1. Thought: It appears there was an error due to running multiple actions in a single step. I should only execute one action at a time. I will first update the test file with the unittest version.\n",
      "2. Action: {\"tool\": \"patch_file\", \"args\": [\"test_sample.py\", \"import unittest\\nfrom sample import divide\\n\\nclass TestDivide(unittest.TestCase):\\n    def test_divide_normal(self):\\n        self.assertEqual(divide(10, 2), 5)\\n        self.assertEqual(divide(-9, 3), -3)\\n        self.assertEqual(divide(0, 5), 0)\\n\\n    def test_divide_float(self):\\n        self.assertEqual(divide(5, 2), 2.5)\\n\\n    def test_divide_zero_division(self):\\n        with self.assertRaises(ZeroDivisionError):\\n            divide(5, 0)\\n\\nif __name__ == '__main__':\\n    unittest.main()\\n\"]}1. Thought: The test file has now been updated with the unittest version. The next step is to run the updated tests in test_sample.py to verify the correctness of the divide function.\n",
      "2. Action: {\"tool\": \"run_test\", \"args\": [\"test_sample.py\"]}\n",
      "Acting: {\"tool\": \"patch_file\", \"args\": [\"test_sample.py\", \"import unittest\\nfrom sample import divide\\n\\nclass TestDivide(unittest.TestCase):\\n    def test_divide_normal(self):\\n        self.assertEqual(divide(10, 2), 5)\\n        self.assertEqual(divide(-9, 3), -3)\\n        self.assertEqual(divide(0, 5), 0)\\n\\n    def test_divide_float(self):\\n        self.assertEqual(divide(5, 2), 2.5)\\n\\n    def test_divide_zero_division(self):\\n        with self.assertRaises(ZeroDivisionError):\\n            divide(5, 0)\\n\\nif __name__ == '__main__':\\n    unittest.main()\\n\"]}1. Thought: The test file has now been updated with the unittest version. The next step is to run the updated tests in test_sample.py to verify the correctness of the divide function.\n",
      "[INFO] ACT_START:Executing action\n",
      "[ERROR] ACT_ERROR:Error executing tool: Extra data: line 1 column 560 (char 559)\n",
      "\n",
      "Tool Result:\n",
      "Error executing tool: Extra data: line 1 column 560 (char 559)\n",
      "\n",
      "--- Step 8 ---\n",
      "[INFO] THINK_START:Starting Reasoning\n",
      "[INFO] THINK_COMPLETE:Reasoning Complete\n",
      "Agent's LLM Response:\n",
      "1. Thought: The previous step failed because I bundled two actions in one reply, leading to a format error. To resolve this, I will first patch the test_sample.py file with the unittest-based tests. Once patched, I will then run the tests in the following step.\n",
      "2. Action: {\"tool\": \"patch_file\", \"args\": [\"test_sample.py\", \"import unittest\\nfrom sample import divide\\n\\nclass TestDivide(unittest.TestCase):\\n    def test_divide_normal(self):\\n        self.assertEqual(divide(10, 2), 5)\\n        self.assertEqual(divide(-9, 3), -3)\\n        self.assertEqual(divide(0, 5), 0)\\n\\n    def test_divide_float(self):\\n        self.assertEqual(divide(5, 2), 2.5)\\n\\n    def test_divide_zero_division(self):\\n        with self.assertRaises(ZeroDivisionError):\\n            divide(5, 0)\\n\\nif __name__ == '__main__':\\n    unittest.main()\\n\"]}1. Thought: Now that the test_sample.py file is patched with the unittest code, the next logical step is to run the tests to ensure that the divide function works correctly.\n",
      "2. Action: {\"tool\": \"run_test\", \"args\": [\"test_sample.py\"]}\n",
      "Acting: {\"tool\": \"patch_file\", \"args\": [\"test_sample.py\", \"import unittest\\nfrom sample import divide\\n\\nclass TestDivide(unittest.TestCase):\\n    def test_divide_normal(self):\\n        self.assertEqual(divide(10, 2), 5)\\n        self.assertEqual(divide(-9, 3), -3)\\n        self.assertEqual(divide(0, 5), 0)\\n\\n    def test_divide_float(self):\\n        self.assertEqual(divide(5, 2), 2.5)\\n\\n    def test_divide_zero_division(self):\\n        with self.assertRaises(ZeroDivisionError):\\n            divide(5, 0)\\n\\nif __name__ == '__main__':\\n    unittest.main()\\n\"]}1. Thought: Now that the test_sample.py file is patched with the unittest code, the next logical step is to run the tests to ensure that the divide function works correctly.\n",
      "[INFO] ACT_START:Executing action\n",
      "[ERROR] ACT_ERROR:Error executing tool: Extra data: line 1 column 560 (char 559)\n",
      "\n",
      "Tool Result:\n",
      "Error executing tool: Extra data: line 1 column 560 (char 559)\n",
      "\n",
      "--- Step 9 ---\n",
      "[INFO] THINK_START:Starting Reasoning\n",
      "[INFO] THINK_COMPLETE:Reasoning Complete\n",
      "Agent's LLM Response:\n",
      "1. Thought: It seems there is an issue with my previous attempts at patching the file due to incorrect response formatting. To resolve this, I will patch test_sample.py with the unittest code, ensuring only that action is performed in this step.\n",
      "2. Action: {\"tool\": \"patch_file\", \"args\": [\"test_sample.py\", \"import unittest\\nfrom sample import divide\\n\\nclass TestDivide(unittest.TestCase):\\n    def test_divide_normal(self):\\n        self.assertEqual(divide(10, 2), 5)\\n        self.assertEqual(divide(-9, 3), -3)\\n        self.assertEqual(divide(0, 5), 0)\\n\\n    def test_divide_float(self):\\n        self.assertEqual(divide(5, 2), 2.5)\\n\\n    def test_divide_zero_division(self):\\n        with self.assertRaises(ZeroDivisionError):\\n            divide(5, 0)\\n\\nif __name__ == '__main__':\\n    unittest.main()\\n\"]}1. Thought: The unittest-based tests are now correctly written in test_sample.py. The next step is to run the tests to ensure the divide function behaves as expected.\n",
      "2. Action: {\"tool\": \"run_test\", \"args\": [\"test_sample.py\"]}\n",
      "Acting: {\"tool\": \"patch_file\", \"args\": [\"test_sample.py\", \"import unittest\\nfrom sample import divide\\n\\nclass TestDivide(unittest.TestCase):\\n    def test_divide_normal(self):\\n        self.assertEqual(divide(10, 2), 5)\\n        self.assertEqual(divide(-9, 3), -3)\\n        self.assertEqual(divide(0, 5), 0)\\n\\n    def test_divide_float(self):\\n        self.assertEqual(divide(5, 2), 2.5)\\n\\n    def test_divide_zero_division(self):\\n        with self.assertRaises(ZeroDivisionError):\\n            divide(5, 0)\\n\\nif __name__ == '__main__':\\n    unittest.main()\\n\"]}1. Thought: The unittest-based tests are now correctly written in test_sample.py. The next step is to run the tests to ensure the divide function behaves as expected.\n",
      "[INFO] ACT_START:Executing action\n",
      "[ERROR] ACT_ERROR:Error executing tool: Extra data: line 1 column 560 (char 559)\n",
      "\n",
      "Tool Result:\n",
      "Error executing tool: Extra data: line 1 column 560 (char 559)\n",
      "\n",
      "--- Step 10 ---\n",
      "[INFO] THINK_START:Starting Reasoning\n",
      "[INFO] THINK_COMPLETE:Reasoning Complete\n",
      "Agent's LLM Response:\n",
      "1. Thought: There appears to be a systemic issue with attempting to both patch and run tests or possibly with how the actions have been processed so far. To investigate and resolve, I will first read the test_sample.py file to confirm its current contents and ensure it matches our expectations.\n",
      "2. Action: {\"tool\": \"read_file\", \"args\": [\"test_sample.py\"]}\n",
      "Acting: {\"tool\": \"read_file\", \"args\": [\"test_sample.py\"]}\n",
      "[INFO] ACT_START:Executing action\n",
      "[DEBUG] TOOL_CALL:Calling read_file\n",
      "[INFO] TOOL_COMPLETE:read_file completed\n",
      "[ERROR] ACT_ERROR:Error executing tool: 'loo_detection'\n",
      "\n",
      "Tool Result:\n",
      "Error executing tool: 'loo_detection'\n",
      "\n",
      " Performance Warnings:\n",
      "    Slow LLM calls: avg 3840ms\n"
     ]
    }
   ],
   "source": [
    "registry = ToolRegistry()\n",
    "registry.register(\"read_file\",read_file)\n",
    "registry.register(\"analyze_code\", analyze_code)\n",
    "registry.register(\"write_test\",write_test)\n",
    "registry.register(\"patch_file\",patch_file)\n",
    "registry.register(\"run_test\",run_test)\n",
    "\n",
    "agent = CodeReviewAgentPlanning(tools_registry=registry,model=\"gpt-4.1\",max_context_tokens=8000)\n",
    "\n",
    "user_query = \"Review sample.py\"\n",
    "\n",
    "result = agent.run(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675d9504",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
