{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b65e5bd1",
   "metadata": {},
   "source": [
    "# Observability and Debugging AI Agents\n",
    "As your AI Agent grows more sophisticated, handling multi step plans, maintaining memory and using multiple tools, understanding what it's doing and why becomes important.  \n",
    "Without prooper observability, debugging agent failures feels like operating in the dark\n",
    "\n",
    "I this tutorial, we will add instrumentation to our code review agent, covering:\n",
    "* **Structured logging** for every agent action\n",
    "* **Trace visualization** to understand thought -> tool -> result chains\n",
    "* **Token usage and cost tracking** for budget management\n",
    "* **Performance metrics** to identify bottlenecks\n",
    "* **Error detection** for loops and excessive tool usage\n",
    "\n",
    "By the end, you will have patterns for instrumenting any agent system - patterns that are similar to professional observability tools.\n",
    "\n",
    "## What is Observability?\n",
    "**Observability** is the ability to understand what's happening inside a system by examining its outputs. Unlike traditional monitoring, which answers *\"is it working?\"*, observability answers *\"why isn't it working?\"* and *\"what exactly happened?\"*\n",
    "Think of it like this: A dashboard showing \"CPU at 80%\" is monitoring. Being able to trace why CPU spiked—seeing that it happened during a specific LLM call processing a 10,000-token prompt, which triggered three tool calls, one of which failed and retried—that's observability.\n",
    "### The Three Pillars of Observability\n",
    "Observability is built on three types of data, often called \"telemetry\":\n",
    "\n",
    "1. **Logs**\n",
    "Individual event records with timestamps.\n",
    "    * **What:** \"At 14:32:15, the agent called read_file('calculator.py')\"\n",
    "    * **When to use:** Debugging specific events, understanding what happened\n",
    "    * **Example:** Error messages, audit trails, state changes\n",
    "\n",
    "2. **Metrics**\n",
    "Aggregated numerical measurements over time.\n",
    "    * **What:** \"Average LLM latency: 450ms\" or \"Tool calls per minute: 12\"\n",
    "    * **When to use:** Monitoring trends, detecting anomalies, capacity planning\n",
    "    * **Example:** Request counts, duration histograms, error rates\n",
    "\n",
    "3. **Traces**\n",
    "Connected records showing how a single request flows through your system.\n",
    "    * **What:** A tree showing: User query → Agent thinks → Calls read_file → Agent thinks → Calls analyze_code → Returns answer\n",
    "    * **When to use:** Understanding execution flow, finding bottlenecks\n",
    "    * **Example:** The full journey of one agent task from start to finish\n",
    "\n",
    "### How They Work Together\n",
    "Imagine your agent fails on a user request:\n",
    "\n",
    "1. **Metrics** alert you: \"Error rate jumped to 15%\"\n",
    "2. **Traces show you:** \"Failures happening after the 3rd tool call in multi-step plans\"\n",
    "3. **Logs reveal:** \"Tool 'patch_file' threw 'Permission Denied' error\"\n",
    "\n",
    "Each pillar provides different insight; together they give you complete visibility.\n",
    "### Why This Matters for AI Agents\n",
    "Traditional software follows predictable code paths. AI agents are non-deterministic:\n",
    "\n",
    "* The LLM might choose different tools each run\n",
    "* Reasoning steps vary based on context\n",
    "* Failures can cascade through multi-step plans\n",
    "\n",
    "Without observability, debugging feels like guesswork. With it, you can:\n",
    "\n",
    "* See the exact sequence of thoughts and actions\n",
    "* Identify why the agent got stuck in a loop\n",
    "* Track which operations consume the most tokens (and cost)\n",
    "* Understand performance bottlenecks\n",
    "\n",
    "In this tutoral, we'll build all three pillars; logs, metrics, and traces—into our code review agent, giving you complete visibility into its behavior.\n",
    "\n",
    "It's important to note that while we are building the observability from scratch for learning purposes, it is best practice in production to use dedicated observability tools and standards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d287a08a",
   "metadata": {},
   "source": [
    "### Structured Logging\n",
    "We want to add structured logs that capture rich metadata about every agent action.\n",
    "\n",
    "### Add a logging layer\n",
    "* **Timestamps:** Every log gets a UTC timestamp for analysis\n",
    "* **Event types:** Lets us categorize logs (e.g. \"TOOL_CALL\",\"LLM_REQUEST\") for filtering\n",
    "* **Metadata:** For context specific information\n",
    "* **Agent ID:** Identify agent instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af7d0882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "from typing import Any, Optional\n",
    "\n",
    "class LogLevel(Enum):\n",
    "    DEBUG = \"DEBUG\"\n",
    "    INFO = \"INFO\"\n",
    "    WARNING = \"WARNING\"\n",
    "    ERROR = \"ERROR\"\n",
    "\n",
    "class AgentLogger:\n",
    "    \"\"\"Structured logging for agent actions\"\"\"\n",
    "    def __init__(self, agent_id: str = \"agent-1\"):\n",
    "        self.agent_id = agent_id\n",
    "        self.logs = []\n",
    "\n",
    "    def log(self, level: LogLevel, event_type: str, message: str, \n",
    "            metadata: Optional[dict[str, Any]] = None):\n",
    "        \"\"\"Create a structured log entry\"\"\"\n",
    "        log_entry = {\n",
    "            \"timestamp\": datetime.utcnow().isoformat(),\n",
    "            \"agent_id\": self.agent_id,\n",
    "            \"level\": level.value,\n",
    "            \"event_type\": event_type,\n",
    "            \"message\": message,\n",
    "            \"metadata\": metadata or {}\n",
    "        }\n",
    "        self.logs.append(log_entry)\n",
    "\n",
    "        # Also print for real-time feedback\n",
    "        print(f\"[{level.value}] {event_type}: {message}\")\n",
    "    \n",
    "    def get_logs(self, event_type: Optional[str] = None) -> list:\n",
    "        \"\"\"Retrieve logs optionally filtered by event type\"\"\"\n",
    "        if event_type:\n",
    "            return [log for log in self.logs if log[\"event_type\"] == event_type]\n",
    "        return self.logs\n",
    "\n",
    "    def save_logs(self, file_path: str):\n",
    "        \"\"\"Persist logs to a JSON file\"\"\"\n",
    "        with open(file_path, \"w\") as f:\n",
    "            json.dump(self.logs, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0567b160",
   "metadata": {},
   "source": [
    "### Integrating logging into the Agent\n",
    "* Add logging to ``__init()__``\n",
    "\n",
    "```python\n",
    "class CodeReviewAgentObservable:\n",
    "    def __init__(self, tools_registry: ToolRegistry, \n",
    "                model=\"gpt-4.1\", memory_file=\"agent_memory.json\",\n",
    "                summarize_after=10, max_context_tokens=6000):\n",
    "        # ...Existing init code...\n",
    "\n",
    "        # Add logger\n",
    "        self.logger = AgentLogger(agent_id=f\"code-review-{int(time.time())}\")\n",
    "\n",
    "        # ...The rest of init code...\n",
    "```\n",
    "\n",
    "* Add logging to `think()`\n",
    "```python\n",
    "def think(self, user_input: str):\n",
    "    \"\"\"LLM enhanced thinking with logging\"\"\"\n",
    "    self.logger.log(LogLevel.INFO, \"THINK_START\", \"Starting Reasoning\", \n",
    "                   {\"user_input\": user_input[:100]})\n",
    "\n",
    "    # ...rest of think code...\n",
    "    \n",
    "    response = openai.responses.create(model=self.model, input=messages)\n",
    "    decision = response.output_text\n",
    "\n",
    "    # Log end of thinking\n",
    "    self.logger.log(LogLevel.INFO, \"THINK_COMPLETE\", \"Reasoning Complete\", \n",
    "                   {\"decision\": decision[:200]})\n",
    "\n",
    "    # ...rest of thinking...  \n",
    "```\n",
    "* Add logging to `act()`\n",
    "```python\n",
    "def act(self, action: dict):\n",
    "    \"\"\"Execute tool with logging\"\"\"\n",
    "    self.logger.log(LogLevel.INFO, \"ACT_START\", \n",
    "                   \"Executing action\",\n",
    "                   {\"tool\": action.get(\"tool\"), \"args\": action.get(\"args\", [])})\n",
    "    \n",
    "    try:\n",
    "        tool_name = action.get(\"tool\")\n",
    "        args = action.get(\"args\", [])\n",
    "        \n",
    "        self.logger.log(LogLevel.DEBUG, \"TOOL_CALL\", \n",
    "                       f\"Calling {tool_name}\",\n",
    "                       {\"tool\": tool_name, \"args\": args})\n",
    "        \n",
    "        start_time = time.time()\n",
    "        result = self.tools.call(tool_name, *args)\n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        self.logger.log(LogLevel.INFO, \"TOOL_COMPLETE\", \n",
    "                       f\"{tool_name} completed\",\n",
    "                       {\"tool\": tool_name, \"duration_ms\": duration * 1000,\n",
    "                        \"result_length\": len(str(result))})\n",
    "        \n",
    "        # ... rest of act logic ...\n",
    "        \n",
    "    except Exception as e:\n",
    "        self.logger.log(LogLevel.ERROR, \"ACT_ERROR\", \n",
    "                       f\"Action failed: {str(e)}\",\n",
    "                       {\"action\": action, \"error\": str(e)})\n",
    "        # ... error handling ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedd4cfe",
   "metadata": {},
   "source": [
    "## Trace Hierarchies\n",
    "Logs are flat, they dont show *relationships* between operation. A trace captures the nested structure of agent execution.\n",
    "\n",
    "### Building a Trace Structure\n",
    "* **Spans:** Individual units of work\n",
    "* **Hierarchy:** Child spans nest under parents to show causality\n",
    "* **Context propagation:** `current_span_id` tracks where we are in the call stack\n",
    "* **Lazy evaluation:** Only root spans are saved to `traces`\n",
    "* **Trace Vizualizer:** Display traces in a way that is easy to read and interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7feededf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import uuid\n",
    "\n",
    "class Span:\n",
    "    \"\"\"Represents a single unit of work in a trace\"\"\"\n",
    "\n",
    "    def __init__(self, name: str, span_type: str, parent_id: Optional[str] = None):\n",
    "        self.span_id = str(uuid.uuid4())[:8]\n",
    "        self.parent_id = parent_id\n",
    "        self.name = name\n",
    "        self.span_type = span_type\n",
    "        self.start_time = time.time()\n",
    "        self.end_time = None\n",
    "        self.status = \"running\"\n",
    "        self.metadata = {}\n",
    "        self.children: list[Span] = []\n",
    "\n",
    "    def end(self, status: str = \"success\", metadata: Optional[dict] = None):\n",
    "        \"\"\"Mark span as complete\"\"\"\n",
    "        self.end_time = time.time()\n",
    "        self.status = status\n",
    "        if metadata:\n",
    "            self.metadata.update(metadata)\n",
    "    \n",
    "    def duration_ms(self) -> float:\n",
    "        \"\"\"Calculate span duration in milliseconds\"\"\"\n",
    "        if self.end_time:\n",
    "            return (self.end_time - self.start_time) * 1000\n",
    "        return (time.time() - self.start_time) * 1000\n",
    "    \n",
    "    def add_child(self, child: 'Span'):\n",
    "        \"\"\"Add child span\"\"\"\n",
    "        self.children.append(child)\n",
    "\n",
    "    def to_dict(self) -> dict:\n",
    "        \"\"\"Convert span to dict for serialization\"\"\"\n",
    "        return {\n",
    "            \"span_id\": self.span_id,\n",
    "            \"parent_id\": self.parent_id,\n",
    "            \"name\": self.name,\n",
    "            \"type\": self.span_type,\n",
    "            \"start_time\": self.start_time,\n",
    "            \"end_time\": self.end_time,\n",
    "            \"duration_ms\": self.duration_ms(),\n",
    "            \"status\": self.status,\n",
    "            \"metadata\": self.metadata,\n",
    "            \"children\": [child.to_dict() for child in self.children]\n",
    "        }\n",
    "\n",
    "class TraceManager:\n",
    "    \"\"\"Manages execution traces\"\"\"\n",
    "    def __init__(self):\n",
    "        self.traces = []\n",
    "        self.active_spans = {}\n",
    "        self.current_span_id = None\n",
    "    \n",
    "    def start_span(self, name: str, span_type: str) -> str:\n",
    "        \"\"\"Create and activate a new span\"\"\"\n",
    "        parent_id = self.current_span_id\n",
    "        span = Span(name, span_type, parent_id)\n",
    "        self.active_spans[span.span_id] = span\n",
    "\n",
    "        if parent_id and parent_id in self.active_spans:\n",
    "            self.active_spans[parent_id].add_child(span)\n",
    "        \n",
    "        self.current_span_id = span.span_id\n",
    "        return span.span_id\n",
    "    \n",
    "    def end_span(self, span_id: str, status: str = \"success\", \n",
    "                 metadata: Optional[dict] = None):\n",
    "        \"\"\"Complete a span and update current span\"\"\"\n",
    "        if span_id in self.active_spans:\n",
    "            span = self.active_spans[span_id]\n",
    "            span.end(status, metadata)\n",
    "\n",
    "            # Move current span to parent\n",
    "            if span.parent_id:\n",
    "                self.current_span_id = span.parent_id\n",
    "            else:\n",
    "                # Root span completed - save trace\n",
    "                self.traces.append(span)\n",
    "                self.current_span_id = None\n",
    "\n",
    "    def get_current_span(self) -> Optional[Span]:\n",
    "        \"\"\"Get the currently active span\"\"\"\n",
    "        if self.current_span_id:\n",
    "            return self.active_spans.get(self.current_span_id)\n",
    "        return None\n",
    "    \n",
    "    def save_traces(self, file_path: str):\n",
    "        \"\"\"Save all traces to a file\"\"\"\n",
    "        traces_data = [trace.to_dict() for trace in self.traces]\n",
    "        with open(file_path, \"w\") as f:\n",
    "            json.dump(traces_data, f, indent=2)\n",
    "            \n",
    "class TraceVisualizer:\n",
    "    \"\"\"Generate human readable trace visualizations\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def format_trace(span: dict, indent: int = 0) -> str:\n",
    "        \"\"\"Recursively format a trace and its children\"\"\"\n",
    "        prefix = \"  \" * indent\n",
    "        \n",
    "        duration = span[\"duration_ms\"]\n",
    "        duration_str = f\"{duration:.0f}ms\"\n",
    "\n",
    "        status_icon = f\"❌ Status: {span[\"status\"]}\" if span[\"status\"] == \"ERROR\" else f\"✅ Status: {span[\"status\"]}\" \n",
    "\n",
    "        # Build line\n",
    "        line = f\"{prefix}{status_icon} {span['name']} ({span['type']}) - {duration_str}\"\n",
    "\n",
    "        if span.get(\"metadata\"):\n",
    "            metadata = span[\"metadata\"]\n",
    "            if \"cost_usd\" in metadata:\n",
    "                line += f\" [${metadata['cost_usd']:.4f}]\"\n",
    "            if \"error\" in metadata:\n",
    "                line += f\" [ERROR: {metadata['error']}]\"\n",
    "\n",
    "        lines = [line]\n",
    "\n",
    "        # Recursively format children\n",
    "        for child in span.get(\"children\", []):\n",
    "            lines.append(TraceVisualizer.format_trace(child, indent + 1))\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "    \n",
    "    @staticmethod\n",
    "    def print_all_traces(traces: List[dict]):\n",
    "        \"\"\"Print all traces in a readable format\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"EXECUTION TRACES\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        for i, trace in enumerate(traces, 1):\n",
    "            print(f\"Trace {i}:\")\n",
    "            print(TraceVisualizer.format_trace(trace))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51de944",
   "metadata": {},
   "source": [
    "### Integrate Tracing into the agent\n",
    "* Add the trace manager to the agent\n",
    "```python\n",
    "class CodeReviewAgentObservable:\n",
    "    def __init__(self, tools_registry: ToolRegistry, ...):\n",
    "        # ... existing init ...\n",
    "        self.logger = AgentLogger(agent_id=f\"code-review-{int(time.time())}\")\n",
    "        self.tracer = TraceManager()\n",
    "```\n",
    "\n",
    "* Update the `run()` method to add a root span\n",
    "```python\n",
    "def run(self, user_query: str, max_iterations=10):\n",
    "    \"\"\"Main execution loop with tracing\"\"\"\n",
    "    \n",
    "    # Create root span for entire run\n",
    "    run_span_id = self.tracer.start_span(\n",
    "        name=f\"Agent Run: {user_query[:50]}\", \n",
    "        span_type=\"AGENT_RUN\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        current_input = user_query\n",
    "        \n",
    "        for step in range(max_iterations):\n",
    "            # Create span for each iteration\n",
    "            iter_span_id = self.tracer.start_span(\n",
    "                name=f\"Iteration {step + 1}\",\n",
    "                span_type=\"ITERATION\"\n",
    "            )\n",
    "            \n",
    "            # ... existing logic ...\n",
    "            \n",
    "            if \"answer\" in parsed_response:\n",
    "                self.tracer.end_span(iter_span_id, \"SUCCESS\", \n",
    "                                    {\"final_answer\": parsed_response[\"answer\"][:100]})\n",
    "                self.tracer.end_span(run_span_id, \"SUCCESS\",\n",
    "                                    {\"total_iterations\": step + 1})\n",
    "                return parsed_response[\"answer\"]\n",
    "                \n",
    "            if \"action\" in parsed_response:\n",
    "                observation = self.act(parsed_response[\"action\"])\n",
    "                current_input = f\"Observation: {observation}\"\n",
    "                \n",
    "            self.tracer.end_span(iter_span_id, \"SUCCESS\")\n",
    "            \n",
    "        # Max iterations reached\n",
    "        self.tracer.end_span(run_span_id, \"MAX_ITERATIONS\",\n",
    "                           {\"completed_steps\": len(self.completed_steps),\n",
    "                            \"total_steps\": len(self.current_plan)})\n",
    "        return \"Task Incomplete: Max steps reached\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        self.tracer.end_span(run_span_id, \"ERROR\", {\"error\": str(e)})\n",
    "        raise\n",
    "```\n",
    "\n",
    "* add spans to the `think()` method\n",
    "```python\n",
    "def think(self, user_input: str):\n",
    "    \"\"\"Reasoning with tracing\"\"\"\n",
    "    think_span_id = self.tracer.start_span(\"Think\", \"LLM_CALL\")\n",
    "    \n",
    "    try:\n",
    "        # ... existing think logic ...\n",
    "        \n",
    "        response = openai.responses.create(model=self.model, input=messages)\n",
    "        decision = response.output_text\n",
    "        \n",
    "        self.tracer.end_span(think_span_id, \"SUCCESS\",\n",
    "                           {\"input_length\": len(user_input),\n",
    "                            \"output_length\": len(decision)})\n",
    "        return decision\n",
    "        \n",
    "    except Exception as e:\n",
    "        self.tracer.end_span(think_span_id, \"ERROR\", {\"error\": str(e)})\n",
    "        raise\n",
    "```\n",
    "* Add spans to the `act()` method\n",
    "```python\n",
    "def act(self, action: dict):\n",
    "    \"\"\"Tool execution with tracing\"\"\"\n",
    "    act_span_id = self.tracer.start_span(\"Act\", \"TOOL_EXECUTION\")\n",
    "    \n",
    "    try:\n",
    "        tool_name = action.get(\"tool\")\n",
    "        args = action.get(\"args\", [])\n",
    "        \n",
    "        # Create nested span for the specific tool\n",
    "        tool_span_id = self.tracer.start_span(\n",
    "            f\"Tool: {tool_name}\",\n",
    "            \"TOOL_CALL\"\n",
    "        )\n",
    "        \n",
    "        result = self.tools.call(tool_name, *args)\n",
    "        \n",
    "        self.tracer.end_span(tool_span_id, \"SUCCESS\",\n",
    "                           {\"tool\": tool_name, \"result_size\": len(str(result))})\n",
    "        \n",
    "        # ... rest of act logic ...\n",
    "        \n",
    "        self.tracer.end_span(act_span_id, \"SUCCESS\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        self.tracer.end_span(act_span_id, \"ERROR\", {\"error\": str(e)})\n",
    "        return f\"Error executing tool: {e}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9456c4b6",
   "metadata": {},
   "source": [
    "## Token Usage and Cost Tracking\n",
    "LLM costs can add up quickly, Let's track token usage and estimate costs per operation\n",
    "\n",
    "### Token Counter\n",
    "* Keep track of input and output token counts\n",
    "* Can calculate estimated LLM calls cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35aff225",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenTracker:\n",
    "    \"\"\"Track token usage and estimate costs\"\"\"\n",
    "\n",
    "    # Pricing per 1M tokens \n",
    "    PRICING = {\n",
    "        \"gpt-4.1\": {\"input\": 2.50, \"output\": 10.00},\n",
    "        \"gpt-4.1-mini\": {\"input\": 0.15, \"output\": 0.60}\n",
    "    }\n",
    "\n",
    "    def __init__(self, model: str):\n",
    "        self.model = model\n",
    "        self.total_input_tokens = 0\n",
    "        self.total_output_tokens = 0\n",
    "        self.call_count = 0\n",
    "        self.token_log = []\n",
    "    \n",
    "    def track_usage(self, input_tokens: int, output_tokens: int, \n",
    "                    operation: str = \"llm_call\"):\n",
    "        self.total_input_tokens += input_tokens\n",
    "        self.total_output_tokens += output_tokens\n",
    "        self.call_count += 1\n",
    "\n",
    "        entry = {\n",
    "            \"timestamp\": datetime.utcnow().isoformat(),\n",
    "            \"operation\": operation,\n",
    "            \"input_tokens\": input_tokens,\n",
    "            \"output_tokens\": output_tokens,\n",
    "            \"cost_usd\": self._calculate_cost(input_tokens, output_tokens)\n",
    "        }\n",
    "        self.token_log.append(entry)\n",
    "    \n",
    "    def _calculate_cost(self, input_tokens: int, output_tokens: int) -> float:\n",
    "        \"\"\"Calculate cost in USD\"\"\"\n",
    "        if self.model not in self.PRICING:\n",
    "            return 0.0\n",
    "        pricing = self.PRICING[self.model]\n",
    "        input_cost = (input_tokens / 1000000) * pricing[\"input\"]\n",
    "        output_cost = (output_tokens / 1000000) * pricing[\"output\"]\n",
    "        return input_cost + output_cost\n",
    "    \n",
    "    def get_summary(self) -> dict:\n",
    "        \"\"\"Get usage summary\"\"\"\n",
    "        return {\n",
    "            \"model\": self.model,\n",
    "            \"total_calls\": self.call_count,\n",
    "            \"total_input_tokens\": self.total_input_tokens,\n",
    "            \"total_output_tokens\": self.total_output_tokens,\n",
    "            \"total_tokens\": self.total_input_tokens + self.total_output_tokens,\n",
    "            \"estimated_cost_usd\": self._calculate_cost(\n",
    "                self.total_input_tokens, self.total_output_tokens\n",
    "            )\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd87f8fd",
   "metadata": {},
   "source": [
    "### Add Token Tracking to the Agent\n",
    "* Update agent *initialization* to add the tracker\n",
    "* Update `think()` method\n",
    "\n",
    "```python\n",
    "class CodeReviewAgentObservable:\n",
    "    def __init__(self, tools_registry: ToolRegistry, model=\"gpt-4.1\", ...):\n",
    "        # ... existing init ...\n",
    "        self.token_tracker = TokenTracker(model)\n",
    "        \n",
    "    def think(self, user_input: str):\n",
    "        \"\"\"Reasoning with token tracking\"\"\"\n",
    "        think_span_id = self.tracer.start_span(\"Think\", \"LLM_CALL\")\n",
    "        \n",
    "        # ... build messages ...\n",
    "        \n",
    "        # Count input tokens\n",
    "        input_text = json.dumps([msg[\"content\"] for msg in messages])\n",
    "        input_tokens = self.count_tokens(input_text)\n",
    "        \n",
    "        response = openai.responses.create(model=self.model, input=messages)\n",
    "        decision = response.output_text\n",
    "        \n",
    "        # Count output tokens\n",
    "        output_tokens = self.count_tokens(decision)\n",
    "        \n",
    "        # Track usage\n",
    "        self.token_tracker.track_usage(input_tokens, output_tokens, \"think\")\n",
    "        \n",
    "        self.tracer.end_span(think_span_id, \"SUCCESS\", {\n",
    "            \"input_tokens\": input_tokens,\n",
    "            \"output_tokens\": output_tokens,\n",
    "            \"cost_usd\": self.token_tracker._calculate_cost(input_tokens, output_tokens)\n",
    "        })\n",
    "        \n",
    "        # ... rest of think ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f332eb",
   "metadata": {},
   "source": [
    "### Performance Metrics and Anomaly Detection\n",
    "Track metrics to identify performance issues and agent misbehaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcdbbc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import List\n",
    "class MetricsCollector:\n",
    "    \"\"\"Collect and analyze performance metrics\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            \"iteration_count\": 0,\n",
    "            \"tool_calls\": {},  # tool_name: count\n",
    "            \"tool_latencies\": {},  # tool_name: durations\n",
    "            \"llm_latencies\": [],\n",
    "            \"errors\": [],\n",
    "            \"loop_detection\": []  # Track repeated tool calls\n",
    "        }\n",
    "        self.last_n_tools = []  # Sliding window for loop detection\n",
    "    \n",
    "    def record_iteration(self):\n",
    "        \"\"\"Increment iteration counter\"\"\"\n",
    "        self.metrics[\"iteration_count\"] += 1\n",
    "    \n",
    "    def record_tool_call(self, tool_name: str, duration_ms: float):\n",
    "        \"\"\"Record a tool invocation\"\"\"\n",
    "        if tool_name not in self.metrics[\"tool_calls\"]:\n",
    "            self.metrics[\"tool_calls\"][tool_name] = 0\n",
    "            self.metrics[\"tool_latencies\"][tool_name] = []\n",
    "        \n",
    "        self.metrics[\"tool_calls\"][tool_name] += 1\n",
    "        self.metrics[\"tool_latencies\"][tool_name].append(duration_ms)\n",
    "\n",
    "        # Loop detection: track last 5 tool calls\n",
    "        self.last_n_tools.append(tool_name)\n",
    "        if len(self.last_n_tools) > 5:\n",
    "            self.last_n_tools.pop(0)\n",
    "        \n",
    "        # Check for repeated patterns\n",
    "        if len(self.last_n_tools) == 5:\n",
    "            if len(set(self.last_n_tools)) <= 2:  # 1 or 2 unique tool calls in last 5\n",
    "                self.metrics[\"loop_detection\"].append({\n",
    "                    \"iteration\": self.metrics[\"iteration_count\"],\n",
    "                    \"pattern\": self.last_n_tools.copy()\n",
    "                })\n",
    "    \n",
    "    def record_llm_latency(self, duration_ms: float):\n",
    "        \"\"\"Record LLM call duration\"\"\"\n",
    "        self.metrics[\"llm_latencies\"].append(duration_ms)\n",
    "    \n",
    "    def record_error(self, error_type: str, details: str):\n",
    "        \"\"\"Record an error\"\"\"\n",
    "        self.metrics[\"errors\"].append({\n",
    "            \"timestamp\": datetime.utcnow().isoformat(),\n",
    "            \"type\": error_type,\n",
    "            \"details\": details\n",
    "        })\n",
    "    \n",
    "    def get_summary(self) -> dict:\n",
    "        \"\"\"Generate metrics summary\"\"\"\n",
    "        summary = {\n",
    "            \"total_iterations\": self.metrics[\"iteration_count\"],\n",
    "            \"total_tool_calls\": sum(self.metrics[\"tool_calls\"].values()),\n",
    "            \"tool_usage\": self.metrics[\"tool_calls\"],\n",
    "            \"error_count\": len(self.metrics[\"errors\"]),\n",
    "            \"potential_loops\": len(self.metrics[\"loop_detection\"])\n",
    "        }\n",
    "\n",
    "        # Calculate average latencies\n",
    "        if self.metrics[\"llm_latencies\"]:\n",
    "            summary[\"avg_llm_latency_ms\"] = (\n",
    "                sum(self.metrics[\"llm_latencies\"]) / len(self.metrics[\"llm_latencies\"])\n",
    "            )\n",
    "\n",
    "        summary[\"tool_avg_latencies\"] = {}\n",
    "        for tool, latencies in self.metrics[\"tool_latencies\"].items():\n",
    "            if latencies:\n",
    "                summary[\"tool_avg_latencies\"][tool] = sum(latencies) / len(latencies)\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def check_anomalies(self) -> List[str]:\n",
    "        \"\"\"Detect anomalous behavior\"\"\"\n",
    "        warnings = []\n",
    "        \n",
    "        # Check for excessive iterations\n",
    "        if self.metrics[\"iteration_count\"] > 15:\n",
    "            warnings.append(f\"High iteration count: {self.metrics['iteration_count']}\")\n",
    "        \n",
    "        # Check for tool call loops\n",
    "        if self.metrics[\"loop_detection\"]:\n",
    "            warnings.append(\n",
    "                f\"Possible loop detected: {len(self.metrics['loop_detection'])} instances\"\n",
    "            )\n",
    "        \n",
    "        # Check for excessive errors\n",
    "        if len(self.metrics[\"errors\"]) > 3:\n",
    "            warnings.append(f\"Multiple errors: {len(self.metrics['errors'])}\")\n",
    "        \n",
    "        # Check for slow operations\n",
    "        if self.metrics[\"llm_latencies\"]:\n",
    "            avg_llm = sum(self.metrics[\"llm_latencies\"]) / len(self.metrics[\"llm_latencies\"])\n",
    "            if avg_llm > 2000:\n",
    "                warnings.append(f\"Slow LLM calls: avg {avg_llm:.0f}ms\")\n",
    "        \n",
    "        return warnings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109a01cd",
   "metadata": {},
   "source": [
    "### Integrating Metrics\n",
    "Add the metrics collector and update instrumented methods\n",
    "\n",
    "```python\n",
    "class CodeReviewAgentPlanning:\n",
    "    def __init__(self, tools_registry: ToolRegistry, ...):\n",
    "        # ... existing init ...\n",
    "        self.metrics = MetricsCollector()\n",
    "        \n",
    "    def run(self, user_query: str, max_iterations=10):\n",
    "        \"\"\"Main loop with metrics\"\"\"\n",
    "        run_span_id = self.tracer.start_span(\n",
    "            name=f\"Agent Run: {user_query[:50]}\", \n",
    "            span_type=\"AGENT_RUN\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            step = 0\n",
    "            current_input = user_query\n",
    "            \n",
    "            while step < max_iterations:\n",
    "                self.metrics.record_iteration()\n",
    "                \n",
    "                # ... existing loop logic ...\n",
    "                \n",
    "            # Check for anomalies at the end\n",
    "            warnings = self.metrics.check_anomalies()\n",
    "            if warnings:\n",
    "                print(\"\\n Performance Warnings:\")\n",
    "                for warning in warnings:\n",
    "                    print(f\"  {warning}\")\n",
    "                    \n",
    "            return \"Task Incomplete: Max steps reached\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.metrics.record_error(\"RUNTIME_ERROR\", str(e))\n",
    "            self.tracer.end_span(run_span_id, \"ERROR\", {\"error\": str(e)})\n",
    "            raise\n",
    "        finally:\n",
    "            # Always save instrumentation\n",
    "            self.save_instrumentation()\n",
    "            \n",
    "    def think(self, user_input: str):\n",
    "        \"\"\"Thinking with metrics\"\"\"\n",
    "        think_span_id = self.tracer.start_span(\"Think\", \"LLM_CALL\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # ... existing think logic ...\n",
    "            \n",
    "            response = openai.responses.create(model=self.model, input=messages)\n",
    "            decision = response.output_text\n",
    "            \n",
    "            duration_ms = (time.time() - start_time) * 1000\n",
    "            self.metrics.record_llm_latency(duration_ms)\n",
    "            \n",
    "            # ... rest of think ...\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.metrics.record_error(\"THINK_ERROR\", str(e))\n",
    "            raise\n",
    "            \n",
    "    def act(self, decision: str):\n",
    "        \"\"\"Tool execution with metrics\"\"\"\n",
    "        # ... existing act logic ...\n",
    "        \n",
    "        start_time = time.time()\n",
    "        result = self.tools.call(tool_name, *args)\n",
    "        duration_ms = (time.time() - start_time) * 1000\n",
    "        \n",
    "        self.metrics.record_tool_call(tool_name, duration_ms)\n",
    "        \n",
    "        # ... rest of act ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aa8c68",
   "metadata": {},
   "source": [
    "### Persisting Obervability Data\n",
    "Let's add some utitity methods to the agent to persist and display observability data.  \n",
    "We will update the `run()` method to always perist observability data\n",
    "```python\n",
    "    def run(self, user_query: str, max_iterations=10):\n",
    "        #....rest of run method\n",
    "        except Exception as e:\n",
    "            self.metrics.record_error(\"RUNTIME_ERROR\", str(e))\n",
    "            self.tracer.end_span(run_span_id, \"ERROR\", {\"error\": str(e)})\n",
    "            raise\n",
    "        finally:\n",
    "            # Always save instrumentation\n",
    "            self.save_instrumentation()\n",
    "```\n",
    "\n",
    "```python\n",
    "    def save_instrumentation(self, trace_file=\"traces.json\",log_file=\"log.json\",token_file=\"tokens.json\",metrics_file=\"metrics.json\"):\n",
    "        self.tracer.save_traces(trace_file)\n",
    "        self.logger.save_logs(log_file)\n",
    "\n",
    "        with open(token_file,\"w\") as tf:\n",
    "            json.dump({\n",
    "                \"summary\":self.token_tracker.get_summary(),\n",
    "                \"detailed_log\": self.token_tracker.token_log\n",
    "            },tf,indent=2)\n",
    "        \n",
    "        with open(metrics_file,\"w\") as mf:\n",
    "            json.dump({\n",
    "                \"summary\": self.metrics.get_summary(),\n",
    "                \"detailed_metrics\": self.metrics.metrics,\n",
    "                \"anomalies\": self.metrics.check_anomalies()\n",
    "            },f, indent=2)\n",
    "        \n",
    "        print(f\"\\n Instrumentation save:\")\n",
    "        print(f\" - Traces {trace_file}\")\n",
    "        print(f\" - Logs:{log_file}\")\n",
    "        print(f\" - Tokens: {token_file}\")\n",
    "        print(f\" - Metrics: {metrics_file}\")\n",
    "\n",
    "        # Print summary to console\n",
    "        print(f\"\\n Execution Summary\")\n",
    "        token_summary = self.token_tracker.get_summary()\n",
    "        print(f\" Cost: {token_summary[\"estimated_cost_usd\"]:.4f}\")\n",
    "        print(f\" Tokens: {token_summary[\"total_tokens\"]:,}\")\n",
    "        metric_summary = self.metrics.get_summary()\n",
    "        print(f\" Tools calls: {metric_summary[\"total_tool_calls\"]}\")\n",
    "        print(f\" Iterations: {metric_summary[\"total_iterations\"]}\")\n",
    "    \n",
    "    def print_trace_summary(self):\n",
    "        \"\"\"Print a visual summary of execution traces\"\"\"\n",
    "        traces = [trace.to_dict() for trace in self.tracer.traces]\n",
    "        TraceVisualizer.print_all_traces(traces)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa7a5c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict\n",
    "import openai\n",
    "import os\n",
    "\n",
    "## Set up the tools and tools registry\n",
    "def write_test(file_path:str, test_code: str) -> str:\n",
    "    \"\"\"Write test code to a test file\"\"\"\n",
    "    try:\n",
    "        test_dir = os.path.dirname(file_path) or \"tests\"\n",
    "        if not os.path.exists(test_dir):\n",
    "            os.makedirs(test_dir)\n",
    "\n",
    "        with open(file_path, \"w\") as f:\n",
    "            f.write(test_code)\n",
    "        return f\"Test file created: {file_path}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error writing test file {file_path: {e}}\"\n",
    "\n",
    "def run_test(file_path: str) -> str:\n",
    "    \"\"\"Run a Python test file and return results\"\"\"\n",
    "    try:\n",
    "        import subprocess\n",
    "        result = subprocess.run(\n",
    "            [\"python\",\"-m\",\"pytest\", file_path,\"-v\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=30\n",
    "        )\n",
    "        return f\"Exit code {result.returncode}\\n\\nOuput:\\n{result.stdout}\\n\\nErrors:\\n{result.stderr}\"\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return \"Test execution timed out after 30 seconds\"\n",
    "    except Exception as e:\n",
    "        return f\"Error running tests: {e}\"\n",
    "\n",
    "def read_file(file_path: str) -> str:\n",
    "    \"\"\"Read contents of a Python file\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        return f\"File not found: {file_path}\"\n",
    "    \n",
    "    with open(file_path, \"r\") as f:\n",
    "        return f.read()\n",
    "    \n",
    "def print_review(review: str):\n",
    "    print(f\"Review: {review}\")\n",
    "    return f\"Printed review: {review}\"\n",
    "\n",
    "def patch_file(filepath: str, content: str) -> str:\n",
    "    \"\"\"Writes the given content to a file, completely replacing its current content.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, \"w\") as f:\n",
    "            f.write(content)\n",
    "        return f\"File successfully updated: {filepath}. New content written.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error writing to file {filepath}: {e}\"\n",
    "        \n",
    "class ToolRegistry:\n",
    "    \"\"\"Holds available tools and dispatches them by name.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.tools: Dict[str,Callable] = {}\n",
    "    \n",
    "    def register(self, name:str, func: Callable):\n",
    "        self.tools[name] = func\n",
    "\n",
    "    def call(self, name:str, *args, **kwargs):\n",
    "        if name not in self.tools:\n",
    "            return f\"Unknown tool: {name}\"\n",
    "        return self.tools[name](*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf157b9",
   "metadata": {},
   "source": [
    "### Agent with observability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14da14c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import json\n",
    "\n",
    "class CodeReviewAgentObservable:\n",
    "    def __init__(self, tools_registry: ToolRegistry, model=\"gpt-4.1\",\n",
    "                 memory_file=\"agent_memory.json\", summarize_after=10,\n",
    "                 max_context_tokens=6000):\n",
    "        self.tools = tools_registry\n",
    "        self.model = model\n",
    "        self.conversation_history = []  # Short-term memory\n",
    "        self.memory_file = memory_file\n",
    "        self.load_long_term_memory()  # Long-term memory (key-value store)\n",
    "        self.conversation_summary = \"\"  # Summarized conversation history\n",
    "        self.summarize_after = summarize_after\n",
    "        self.turns_since_summary = 0\n",
    "        self.max_context_tokens = max_context_tokens\n",
    "        \n",
    "        # Planning-specific attributes\n",
    "        self.current_plan = []  # List of planned steps\n",
    "        self.completed_steps = []  # Track what has been done\n",
    "        self.plan_created = False\n",
    "\n",
    "        # Observability components\n",
    "        self.logger = AgentLogger(agent_id=f\"code-review-{int(time.time())}\")\n",
    "        self.logger.log(LogLevel.INFO, \"AGENT_INIT\", \"Agent initialized\",\n",
    "                       {\"model\": model, \"max_tokens\": max_context_tokens})\n",
    "        self.tracer = TraceManager()\n",
    "        self.token_tracker = TokenTracker(model=model)\n",
    "        self.metrics = MetricsCollector()\n",
    "\n",
    "        # Initialize tokenizer for the model\n",
    "        try:\n",
    "            self.tokenizer = tiktoken.encoding_for_model(model)\n",
    "        except:\n",
    "            self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Count tokens in a string\"\"\"\n",
    "        return len(self.tokenizer.encode(text))\n",
    "    \n",
    "    def trim_history_to_fit(self, system_message: str):\n",
    "        \"\"\"Remove old messages until we fit within the token budget\"\"\"\n",
    "        fixed_tokens = self.count_tokens(system_message)\n",
    "        history_tokens = sum([self.count_tokens(msg[\"content\"]) \n",
    "                            for msg in self.conversation_history])\n",
    "        total_tokens = fixed_tokens + history_tokens\n",
    "\n",
    "        while total_tokens > self.max_context_tokens and len(self.conversation_history) > 2:\n",
    "            removed_msg = self.conversation_history.pop(0)\n",
    "            total_tokens -= self.count_tokens(removed_msg[\"content\"])\n",
    "\n",
    "        return total_tokens\n",
    "\n",
    "    def summarize_history(self):\n",
    "        \"\"\"Use LLM to summarize the conversation so far.\"\"\"\n",
    "        if len(self.conversation_history) < 3:\n",
    "            return\n",
    "        \n",
    "        history_text = \"\\n\".join([\n",
    "            f\"{msg['role']}: {msg['content']}\" \n",
    "            for msg in self.conversation_history\n",
    "        ])\n",
    "\n",
    "        summary_prompt = f\"\"\"Summarize this conversation in 3-4 sentences,\n",
    "        preserving key facts, decisions, and actions taken:\n",
    "        {history_text}\n",
    "\n",
    "        Previous Summary: {self.conversation_summary or 'None'}\n",
    "        \"\"\"\n",
    "\n",
    "        response = openai.responses.create(\n",
    "            model=self.model, \n",
    "            input=[{\"role\": \"user\", \"content\": summary_prompt}]\n",
    "        )\n",
    "        self.conversation_summary = response.output_text\n",
    "\n",
    "        # Keep only the last few turns + the summary\n",
    "        recent_turns = self.conversation_history[-4:]\n",
    "        self.conversation_history = recent_turns\n",
    "        self.turns_since_summary = 0\n",
    "\n",
    "    def remember(self, key: str, value: str):\n",
    "        \"\"\"Store information in long term memory.\"\"\"\n",
    "        self.long_term_memory[key] = value\n",
    "        self.save_long_term_memory()\n",
    "    \n",
    "    def recall(self, key: str) -> str:\n",
    "        \"\"\"Retrieve information from long term memory\"\"\"\n",
    "        return self.long_term_memory.get(key, \"No memory found for this key.\")\n",
    "    \n",
    "    def get_relevant_memories(self) -> str:\n",
    "        \"\"\"Format long term memories for inclusion in prompts.\"\"\"\n",
    "        if not self.long_term_memory:\n",
    "            return \"No stored memories\"\n",
    "        \n",
    "        memories = \"\\n\".join([f\"- {k}: {v}\" for k, v in self.long_term_memory.items()])\n",
    "        return f\"Relevant memories:\\n{memories}\"\n",
    "    \n",
    "    def save_long_term_memory(self):\n",
    "        \"\"\"Persist long term memory to JSON file\"\"\"\n",
    "        try:\n",
    "            with open(self.memory_file, \"w\") as f:\n",
    "                json.dump(self.long_term_memory, f, indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save memory to {self.memory_file}: {e}\")\n",
    "\n",
    "    def load_long_term_memory(self):\n",
    "        \"\"\"Load long term memory from JSON file\"\"\"\n",
    "        if os.path.exists(self.memory_file):\n",
    "            try:\n",
    "                with open(self.memory_file, 'r') as f:\n",
    "                    self.long_term_memory = json.load(f)\n",
    "                print(f\"Loaded {len(self.long_term_memory)} memories from {self.memory_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load memory from {self.memory_file}: {e}\")\n",
    "                self.long_term_memory = {}\n",
    "        else:\n",
    "            self.long_term_memory = {}\n",
    "    \n",
    "    def create_plan(self, user_query: str) -> list:\n",
    "        \"\"\"Generate a step by step plan for the user's request\"\"\"\n",
    "        planning_prompt = f\"\"\"\n",
    "        Given this task: \"{user_query}\"\n",
    "        Create a detailed execution plan with numbered steps. \n",
    "        Each step should be a specific action.\n",
    "\n",
    "        Available tools:\n",
    "        - read_file(filepath): Read a file's contents\n",
    "        - patch_file(filepath, content): Update a file\n",
    "        - write_test(file_path, test_code): Create a test file\n",
    "        - run_test(file_path): Execute tests\n",
    "\n",
    "        Format your response as a JSON list of steps:\n",
    "        [\n",
    "            {{\"step\": 1, \"action\": \"description\", \"tool\": \"tool_name\"}},\n",
    "            {{\"step\": 2, \"action\": \"description\", \"tool\": \"tool_name\"}}\n",
    "        ]\n",
    "\n",
    "        Only include necessary steps. Be specific about which files to work with.\n",
    "        Respond with ONLY the JSON array—no markdown, no extra text.\n",
    "        \"\"\"\n",
    "\n",
    "        response = openai.responses.create(\n",
    "            model=self.model,\n",
    "            input=[{\"role\": \"user\", \"content\": planning_prompt}]\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            plan = json.loads(response.output_text)\n",
    "            self.current_plan = plan\n",
    "            self.plan_created = True\n",
    "            return plan\n",
    "        except json.JSONDecodeError:\n",
    "            self.current_plan = [\n",
    "                {\"step\": 1, \"action\": \"Proceed step by step\", \"tool\": \"read_file\"}\n",
    "            ]\n",
    "            self.plan_created = True\n",
    "            return self.current_plan\n",
    "    \n",
    "    def _build_plan_context(self, next_step) -> str:\n",
    "        \"\"\"Format plan information for the prompt\"\"\"\n",
    "        completed = \"\\n\".join([\n",
    "            f\"  ✓ Step {step['step']}: {step['action']}\" \n",
    "            for step in self.completed_steps\n",
    "        ])\n",
    "\n",
    "        if next_step:\n",
    "            current = f\"\\n→ CURRENT: Step {next_step['step']}: {next_step['action']}\"\n",
    "        else:\n",
    "            current = \"\\n→ All steps completed\"\n",
    "        \n",
    "        remaining = \"\\n\".join([\n",
    "            f\"  Step {step['step']}: {step['action']}\" \n",
    "            for step in self.current_plan[len(self.completed_steps)+1:]\n",
    "        ])\n",
    "\n",
    "        return f\"\"\"\n",
    "## Execution Plan Progress\n",
    "\n",
    "Completed:\n",
    "{completed if completed else \"  None yet\"}\n",
    "{current}\n",
    "\n",
    "Remaining:\n",
    "{remaining if remaining else \"  None\"}\n",
    "\"\"\"\n",
    "\n",
    "    def build_system_prompt(self, plan_context: str = \"\") -> str:\n",
    "        \"\"\"Construct the ReAct system prompt with current context and plan.\"\"\"\n",
    "        return f\"\"\"You are a code review assistant using the ReAct pattern with planning.\n",
    "\n",
    "## Available Tools\n",
    "- read_file(filepath): Read contents of a file\n",
    "- patch_file(filepath, content): Replace file contents entirely\n",
    "- write_test(file_path, test_code): Create a test file\n",
    "- run_test(file_path): Execute tests and return results\n",
    "\n",
    "## Context\n",
    "{self.get_relevant_memories()}\n",
    "\n",
    "Conversation summary: {self.conversation_summary or 'This is the start of the conversation.'}\n",
    "\n",
    "{plan_context}\n",
    "\n",
    "## Response Format\n",
    "\n",
    "You MUST respond with valid JSON in one of these two formats:\n",
    "\n",
    "### Format 1: When you need to use a tool\n",
    "{{\n",
    "    \"thought\": \"Your reasoning about what to do and why\",\n",
    "    \"action\": {{\n",
    "        \"tool\": \"tool_name\",\n",
    "        \"args\": [\"arg1\", \"arg2\"]\n",
    "    }}\n",
    "}}\n",
    "\n",
    "### Format 2: When the task is complete\n",
    "{{\n",
    "    \"thought\": \"Your reasoning about why the task is complete\",\n",
    "    \"answer\": \"Your final response to the user\"\n",
    "}}\n",
    "\n",
    "## Rules\n",
    "1. Always include \"thought\" explaining your reasoning\n",
    "2. Include \"action\" when you need to call a tool\n",
    "3. Include \"answer\" only when ALL plan steps are complete\n",
    "4. Never include both \"action\" and \"answer\"\n",
    "5. Respond with ONLY valid JSON—no markdown, no extra text\n",
    "6. Follow the execution plan systematically\n",
    "7. After each successful action, the system will mark that step complete\n",
    "\n",
    "## Example\n",
    "\n",
    "User: Review auth.py and fix any bugs\n",
    "\n",
    "Response 1:\n",
    "{{\"thought\": \"I need to read the file first to see its contents.\", \"action\": {{\"tool\": \"read_file\", \"args\": [\"auth.py\"]}}}}\n",
    "\n",
    "Observation: def check(u): return db.user = u\n",
    "\n",
    "Response 2:\n",
    "{{\"thought\": \"There's a bug: using = (assignment) instead of == (comparison). I'll fix it.\", \"action\": {{\"tool\": \"patch_file\", \"args\": [\"auth.py\", \"def check(u): return db.user == u\"]}}}}\n",
    "\n",
    "Observation: File successfully updated: auth.py\n",
    "\n",
    "Response 3:\n",
    "{{\"thought\": \"The bug is fixed. The comparison operator is now correct.\", \"answer\": \"Fixed auth.py: changed assignment operator (=) to comparison operator (==) in the return statement.\"}}\n",
    "\"\"\"\n",
    "\n",
    "    def think(self, user_input: str):\n",
    "        \"\"\"LLM decides which tool to use with plan awareness and observability.\"\"\"\n",
    "        self.logger.log(LogLevel.INFO, \"THINK_START\", \"Starting Reasoning\",\n",
    "                       {\"user_input\": user_input[:100]})\n",
    "        think_span_id = self.tracer.start_span(\"Think\", \"LLM_CALL\")\n",
    "        \n",
    "        try:\n",
    "            # First request: create a plan\n",
    "            if not self.plan_created:\n",
    "                plan = self.create_plan(user_query=user_input)\n",
    "                plan_summary = \"\\n\".join([\n",
    "                    f\"Step {step['step']}: {step['action']}\" \n",
    "                    for step in plan\n",
    "                ])\n",
    "                \n",
    "                # Return a special response indicating the plan was created\n",
    "                return json.dumps({\n",
    "                    \"thought\": \"I've analyzed the task and created an execution plan.\",\n",
    "                    \"plan_created\": True,\n",
    "                    \"plan\": plan_summary\n",
    "                })\n",
    "\n",
    "            # Add user message to history\n",
    "            self.conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "            self.turns_since_summary += 1\n",
    "\n",
    "            # Check if we should summarize\n",
    "            if self.turns_since_summary >= self.summarize_after:\n",
    "                self.summarize_history()\n",
    "\n",
    "            # Get current step from plan\n",
    "            next_step = None\n",
    "            if len(self.completed_steps) < len(self.current_plan):\n",
    "                next_step = self.current_plan[len(self.completed_steps)]\n",
    "            \n",
    "            # Build context with plan information\n",
    "            plan_context = self._build_plan_context(next_step)\n",
    "\n",
    "            # Include long term memory & summary in system context\n",
    "            system_message_context = self.build_system_prompt(plan_context)\n",
    "\n",
    "            self.trim_history_to_fit(system_message_context)\n",
    "            \n",
    "            # Build prompt with system instructions\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_message_context}\n",
    "            ] + self.conversation_history\n",
    "\n",
    "            # Count input tokens\n",
    "            input_text = json.dumps([msg[\"content\"] for msg in messages])\n",
    "            input_tokens = self.count_tokens(input_text)\n",
    "\n",
    "            start_time = time.time()\n",
    "            response = openai.responses.create(model=self.model, input=messages)\n",
    "            duration_ms = (time.time() - start_time) * 1000\n",
    "\n",
    "            decision = response.output_text\n",
    "\n",
    "            # Add assistant's decision to conversation history\n",
    "            self.conversation_history.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": decision\n",
    "            })\n",
    "\n",
    "            # Count output tokens and track usage\n",
    "            output_tokens = self.count_tokens(decision)\n",
    "            self.token_tracker.track_usage(input_tokens, output_tokens, \"think\")\n",
    "            self.metrics.record_llm_latency(duration_ms)\n",
    "\n",
    "            self.tracer.end_span(think_span_id, \"SUCCESS\", {\n",
    "                \"input_tokens\": input_tokens,\n",
    "                \"output_tokens\": output_tokens,\n",
    "                \"cost_usd\": self.token_tracker._calculate_cost(input_tokens, output_tokens)\n",
    "            })\n",
    "            self.logger.log(LogLevel.INFO, \"THINK_COMPLETE\", \"Reasoning Complete\",\n",
    "                           {\"decision\": decision[:200]})\n",
    "\n",
    "            return decision\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.tracer.end_span(think_span_id, \"ERROR\", {\"error\": str(e)})\n",
    "            self.logger.log(LogLevel.ERROR, \"THINK_ERROR\", f\"Think failed: {str(e)}\",\n",
    "                           {\"error\": str(e)})\n",
    "            raise\n",
    "    \n",
    "    def act(self, action: dict):\n",
    "        \"\"\"Execute the chosen tool and update plan progress with observability.\"\"\"\n",
    "        self.logger.log(LogLevel.INFO, \"ACT_START\", \"Executing action\",\n",
    "                       {\"tool\": action.get(\"tool\"), \"args\": action.get(\"args\", [])})\n",
    "        act_span_id = self.tracer.start_span(\"Act\", \"TOOL_EXECUTION\")\n",
    "\n",
    "        try:\n",
    "            tool_name = action.get(\"tool\")\n",
    "            args = action.get(\"args\", [])\n",
    "\n",
    "            # Create nested span for the specific tool\n",
    "            tool_span_id = self.tracer.start_span(f\"Tool: {tool_name}\", \"TOOL_CALL\")\n",
    "\n",
    "            self.logger.log(LogLevel.DEBUG, \"TOOL_CALL\", f\"Calling {tool_name}\",\n",
    "                           {\"tool\": tool_name, \"args\": args})\n",
    "\n",
    "            start_time = time.time()\n",
    "            result = self.tools.call(tool_name, *args)\n",
    "            duration_ms = (time.time() - start_time) * 1000\n",
    "\n",
    "            self.tracer.end_span(tool_span_id, \"SUCCESS\",\n",
    "                               {\"tool\": tool_name, \"result_size\": len(str(result))})\n",
    "\n",
    "            self.logger.log(LogLevel.INFO, \"TOOL_COMPLETE\", f\"{tool_name} completed\",\n",
    "                           {\"tool\": tool_name, \"duration_ms\": duration_ms})\n",
    "            self.metrics.record_tool_call(tool_name, duration_ms)\n",
    "\n",
    "            # Mark current step as complete\n",
    "            if len(self.completed_steps) < len(self.current_plan):\n",
    "                current_step = self.current_plan[len(self.completed_steps)]\n",
    "                self.completed_steps.append(current_step)\n",
    "\n",
    "            self.conversation_history.append({\"role\": \"system\", \"content\": result})\n",
    "            self.tracer.end_span(act_span_id, \"SUCCESS\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error executing tool: {e}\"\n",
    "            self.logger.log(LogLevel.ERROR, \"ACT_ERROR\", error_msg,\n",
    "                           {\"action\": action, \"error\": str(e)})\n",
    "            self.tracer.end_span(act_span_id, \"ERROR\", {\"error\": str(e)})\n",
    "            self.metrics.record_error(\"TOOL_ERROR\", str(e))\n",
    "            self.conversation_history.append({\n",
    "                \"role\": \"system\",\n",
    "                \"content\": error_msg\n",
    "            })\n",
    "            return error_msg\n",
    "\n",
    "    def run(self, user_query: str, max_iterations=10):\n",
    "        \"\"\"\n",
    "        Main execution loop with planning, ReAct pattern, and full observability.\n",
    "        \n",
    "        Args:\n",
    "            user_query: The user's request\n",
    "            max_iterations: Maximum number of think-act cycles\n",
    "        \n",
    "        Returns:\n",
    "            Final response string\n",
    "        \"\"\"\n",
    "        run_span_id = self.tracer.start_span(\n",
    "            f\"Agent Run: {user_query[:50]}\", \n",
    "            span_type=\"AGENT_RUN\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            current_input = user_query\n",
    "\n",
    "            for step in range(max_iterations):\n",
    "                # Create a span for each iteration\n",
    "                iter_span_id = self.tracer.start_span(\n",
    "                    name=f\"Iteration {step + 1}\",\n",
    "                    span_type=\"ITERATION\"\n",
    "                )\n",
    "                self.metrics.record_iteration()\n",
    "\n",
    "                print(f\"\\n{'-'*60}\")\n",
    "                print(f\"Step {step+1} of {max_iterations}\")\n",
    "                print(f\"{'-'*60}\")\n",
    "\n",
    "                llm_response = self.think(current_input)\n",
    "                print(f\"\\nAgent's LLM Response:\\n{llm_response}\")\n",
    "\n",
    "                # Parse the JSON response\n",
    "                try:\n",
    "                    parsed_response = json.loads(llm_response)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"\\nJSON Parse Error: {e}\")\n",
    "                    self.metrics.record_error(\"JSON_PARSE_ERROR\", str(e))\n",
    "                    current_input = (\n",
    "                        f\"Your response was not valid JSON. Error: {e}\\n\"\n",
    "                        f\"Respond with ONLY valid JSON matching the required format.\"\n",
    "                    )\n",
    "                    self.tracer.end_span(iter_span_id, \"JSON_ERROR\")\n",
    "                    continue\n",
    "\n",
    "                # Handle plan creation response\n",
    "                if parsed_response.get(\"plan_created\"):\n",
    "                    print(f\"\\nPlan Created:\")\n",
    "                    print(parsed_response.get(\"plan\", \"\"))\n",
    "                    current_input = \"Proceed with step 1 of the plan.\"\n",
    "                    self.tracer.end_span(iter_span_id, \"PLAN_CREATED\")\n",
    "                    continue\n",
    "\n",
    "                # Print thought if present\n",
    "                if \"thought\" in parsed_response:\n",
    "                    print(f\"\\nThought: {parsed_response['thought']}\")\n",
    "\n",
    "                # Check for final answer\n",
    "                if \"answer\" in parsed_response:\n",
    "                    print(f\"\\nAnswer: {parsed_response['answer']}\")\n",
    "                    print(f\"\\nProgress: {len(self.completed_steps)}/{len(self.current_plan)} steps completed\")\n",
    "                    \n",
    "                    self.tracer.end_span(iter_span_id, \"SUCCESS\",\n",
    "                                        {\"final_answer\": parsed_response[\"answer\"][:100]})\n",
    "                    self.tracer.end_span(run_span_id, \"SUCCESS\",\n",
    "                                        {\"total_iterations\": step + 1})\n",
    "                    return parsed_response[\"answer\"]\n",
    "                \n",
    "                # Execute action if present\n",
    "                if \"action\" in parsed_response:\n",
    "                    action = parsed_response[\"action\"]\n",
    "                    tool_name = action.get(\"tool\", \"unknown\")\n",
    "                    args = action.get(\"args\", [])\n",
    "\n",
    "                    print(f\"\\nAction: {tool_name}({', '.join(repr(a) for a in args)})\")\n",
    "                    \n",
    "                    observation = self.act(action)\n",
    "                    \n",
    "                    # Truncate long observations for display\n",
    "                    obs_display = observation[:500] + \"...\" if len(str(observation)) > 500 else observation\n",
    "                    print(f\"\\nObservation: {obs_display}\")\n",
    "                    \n",
    "                    current_input = f\"Observation: {observation}\"\n",
    "                    self.tracer.end_span(iter_span_id, \"SUCCESS\")\n",
    "                else:\n",
    "                    # Neither action nor answer\n",
    "                    print(\"\\nResponse missing both 'action' and 'answer'\")\n",
    "                    current_input = (\n",
    "                        \"Your response must include either 'action' (to use a tool) \"\n",
    "                        \"or 'answer' (if the task is complete). Please try again.\"\n",
    "                    )\n",
    "                    self.tracer.end_span(iter_span_id, \"INVALID_RESPONSE\")\n",
    "\n",
    "            self.tracer.end_span(run_span_id, \"MAX_ITERATIONS\",\n",
    "                               {\"completed_steps\": len(self.completed_steps),\n",
    "                                \"total_steps\": len(self.current_plan)})\n",
    "            \n",
    "            # Check for anomalies at the end\n",
    "            warnings = self.metrics.check_anomalies()\n",
    "            if warnings:\n",
    "                print(\"\\n⚠️ Performance Warnings:\")\n",
    "                for warning in warnings:\n",
    "                    print(f\"  {warning}\")\n",
    "\n",
    "            print(f\"\\nMaximum Iterations ({max_iterations}) reached\")\n",
    "            print(f\"Progress: {len(self.completed_steps)}/{len(self.current_plan)} steps completed\")\n",
    "            return \"Task Incomplete: Max steps reached\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.tracer.end_span(run_span_id, \"ERROR\", {\"error\": str(e)})\n",
    "            self.metrics.record_error(\"RUNTIME_ERROR\", str(e))\n",
    "            raise\n",
    "        finally:\n",
    "            self.save_instrumentation()\n",
    "\n",
    "    def save_instrumentation(self, trace_file=\"traces.json\", log_file=\"log.json\",\n",
    "                             token_file=\"tokens.json\", metrics_file=\"metrics.json\"):\n",
    "        \"\"\"Save all observability data to files.\"\"\"\n",
    "        self.tracer.save_traces(trace_file)\n",
    "        self.logger.save_logs(log_file)\n",
    "\n",
    "        with open(token_file, \"w\") as tf:\n",
    "            json.dump({\n",
    "                \"summary\": self.token_tracker.get_summary(),\n",
    "                \"detailed_log\": self.token_tracker.token_log\n",
    "            }, tf, indent=2)\n",
    "        \n",
    "        with open(metrics_file, \"w\") as mf:\n",
    "            json.dump({\n",
    "                \"summary\": self.metrics.get_summary(),\n",
    "                \"detailed_metrics\": self.metrics.metrics,\n",
    "                \"anomalies\": self.metrics.check_anomalies()\n",
    "            }, mf, indent=2)\n",
    "        \n",
    "        print(f\"\\n📊 Instrumentation saved:\")\n",
    "        print(f\"  - Traces: {trace_file}\")\n",
    "        print(f\"  - Logs: {log_file}\")\n",
    "        print(f\"  - Tokens: {token_file}\")\n",
    "        print(f\"  - Metrics: {metrics_file}\")\n",
    "\n",
    "        # Print summary to console\n",
    "        print(f\"\\n📈 Execution Summary\")\n",
    "        token_summary = self.token_tracker.get_summary()\n",
    "        print(f\"  Cost: ${token_summary['estimated_cost_usd']:.4f}\")\n",
    "        print(f\"  Tokens: {token_summary['total_tokens']:,}\")\n",
    "        metric_summary = self.metrics.get_summary()\n",
    "        print(f\"  Tool calls: {metric_summary['total_tool_calls']}\")\n",
    "        print(f\"  Iterations: {metric_summary['total_iterations']}\")\n",
    "    \n",
    "    def print_trace_summary(self):\n",
    "        \"\"\"Print a visual summary of execution traces\"\"\"\n",
    "        traces = [trace.to_dict() for trace in self.tracer.traces]\n",
    "        TraceVisualizer.print_all_traces(traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57b0657f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 memories from agent_memory.json\n",
      "[INFO] AGENT_INIT: Agent initialized\n",
      "\n",
      "------------------------------------------------------------\n",
      "Step 1 of 10\n",
      "------------------------------------------------------------\n",
      "[INFO] THINK_START: Starting Reasoning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asa\\AppData\\Local\\Temp\\ipykernel_44380\\3015332317.py:23: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat(),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agent's LLM Response:\n",
      "{\"thought\": \"I've analyzed the task and created an execution plan.\", \"plan_created\": true, \"plan\": \"Step 1: Read the contents of sample.py to review its implementation.\"}\n",
      "\n",
      "Plan Created:\n",
      "Step 1: Read the contents of sample.py to review its implementation.\n",
      "\n",
      "------------------------------------------------------------\n",
      "Step 2 of 10\n",
      "------------------------------------------------------------\n",
      "[INFO] THINK_START: Starting Reasoning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asa\\AppData\\Local\\Temp\\ipykernel_44380\\1504158801.py:24: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat(),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] THINK_COMPLETE: Reasoning Complete\n",
      "\n",
      "Agent's LLM Response:\n",
      "{\"thought\": \"I need to read the contents of sample.py to begin the review process.\", \"action\": {\"tool\": \"read_file\", \"args\": [\"sample.py\"]}}\n",
      "\n",
      "Thought: I need to read the contents of sample.py to begin the review process.\n",
      "\n",
      "Action: read_file('sample.py')\n",
      "[INFO] ACT_START: Executing action\n",
      "[DEBUG] TOOL_CALL: Calling read_file\n",
      "[INFO] TOOL_COMPLETE: read_file completed\n",
      "\n",
      "Observation: def divide(a, b):\n",
      "    return a / b\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Step 3 of 10\n",
      "------------------------------------------------------------\n",
      "[INFO] THINK_START: Starting Reasoning\n",
      "[INFO] THINK_COMPLETE: Reasoning Complete\n",
      "\n",
      "Agent's LLM Response:\n",
      "{\"thought\": \"The current implementation of divide does not handle division by zero, which will raise a ZeroDivisionError. I should fix this bug by adding a check to prevent division by zero and return an appropriate value or raise a more informative exception.\", \"action\": {\"tool\": \"patch_file\", \"args\": [\"sample.py\", \"def divide(a, b):\\n    if b == 0:\\n        raise ValueError('Division by zero is not allowed')\\n    return a / b\\n\"]}}\n",
      "\n",
      "Thought: The current implementation of divide does not handle division by zero, which will raise a ZeroDivisionError. I should fix this bug by adding a check to prevent division by zero and return an appropriate value or raise a more informative exception.\n",
      "\n",
      "Action: patch_file('sample.py', \"def divide(a, b):\\n    if b == 0:\\n        raise ValueError('Division by zero is not allowed')\\n    return a / b\\n\")\n",
      "[INFO] ACT_START: Executing action\n",
      "[DEBUG] TOOL_CALL: Calling patch_file\n",
      "[INFO] TOOL_COMPLETE: patch_file completed\n",
      "\n",
      "Observation: File successfully updated: sample.py. New content written.\n",
      "\n",
      "------------------------------------------------------------\n",
      "Step 4 of 10\n",
      "------------------------------------------------------------\n",
      "[INFO] THINK_START: Starting Reasoning\n",
      "[INFO] THINK_COMPLETE: Reasoning Complete\n",
      "\n",
      "Agent's LLM Response:\n",
      "{\"thought\": \"The bug is fixed by adding a check for division by zero and raising a ValueError. The implementation is now safer and more informative for users.\", \"answer\": \"Reviewed and fixed sample.py: added error handling for division by zero by raising a ValueError if b is zero.\"}\n",
      "\n",
      "Thought: The bug is fixed by adding a check for division by zero and raising a ValueError. The implementation is now safer and more informative for users.\n",
      "\n",
      "Answer: Reviewed and fixed sample.py: added error handling for division by zero by raising a ValueError if b is zero.\n",
      "\n",
      "Progress: 1/1 steps completed\n",
      "\n",
      "📊 Instrumentation saved:\n",
      "  - Traces: traces.json\n",
      "  - Logs: log.json\n",
      "  - Tokens: tokens.json\n",
      "  - Metrics: metrics.json\n",
      "\n",
      "📈 Execution Summary\n",
      "  Cost: $0.0070\n",
      "  Tokens: 2,207\n",
      "  Tool calls: 2\n",
      "  Iterations: 4\n",
      "\n",
      "============================================================\n",
      "EXECUTION TRACES\n",
      "============================================================\n",
      "Trace 1:\n",
      "✅ Status: SUCCESS Agent Run: Review sample.py (AGENT_RUN) - 8535ms\n",
      "  ❌ Status: PLAN_CREATED Iteration 1 (ITERATION) - 3252ms\n",
      "    ❌ Status: running Think (LLM_CALL) - 8535ms\n",
      "  ✅ Status: SUCCESS Iteration 2 (ITERATION) - 1335ms\n",
      "    ✅ Status: SUCCESS Think (LLM_CALL) - 1335ms [$0.0018]\n",
      "    ✅ Status: SUCCESS Act (TOOL_EXECUTION) - 0ms\n",
      "      ✅ Status: SUCCESS Tool: read_file (TOOL_CALL) - 0ms\n",
      "  ✅ Status: SUCCESS Iteration 3 (ITERATION) - 2511ms\n",
      "    ✅ Status: SUCCESS Think (LLM_CALL) - 2511ms [$0.0027]\n",
      "    ✅ Status: SUCCESS Act (TOOL_EXECUTION) - 0ms\n",
      "      ✅ Status: SUCCESS Tool: patch_file (TOOL_CALL) - 0ms\n",
      "  ✅ Status: SUCCESS Iteration 4 (ITERATION) - 1436ms\n",
      "    ✅ Status: SUCCESS Think (LLM_CALL) - 1436ms [$0.0026]\n"
     ]
    }
   ],
   "source": [
    "registry = ToolRegistry()\n",
    "registry.register(\"read_file\",read_file)\n",
    "registry.register(\"print_review\", print_review)\n",
    "registry.register(\"write_test\",write_test)\n",
    "registry.register(\"patch_file\",patch_file)\n",
    "registry.register(\"run_test\",run_test)\n",
    "\n",
    "agent = CodeReviewAgentObservable(tools_registry=registry,model=\"gpt-4.1\",max_context_tokens=8000)\n",
    "\n",
    "user_query = \"Review sample.py\"\n",
    "\n",
    "result = agent.run(user_query)\n",
    "\n",
    "agent.print_trace_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01866bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
