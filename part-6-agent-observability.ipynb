{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b65e5bd1",
   "metadata": {},
   "source": [
    "# Observability and Debugging AI Agents\n",
    "As your AI Agent grows more sophisticated, handling multi step plans, maintaining memory and using multiple tools, understanding what it's doing and why becomes important.  \n",
    "Without prooper observability, debugging agent failures feels like operating in the dark\n",
    "\n",
    "I this tutorial, we will add instrumentation to our code review agent, covering:\n",
    "* **Structured logging** for every agent action\n",
    "* **Trace visualization** to understand thought -> tool -> result chains\n",
    "* **Token usage and cost tracking** for budget management\n",
    "* **Performance metrics** to identify bottlenecks\n",
    "* **Error detection** for loops and excessive tool usage\n",
    "\n",
    "By the end, you will have patterns for instrumenting any agent system - patterns that are similar to professional observability tools.\n",
    "\n",
    "## What is Observability?\n",
    "**Observability** is the ability to understand what's happening inside a system by examining its outputs. Unlike traditional monitoring, which answers *\"is it working?\"*, observability answers *\"why isn't it working?\"* and *\"what exactly happened?\"*\n",
    "Think of it like this: A dashboard showing \"CPU at 80%\" is monitoring. Being able to trace why CPU spiked—seeing that it happened during a specific LLM call processing a 10,000-token prompt, which triggered three tool calls, one of which failed and retried—that's observability.\n",
    "### The Three Pillars of Observability\n",
    "Observability is built on three types of data, often called \"telemetry\":\n",
    "\n",
    "1. **Logs**\n",
    "Individual event records with timestamps.\n",
    "    * **What:** \"At 14:32:15, the agent called read_file('calculator.py')\"\n",
    "    * **When to use:** Debugging specific events, understanding what happened\n",
    "    * **Example:** Error messages, audit trails, state changes\n",
    "\n",
    "2. **Metrics**\n",
    "Aggregated numerical measurements over time.\n",
    "    * **What:** \"Average LLM latency: 450ms\" or \"Tool calls per minute: 12\"\n",
    "    * **When to use:** Monitoring trends, detecting anomalies, capacity planning\n",
    "    * **Example:** Request counts, duration histograms, error rates\n",
    "\n",
    "3. **Traces**\n",
    "Connected records showing how a single request flows through your system.\n",
    "    * **What:** A tree showing: User query → Agent thinks → Calls read_file → Agent thinks → Calls analyze_code → Returns answer\n",
    "    * **When to use:** Understanding execution flow, finding bottlenecks\n",
    "    * **Example:** The full journey of one agent task from start to finish\n",
    "\n",
    "### How They Work Together\n",
    "Imagine your agent fails on a user request:\n",
    "\n",
    "1. **Metrics** alert you: \"Error rate jumped to 15%\"\n",
    "2. **Traces show you:** \"Failures happening after the 3rd tool call in multi-step plans\"\n",
    "3. **Logs reveal:** \"Tool 'patch_file' threw 'Permission Denied' error\"\n",
    "\n",
    "Each pillar provides different insight; together they give you complete visibility.\n",
    "### Why This Matters for AI Agents\n",
    "Traditional software follows predictable code paths. AI agents are non-deterministic:\n",
    "\n",
    "* The LLM might choose different tools each run\n",
    "* Reasoning steps vary based on context\n",
    "* Failures can cascade through multi-step plans\n",
    "\n",
    "Without observability, debugging feels like guesswork. With it, you can:\n",
    "\n",
    "* See the exact sequence of thoughts and actions\n",
    "* Identify why the agent got stuck in a loop\n",
    "* Track which operations consume the most tokens (and cost)\n",
    "* Understand performance bottlenecks\n",
    "\n",
    "In this article, we'll build all three pillars; logs, metrics, and traces—into our code review agent, giving you complete visibility into its behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d287a08a",
   "metadata": {},
   "source": [
    "### Structured Logging\n",
    "We want to add structured logs that capture rich metadata about every agent action.\n",
    "\n",
    "### Add a logging layer\n",
    "* **Timestamps:** Every log gets a UTC timestamp for analysis\n",
    "* **Event types:** Lets us categorize logs (e.g. \"TOOL_CALL\",\"LLM_REQUEST\") for filtering\n",
    "* **Metadata:** For context specific information\n",
    "* **Agent ID:** Identify agent instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af7d0882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "from typing import Any, Optional\n",
    "\n",
    "class LogLevel(Enum):\n",
    "    DEBUG = \"DEBUG\"\n",
    "    INFO = \"INFO\"\n",
    "    WARNING = \"WARNING\"\n",
    "    ERROR = \"ERROR\"\n",
    "\n",
    "class AgentLogger:\n",
    "    \"\"\"Structured logging for agent actions\"\"\"\n",
    "    def __init__(self, agent_id:str=\"agent-1\"):\n",
    "        self.agent_id = agent_id\n",
    "        self.logs = []\n",
    "\n",
    "    def log(self, level:LogLevel,event_type:str,message:str, metadata: Optional[dict[str,Any]]):\n",
    "        \"\"\"Create a structured log entry\"\"\"\n",
    "        log_entry = {\n",
    "            \"timestamp\": datetime.utcnow().isoformat(),\n",
    "            \"agent_id\": self.agent_id,\n",
    "            \"level\": level.value,\n",
    "            \"event_type\": event_type,\n",
    "            \"message\":message,\n",
    "            \"metadata\": metadata or {}\n",
    "        }\n",
    "        self.logs.append(log_entry)\n",
    "\n",
    "        # Also print for real-time feedback\n",
    "        print(f\"[{level.value}] {event_type}:{message}\")\n",
    "    \n",
    "    def get_logs(self, event_type:Optional[str] = None) -> list:\n",
    "        \"\"\"Retrieve logs optionally filtered by event type\"\"\"\n",
    "        if event_type:\n",
    "            return [log for log in self.logs if log[\"event_type\"]==event_type]\n",
    "        \n",
    "        return self.logs\n",
    "\n",
    "    def save_logs(self,file_path:str):\n",
    "        \"\"\"Persist logs to a JSON file\"\"\"\n",
    "        with open(file_path,\"w\") as f:\n",
    "            json.dump(self.logs,f,indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0567b160",
   "metadata": {},
   "source": [
    "### Integrating logging into the Agent\n",
    "* Add logging to ``__init()__``\n",
    "```python\n",
    "class CodeReviewAgentObservable:\n",
    "    def __init__(self,tools_registry: ToolRegistry, \n",
    "                model=\"gpt-4.1\",memory_file=\"agent_memory.json\",\n",
    "                summarize_after=10,max_context_tokens=6000):\n",
    "        ## ...Exising init code\n",
    "\n",
    "        # Add logger\n",
    "        self.logger = AgentLogger(agent_id=f\"code-reivew-{int(time.time())}\")\n",
    "\n",
    "        ## ...The rest of init code\n",
    "```\n",
    "* Add logging to `think()`\n",
    "```python\n",
    "def think(self, user_input:str):\n",
    "        \"\"\"LLM enhanced thinking with logging\"\"\"\n",
    "        self.logger.log(LogLevel.INFO,\"THINK_START\",\"Starting Reasoning\",{\"user_input\":user_input[:100]})\n",
    "\n",
    "        #....rest of think code\n",
    "        response = openai.responses.create(model=self.model, input=messages)\n",
    "\n",
    "        duration_ms = (time.time() - start_time) * 1000\n",
    "\n",
    "        decision = response.output_text\n",
    "\n",
    "        # Add assistant's decision to conversation history\n",
    "        self.conversation_history.append({\n",
    "            \"role\":\"assistant\",\n",
    "            \"content\": decision\n",
    "        })\n",
    "\n",
    "        # Log end of thinking\n",
    "        self.logger.log(LogLevel.INFO,\"THINK_COMPLETE\",\"Reasoning Complete\",{\"decision\":decision})\n",
    "\n",
    "        #...rest of thinking        \n",
    "```\n",
    "* Add logging to `act()`\n",
    "```python\n",
    "def act(self, decision: str):\n",
    "    \"\"\"Execute tool with logging\"\"\"\n",
    "    self.logger.log(LogLevel.INFO, \"ACT_START\", \n",
    "                   \"Executing action\",\n",
    "                   {\"decision\": decision[:200]})\n",
    "    \n",
    "    try:\n",
    "        parsed = json.loads(decision)\n",
    "        tool_name = parsed[\"tool\"]\n",
    "        args = parsed.get(\"args\", [])\n",
    "        \n",
    "        self.logger.log(LogLevel.DEBUG, \"TOOL_CALL\", \n",
    "                       f\"Calling {tool_name}\",\n",
    "                       {\"tool\": tool_name, \"args\": args})\n",
    "        \n",
    "        start_time = time.time()\n",
    "        result = self.tools.call(tool_name, *args)\n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        self.logger.log(LogLevel.INFO, \"TOOL_COMPLETE\", \n",
    "                       f\"{tool_name} completed\",\n",
    "                       {\"tool\": tool_name, \"duration_ms\": duration * 1000,\n",
    "                        \"result_length\": len(str(result))})\n",
    "        \n",
    "        # ... rest of act logic ...\n",
    "        \n",
    "    except Exception as e:\n",
    "        self.logger.log(LogLevel.ERROR, \"ACT_ERROR\", \n",
    "                       f\"Action failed: {str(e)}\",\n",
    "                       {\"decision\": decision, \"error\": str(e)})\n",
    "        # ... error handling ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedd4cfe",
   "metadata": {},
   "source": [
    "## Trace Hierarchies\n",
    "Logs are flat, they dont show *relationships* between operation. A trace captures the nested structure of agent execution.\n",
    "\n",
    "### Building a Trace Structure\n",
    "* **Spans:** Individual units of work\n",
    "* **Hierarchy:** Child spans nest under parents to show causality\n",
    "* **Context propagation:** `current_span_id` tracks where we are in the call stack\n",
    "* **Lazy evaluation:** Only root spans are saved to `traces`\n",
    "* **Trace Vizualizer:** Display traces in a way that is easy to read and interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7feededf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import uuid\n",
    "\n",
    "class Span:\n",
    "    \"\"\"Represents a single unit of work in a trace\"\"\"\n",
    "\n",
    "    def __init__(self, name:str,span_type:str,parent_id:Optional[str]=None):\n",
    "        self.span_id = str(uuid.uuid4())[:8]\n",
    "        self.parent_id = parent_id\n",
    "        self.name = name\n",
    "        self.span_type = span_type\n",
    "        self.start_time = time.time()\n",
    "        self.end_time = None\n",
    "        self.status = \"running\"\n",
    "        self.metadata = {}\n",
    "        self.children : list[Span] =[]\n",
    "\n",
    "    def end(self,status:str=\"success\",metadata:Optional[dict]=None):\n",
    "        \"\"\"Mark span as complete\"\"\"\n",
    "        self.end_time= time.time()\n",
    "        self.status = status\n",
    "        if metadata:\n",
    "            self.metadata.update(metadata)\n",
    "    \n",
    "    def duration_ms(self) -> float:\n",
    "        \"\"\"Calculate span duration in milliseconds\"\"\"\n",
    "        if self.end_time:\n",
    "            return (self.end_time - self.start_time) * 1000\n",
    "        return (time.time() - self.start_time) * 1000\n",
    "    \n",
    "    def add_child(self, child:'Span'):\n",
    "        \"\"\"Add child span\"\"\"\n",
    "        self.children.append(child)\n",
    "\n",
    "    def to_dict(self) -> dict:\n",
    "        \"\"\"Convert span to dict for serializatioin\"\"\"\n",
    "        return {\n",
    "            \"span_id\": self.span_id,\n",
    "            \"parent_id\": self.parent_id,\n",
    "            \"name\": self.name,\n",
    "            \"type\": self.span_type,\n",
    "            \"start_time\": self.start_time,\n",
    "            \"end_time\":self.end_time,\n",
    "            \"duration_ms\": self.duration_ms(),\n",
    "            \"status\":self.status,\n",
    "            \"metadata\": self.metadata,\n",
    "            \"children\":[child.to_dict() for child in self.children]\n",
    "        }\n",
    "\n",
    "class TraceManager:\n",
    "    \"\"\"Manages execution traces\"\"\"\n",
    "    def __init__(self):\n",
    "        self.traces = []\n",
    "        self.active_spans = {}\n",
    "        self.current_span_id = None\n",
    "    \n",
    "    def start_span(self, name:str, span_type: str) -> str:\n",
    "        \"\"\"Create and activate a new span\"\"\"\n",
    "        parent_id = self.current_span_id\n",
    "        span = Span(name, span_type,parent_id)\n",
    "        self.active_spans[span.span_id] = span\n",
    "\n",
    "        if parent_id and parent_id in self.active_spans:\n",
    "            self.active_spans[parent_id].add_child(span)\n",
    "        \n",
    "        self.current_span_id = span.span_id\n",
    "        return span.span_id\n",
    "    \n",
    "    def end_span(self, span_id:str, status: str = \"success\",metadata: Optional[dict] = None):\n",
    "        \"\"\"Complete a span and update current span\"\"\"\n",
    "        if span_id in self.active_spans:\n",
    "            span = self.active_spans[span_id]\n",
    "            span.end(status, metadata)\n",
    "\n",
    "            # Move current span to parent\n",
    "            if span.parent_id:\n",
    "                self.current_span_id = span.parent_id\n",
    "            else:\n",
    "                # Root span completed - save trace\n",
    "                self.traces.append(span)\n",
    "                self.current_span_id = None\n",
    "\n",
    "    def get_current_span(self) -> Optional[Span]:\n",
    "        \"\"\"Get the currently active span\"\"\"\n",
    "        if self.current_span_id:\n",
    "            return self.active_spans.get(self.current_span_id)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def save_traces(self,file_path:str):\n",
    "        \"\"\"Save all traces to a file\"\"\"\n",
    "        traces_data = [trace.to_dict() for trace in self.traces]\n",
    "        with open(file_path,\"w\") as f:\n",
    "            json.dump(traces_data,f,indent=2)\n",
    "            \n",
    "class TraceVisualizer:\n",
    "    \"\"\"Generate human readable trace vizualizations\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def format_trace(span: dict, indent: int = 0) -> str:\n",
    "        \"\"\"receursively format a trace and it's children\"\"\"\n",
    "        prefix = \"  \" * indent\n",
    "        \n",
    "        duration = span[\"duration_ms\"]\n",
    "        duration_str = f\"{duration:.0f}ms\"\n",
    "\n",
    "        status_icon = \"☑️\" if span[\"status\"] == \"SUCCESS\" else \"❌\"\n",
    "\n",
    "        # Build line\n",
    "        line = f\"{prefix}{status_icon} {span[\"name\"]} ({span[\"type\"]}) - {duration_str}\"\n",
    "\n",
    "        if span.get(\"metadata\"):\n",
    "            metadata = span[\"metadata\"]\n",
    "            if \"cost_usd\" in metadata:\n",
    "                line += f\"[${metadata[\"cost_usd\"]:.4f}]\"\n",
    "            if \"error\" in metadata:\n",
    "                line += f\" [ERROR: {metadata[\"error\"]}]\"\n",
    "\n",
    "        lines = [line]\n",
    "\n",
    "        # Recusrively format children\n",
    "        for child in span.get(\"children\", []):\n",
    "            lines.append(TraceVisualizer.format_trace(child,indent + 1))\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "    \n",
    "    @staticmethod\n",
    "    def print_all_traces(traces: List[dict]):\n",
    "        \"\"\"Print all traces in a printable format\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"EXECUTION TRACES\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        for i, trace in enumerate(traces, 1):\n",
    "            print(f\"Trace {i}:\")\n",
    "            print(TraceVisualizer.format_trace(trace))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51de944",
   "metadata": {},
   "source": [
    "### Integrate Tracing into the agent\n",
    "* Add the trace manager to the agent\n",
    "```python\n",
    "class CodeReviewAgentObservable:\n",
    "    def __init__(self, tools_registry: ToolRegistry, ...):\n",
    "        # ... existing init ...\n",
    "        self.logger = AgentLogger(agent_id=f\"code-review-{int(time.time())}\")\n",
    "        self.tracer = TraceManager()\n",
    "```\n",
    "\n",
    "* Update the `run()` method to add a root span\n",
    "```python\n",
    "def run(self, user_query: str, max_iterations=10):\n",
    "    \"\"\"Main execution loop with tracing\"\"\"\n",
    "    \n",
    "    # Create root span for entire run\n",
    "    run_span_id = self.tracer.start_span(\n",
    "        name=f\"Agent Run: {user_query[:50]}\", \n",
    "        span_type=\"AGENT_RUN\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        step = 0\n",
    "        current_input = user_query\n",
    "        \n",
    "        while step < max_iterations:\n",
    "            # Create span for each iteration\n",
    "            iter_span_id = self.tracer.start_span(\n",
    "                name=f\"Iteration {step + 1}\",\n",
    "                span_type=\"ITERATION\"\n",
    "            )\n",
    "            \n",
    "            print(f\"\\n--- Step {step+1} ---\")\n",
    "            \n",
    "            llm_response = self.think(current_input)\n",
    "            \n",
    "            # ... existing logic ...\n",
    "            \n",
    "            if \"Answer:\" in llm_response:\n",
    "                final_answer = llm_response.split(\"Answer:\", 1)[1].strip()\n",
    "                self.tracer.end_span(iter_span_id, \"SUCCESS\", \n",
    "                                    {\"final_answer\": final_answer[:100]})\n",
    "                self.tracer.end_span(run_span_id, \"SUCCESS\",\n",
    "                                    {\"total_iterations\": step + 1})\n",
    "                return final_answer\n",
    "                \n",
    "            if \"Action:\" in llm_response:\n",
    "                action_line = llm_response.split(\"Action:\", 1)[1].split(\"\\n\")[0].strip()\n",
    "                tool_result = self.act(action_line)\n",
    "                current_input = f\"Observation:{tool_result}\"\n",
    "                \n",
    "            self.tracer.end_span(iter_span_id, \"SUCCESS\")\n",
    "            step += 1\n",
    "            \n",
    "        # Max iterations reached\n",
    "        self.tracer.end_span(run_span_id, \"MAX_ITERATIONS\",\n",
    "                           {\"completed_steps\": len(self.completed_steps),\n",
    "                            \"total_steps\": len(self.current_plan)})\n",
    "        return \"Task Incomplete: Max steps reached\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        self.tracer.end_span(run_span_id, \"ERROR\", {\"error\": str(e)})\n",
    "        raise\n",
    "```\n",
    "\n",
    "* add spans to the `think()` method\n",
    "```python\n",
    "def think(self, user_input: str):\n",
    "    \"\"\"Reasoning with tracing\"\"\"\n",
    "    think_span_id = self.tracer.start_span(\"Think\", \"LLM_CALL\")\n",
    "    \n",
    "    try:\n",
    "        # ... existing think logic ...\n",
    "        \n",
    "        response = openai.responses.create(model=self.model, input=messages)\n",
    "        decision = response.output_text\n",
    "        \n",
    "        self.tracer.end_span(think_span_id, \"SUCCESS\",\n",
    "                           {\"input_length\": len(user_input),\n",
    "                            \"output_length\": len(decision)})\n",
    "        return decision\n",
    "        \n",
    "    except Exception as e:\n",
    "        self.tracer.end_span(think_span_id, \"ERROR\", {\"error\": str(e)})\n",
    "        raise\n",
    "```\n",
    "* Add spans to the `act()` method\n",
    "```python\n",
    "def act(self, decision: str):\n",
    "    \"\"\"Tool execution with tracing\"\"\"\n",
    "    act_span_id = self.tracer.start_span(\"Act\", \"TOOL_EXECUTION\")\n",
    "    \n",
    "    try:\n",
    "        parsed = json.loads(decision)\n",
    "        tool_name = parsed[\"tool\"]\n",
    "        args = parsed.get(\"args\", [])\n",
    "        \n",
    "        # Create nested span for the specific tool\n",
    "        tool_span_id = self.tracer.start_span(\n",
    "            f\"Tool: {tool_name}\",\n",
    "            \"TOOL_CALL\"\n",
    "        )\n",
    "        \n",
    "        result = self.tools.call(tool_name, *args)\n",
    "        \n",
    "        self.tracer.end_span(tool_span_id, \"SUCCESS\",\n",
    "                           {\"tool\": tool_name, \"result_size\": len(str(result))})\n",
    "        \n",
    "        # ... rest of act logic ...\n",
    "        \n",
    "        self.tracer.end_span(act_span_id, \"SUCCESS\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        self.tracer.end_span(act_span_id, \"ERROR\", {\"error\": str(e)})\n",
    "        return f\"Error executing tool: {e}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9456c4b6",
   "metadata": {},
   "source": [
    "## Token Usage and Cost Tracking\n",
    "LLM costs can add up quickly, Let's track token usage and estimate costs per operation\n",
    "\n",
    "### Token Counter\n",
    "* Keep track of input and output token counts\n",
    "* Can calculate estimated LLM calls cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35aff225",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenTracker:\n",
    "    \"\"\"Track token usage and estimate costs\"\"\"\n",
    "\n",
    "    # Pricing per 1M tokens \n",
    "    PRICING = {\n",
    "        \"gpt-4.1\": {\"input\":2.50,\"output\":10.00},\n",
    "        \"gpt-4.1-mini\":{\"input\":0.15,\"output\":0.60}\n",
    "    }\n",
    "\n",
    "    def __init__(self,model:str):\n",
    "        self.model = model\n",
    "        self.total_input_tokens = 0\n",
    "        self.total_output_tokens = 0\n",
    "        self.call_count = 0\n",
    "        self.token_log = []\n",
    "    \n",
    "    def track_usage(self, input_tokens: int, output_tokens: int,operation:str = \"llm_call\"):\n",
    "        self.total_input_tokens += input_tokens\n",
    "        self.total_output_tokens += output_tokens\n",
    "        self.call_count += 1\n",
    "\n",
    "        entry = {\n",
    "            \"timestamp\": datetime.utcnow().isoformat(),\n",
    "            \"operation\": operation,\n",
    "            \"input_tokens\": input_tokens,\n",
    "            \"output_tokens\": output_tokens,\n",
    "            \"cost_usd\": self._calculate_cost(input_tokens,output_tokens)\n",
    "        }\n",
    "        self.token_log.append(entry)\n",
    "    \n",
    "    def _calculate_cost(self,input_tokens:int, output_tokens:int) -> float:\n",
    "        \"\"\"Calculate cost in USD\"\"\"\n",
    "        if self.model not in self.PRICING:\n",
    "            return 0.0\n",
    "        pricing = self.PRICING[self.model]\n",
    "        input_cost = (input_tokens/1000000) * pricing[\"input\"]\n",
    "        output_cost = (output_tokens/1000000) * pricing[\"output\"]\n",
    "        return input_cost + output_cost\n",
    "    \n",
    "    def get_summary(self) -> dict:\n",
    "        \"\"\"Get usage summary\"\"\"\n",
    "        return {\n",
    "            \"model\":self.model,\n",
    "            \"total_call\": self.call_count,\n",
    "            \"total_input_tokens\": self.total_input_tokens,\n",
    "            \"total_output_tokens\": self.total_output_tokens,\n",
    "            \"total_tokens\": self.total_input_tokens + self.total_output_tokens,\n",
    "            \"estimated_cost_usd\": self._calculate_cost(self.total_input_tokens,self.total_output_tokens)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd87f8fd",
   "metadata": {},
   "source": [
    "### Add Token Tracking to the Agent\n",
    "* Update agent *initialization* to add the tracker\n",
    "* Update `think()` method\n",
    "\n",
    "```python\n",
    "class CodeReviewAgentObservable:\n",
    "    def __init__(self, tools_registry: ToolRegistry, model=\"gpt-4.1\", ...):\n",
    "        # ... existing init ...\n",
    "        self.token_tracker = TokenTracker(model)\n",
    "        \n",
    "    def think(self, user_input: str):\n",
    "        \"\"\"Reasoning with token tracking\"\"\"\n",
    "        think_span_id = self.tracer.start_span(\"Think\", \"LLM_CALL\")\n",
    "        \n",
    "        # ... build messages ...\n",
    "        \n",
    "        # Count input tokens\n",
    "        input_text = json.dumps([msg[\"content\"] for msg in messages])\n",
    "        input_tokens = self.count_tokens(input_text)\n",
    "        \n",
    "        response = openai.responses.create(model=self.model, input=messages)\n",
    "        decision = response.output_text\n",
    "        \n",
    "        # Count output tokens\n",
    "        output_tokens = self.count_tokens(decision)\n",
    "        \n",
    "        # Track usage\n",
    "        self.token_tracker.track_usage(input_tokens, output_tokens, \"think\")\n",
    "        \n",
    "        self.tracer.end_span(think_span_id, \"SUCCESS\", {\n",
    "            \"input_tokens\": input_tokens,\n",
    "            \"output_tokens\": output_tokens,\n",
    "            \"cost_usd\": self.token_tracker._calculate_cost(input_tokens, output_tokens)\n",
    "        })\n",
    "        \n",
    "        # ... rest of think ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f332eb",
   "metadata": {},
   "source": [
    "### Performance Metrics and Anomaly Detection\n",
    "Track metrics to identify performance issues and agent misbehaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdbbc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "class MetricsCollector:\n",
    "    \"\"\"Collect and analyze performance metrics\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            \"iteration_count\": 0,\n",
    "            \"tool_calls\": {}, # tool_name: count\n",
    "            \"tool_latencies\":{}, # tool_name: durations\n",
    "            \"llm_latencies\": [],\n",
    "            \"errors\": [],\n",
    "            \"loop_detection\": [] # Track repeated tool calls\n",
    "        }\n",
    "        self.last_n_tools = [] # Sliding window for loop detection\n",
    "    \n",
    "    def record_iteration(self):\n",
    "        \"\"\"Increment iteration counter\"\"\"\n",
    "        self.metrics[\"iteration_count\"] += 1\n",
    "    \n",
    "    def record_tool_call(self, tool_name: str, duration_ms: float):\n",
    "        \"\"\"Record a tool invocation\"\"\"\n",
    "        if tool_name not in self.metrics[\"tool_calls\"]:\n",
    "            self.metrics[\"tool_calls\"][tool_name] = 0\n",
    "            self.metrics[\"tool_latencies\"][tool_name] = []\n",
    "        \n",
    "        self.metrics[\"tool_calls\"][tool_name] += 1\n",
    "        self.metrics[\"tool_latencies\"][tool_name].append(duration_ms)\n",
    "\n",
    "        # Loop detection: track last 5 tool calls\n",
    "        self.last_n_tools.append(tool_name)\n",
    "        if len(self.last_n_tools) > 5:\n",
    "            self.last_n_tools.pop(0)\n",
    "        \n",
    "        #Check for repeated patterns\n",
    "        if len(self.last_n_tools) == 5:\n",
    "            if len(set(self.last_n_tools)) >=2: # 1 or 2 two unique tool calls in last 5\n",
    "                self.metrics[\"loop_detection\"].append({\n",
    "                    \"iteration\": self.metrics[\"iteration_count\"],\n",
    "                    \"pattern\": self.last_n_tools.copy()\n",
    "                })\n",
    "    \n",
    "    def record_llm_latency(self, duration_ms: float):\n",
    "        \"\"\"Record LLM call duration\"\"\"\n",
    "        self.metrics[\"llm_latencies\"].append(duration_ms)\n",
    "    \n",
    "    def record_error(self, error_type:str, details:str):\n",
    "        \"\"\"Record an error\"\"\"\n",
    "        self.metrics[\"errors\"].append(\n",
    "            {\n",
    "                \"timestamp\": datetime.utcnow().isoformat(),\n",
    "                \"type\": error_type,\n",
    "                \"details\": details\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def get_summary(self) -> dict:\n",
    "        \"\"\"Generate metrics summary\"\"\"\n",
    "\n",
    "        summary = {\n",
    "            \"total_iterations\": self.metrics[\"iteration_count\"],\n",
    "            \"total_tool_calls\": sum(self.metrics[\"tool_calls\"].values()),\n",
    "            \"tool_usage\": self.metrics[\"tool_calls\"],\n",
    "            \"error_count\": len(self.metrics[\"errors\"]),\n",
    "            \"potential_loops\": len(self.metrics[\"loop_detection\"])\n",
    "        }\n",
    "\n",
    "        # Calculate average latencies\n",
    "        if self.metrics[\"llm_latencies\"]:\n",
    "            summary[\"avg_llm_latency_ms\"] = sum(self.metrics[\"llm_latencies\"]) /len(self.metrics[\"llm_latencies\"])\n",
    "\n",
    "        summary[\"tool_avg_latencies\"] = {}\n",
    "        for tool, latencies in self.metrics[\"tool_latencies\"].items():\n",
    "            if latencies:\n",
    "                summary[\"tool_avg_latencies\"][tool] =sum(latencies) / len(latencies)\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def check_anomalies(self) -> List[str]  :\n",
    "        \"\"\"Detect anomalous behaviour\"\"\"\n",
    "\n",
    "        wanrings = []\n",
    "        # Check for excessive iterations\n",
    "        if self.metrics[\"iteration_count\"] > 15:\n",
    "            wanrings.append(f\" High iteration count: {self.metrics[\"iteration_count\"]}\")\n",
    "        \n",
    "        # Check for tool call loops\n",
    "        if self.metrics[\"loop_detection\"]:\n",
    "            wanrings.append(f\"Possible loop detected: {len(self.metrics[\"loop_detection\"])} instances\")\n",
    "        # Check for excessive errors\n",
    "        if len(self.metrics[\"errors\"]) > 3:\n",
    "            wanrings.append(f\"Muliple errors: {len(self.metrics[\"errors\"])}\")\n",
    "        \n",
    "        # Check for slow operations\n",
    "        if self.metrics[\"llm_latencies\"]:\n",
    "            avg_llm = sum(self.metrics[\"llm_latencies\"])/len(self.metrics[\"llm_latencies\"])\n",
    "            if avg_llm > 2000:\n",
    "                wanrings.append(f\"Slow LLM calls: avg {avg_llm:.0f}ms\")\n",
    "        \n",
    "        return wanrings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109a01cd",
   "metadata": {},
   "source": [
    "### Integrating Metrics\n",
    "Add the metrics collector and update instrumented methods\n",
    "\n",
    "```python\n",
    "class CodeReviewAgentPlanning:\n",
    "    def __init__(self, tools_registry: ToolRegistry, ...):\n",
    "        # ... existing init ...\n",
    "        self.metrics = MetricsCollector()\n",
    "        \n",
    "    def run(self, user_query: str, max_iterations=10):\n",
    "        \"\"\"Main loop with metrics\"\"\"\n",
    "        run_span_id = self.tracer.start_span(\n",
    "            name=f\"Agent Run: {user_query[:50]}\", \n",
    "            span_type=\"AGENT_RUN\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            step = 0\n",
    "            current_input = user_query\n",
    "            \n",
    "            while step < max_iterations:\n",
    "                self.metrics.record_iteration()\n",
    "                \n",
    "                # ... existing loop logic ...\n",
    "                \n",
    "            # Check for anomalies at the end\n",
    "            warnings = self.metrics.check_anomalies()\n",
    "            if warnings:\n",
    "                print(\"\\n⚠️  Performance Warnings:\")\n",
    "                for warning in warnings:\n",
    "                    print(f\"  {warning}\")\n",
    "                    \n",
    "            return \"Task Incomplete: Max steps reached\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.metrics.record_error(\"RUNTIME_ERROR\", str(e))\n",
    "            self.tracer.end_span(run_span_id, \"ERROR\", {\"error\": str(e)})\n",
    "            raise\n",
    "        finally:\n",
    "            # Always save instrumentation\n",
    "            self.save_instrumentation()\n",
    "            \n",
    "    def think(self, user_input: str):\n",
    "        \"\"\"Thinking with metrics\"\"\"\n",
    "        think_span_id = self.tracer.start_span(\"Think\", \"LLM_CALL\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # ... existing think logic ...\n",
    "            \n",
    "            response = openai.responses.create(model=self.model, input=messages)\n",
    "            decision = response.output_text\n",
    "            \n",
    "            duration_ms = (time.time() - start_time) * 1000\n",
    "            self.metrics.record_llm_latency(duration_ms)\n",
    "            \n",
    "            # ... rest of think ...\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.metrics.record_error(\"THINK_ERROR\", str(e))\n",
    "            raise\n",
    "            \n",
    "    def act(self, decision: str):\n",
    "        \"\"\"Tool execution with metrics\"\"\"\n",
    "        # ... existing act logic ...\n",
    "        \n",
    "        start_time = time.time()\n",
    "        result = self.tools.call(tool_name, *args)\n",
    "        duration_ms = (time.time() - start_time) * 1000\n",
    "        \n",
    "        self.metrics.record_tool_call(tool_name, duration_ms)\n",
    "        \n",
    "        # ... rest of act ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa7a5c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict\n",
    "import openai\n",
    "import os\n",
    "\n",
    "\n",
    "## Set up the tools and tools registry\n",
    "def write_test(file_path:str, test_code: str) -> str:\n",
    "    \"\"\"Write test code to a test file\"\"\"\n",
    "    try:\n",
    "        test_dir = os.path.dirname(file_path) or \"tests\"\n",
    "        if not os.path.exists(test_dir):\n",
    "            os.makedirs(test_dir)\n",
    "\n",
    "        with open(file_path, \"w\") as f:\n",
    "            f.write(test_code)\n",
    "        return f\"Test file created: {file_path}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error writing test file {file_path: {e}}\"\n",
    "\n",
    "def run_test(file_path: str) -> str:\n",
    "    \"\"\"Run a Python test file and return results\"\"\"\n",
    "    try:\n",
    "        import subprocess\n",
    "        result = subprocess.run(\n",
    "            [\"python\",\"-m\",\"pytest\", file_path,\"-v\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=30\n",
    "        )\n",
    "        return f\"Exit code {result.returncode}\\n\\nOuput:\\n{result.stdout}\\n\\nErrors:\\n{result.stderr}\"\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return \"Test execution timed out after 30 seconds\"\n",
    "    except Exception as e:\n",
    "        return f\"Error running tests: {e}\"\n",
    "\n",
    "def read_file(file_path: str) -> str:\n",
    "    \"\"\"Read contents of a Python file\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        return f\"File not found: {file_path}\"\n",
    "    \n",
    "    with open(file_path, \"r\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def analyze_code(code: str) -> str:\n",
    "    \"\"\"Ask an LLM to analyze the provided code.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful code review assistant.\n",
    "    Analyze the following Python code and suggest one improvement.\n",
    "\n",
    "    Code:\n",
    "    {code}\n",
    "    \"\"\"\n",
    "\n",
    "    response = openai.responses.create(model=\"gpt-4.1-mini\",input=[{\"role\":\"user\",\"content\":prompt}])\n",
    "\n",
    "    return response.output_text\n",
    "\n",
    "def patch_file(filepath: str, content: str) -> str:\n",
    "    \"\"\"Writes the given content to a file, completely replacing its current content.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, \"w\") as f:\n",
    "            f.write(content)\n",
    "        return f\"File successfully updated: {filepath}. New content written.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error writing to file {filepath}: {e}\"\n",
    "        \n",
    "class ToolRegistry:\n",
    "    \"\"\"Holds available tools and dispatches them by name.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.tools: Dict[str,Callable] = {}\n",
    "    \n",
    "    def register(self, name:str, func: Callable):\n",
    "        self.tools[name] = func\n",
    "\n",
    "    def call(self, name:str, *args, **kwargs):\n",
    "        if name not in self.tools:\n",
    "            return f\"Unknown tool: {name}\"\n",
    "        return self.tools[name](*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aa8c68",
   "metadata": {},
   "source": [
    "### Persisting Obervability Data\n",
    "Let's add some utitity methods to the agent to persist and display observability data.  \n",
    "We will update the `run()` method to always perist observability data\n",
    "```python\n",
    "    def run(self, user_query: str, max_iterations=10):\n",
    "        #....rest of run method\n",
    "        except Exception as e:\n",
    "            self.metrics.record_error(\"RUNTIME_ERROR\", str(e))\n",
    "            self.tracer.end_span(run_span_id, \"ERROR\", {\"error\": str(e)})\n",
    "            raise\n",
    "        finally:\n",
    "            # Always save instrumentation\n",
    "            self.save_instrumentation()\n",
    "```\n",
    "\n",
    "```python\n",
    "    def save_instrumentation(self, trace_file=\"traces.json\",log_file=\"log.json\",token_file=\"tokens.json\",metrics_file=\"metrics.json\"):\n",
    "        self.tracer.save_traces(trace_file)\n",
    "        self.logger.save_logs(log_file)\n",
    "\n",
    "        with open(token_file,\"w\") as tf:\n",
    "            json.dump({\n",
    "                \"summary\":self.token_tracker.get_summary(),\n",
    "                \"detailed_log\": self.token_tracker.token_log\n",
    "            },tf,indent=2)\n",
    "        \n",
    "        with open(metrics_file,\"w\") as mf:\n",
    "            json.dump({\n",
    "                \"summary\": self.metrics.get_summary(),\n",
    "                \"detailed_metrics\": self.metrics.metrics,\n",
    "                \"anomalies\": self.metrics.check_anomalies()\n",
    "            },f, indent=2)\n",
    "        \n",
    "        print(f\"\\n Instrumentation save:\")\n",
    "        print(f\" - Traces {trace_file}\")\n",
    "        print(f\" - Logs:{log_file}\")\n",
    "        print(f\" - Tokens: {token_file}\")\n",
    "        print(f\" - Metrics: {metrics_file}\")\n",
    "\n",
    "        # Print summary to console\n",
    "        print(f\"\\n Execution Summary\")\n",
    "        token_summary = self.token_tracker.get_summary()\n",
    "        print(f\" Cost: {token_summary[\"estimated_cost_usd\"]:.4f}\")\n",
    "        print(f\" Tokens: {token_summary[\"total_tokens\"]:,}\")\n",
    "        metric_summary = self.metrics.get_summary()\n",
    "        print(f\" Tools calls: {metric_summary[\"total_tool_calls\"]}\")\n",
    "        print(f\" Iterations: {metric_summary[\"total_iterations\"]}\")\n",
    "    \n",
    "    def print_trace_summary(self):\n",
    "        \"\"\"Print a visual summary of execution traces\"\"\"\n",
    "        traces = [trace.to_dict() for trace in self.tracer.traces]\n",
    "        TraceVisualizer.print_all_traces(traces)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf157b9",
   "metadata": {},
   "source": [
    "### Agent with observability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14da14c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import json\n",
    "\n",
    "class CodeReviewAgentObservable:\n",
    "    def __init__(self,tools_registry: ToolRegistry, model=\"gpt-4.1\",memory_file=\"agent_memory.json\",summarize_after=10,max_context_tokens=6000):\n",
    "        self.tools = tools_registry\n",
    "        self.model = model\n",
    "        self.conversation_history = [] # Short-term memory\n",
    "        self.memory_file = memory_file\n",
    "        self.load_long_term_memory() # Long-term memory (key-value store)\n",
    "        self.conversation_summary = \"\" # Summarized conversation history\n",
    "        self.summarize_after = summarize_after\n",
    "        self.turns_since_summary = 0\n",
    "        self.max_context_tokens = max_context_tokens\n",
    "        self.current_plan = [] #List of planned steps\n",
    "        self.completed_steps = [] # Track what has been done\n",
    "        self.plan_created = False\n",
    "\n",
    "        # Add logger\n",
    "        self.logger = AgentLogger(agent_id=f\"code-reivew-{int(time.time())}\")\n",
    "        self.logger.log(LogLevel.INFO,\"AGENT_INIT\",\"Agent initialized\",{\"model\":model,\"max_token\":max_context_tokens})\n",
    "        self.tracer = TraceManager()\n",
    "        self.token_tracker = TokenTracker(model=model)\n",
    "        self.metrics = MetricsCollector()\n",
    "\n",
    "        # Initialize tokenizer for the model\n",
    "        try:\n",
    "            self.tokenizer = tiktoken.encoding_for_model(model)\n",
    "        except:\n",
    "            self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    def count_tokens(self, text:str) -> int:\n",
    "        \"\"\"Count tokens in a string\"\"\"\n",
    "        return len(self.tokenizer.encode(text))\n",
    "    \n",
    "    def trim_history_to_fit(self, system_message:str):\n",
    "        \"\"\"Remove old messages until we fit within the token budget\"\"\"\n",
    "\n",
    "        # Count tokens in system message\n",
    "        fixed_tokens = self.count_tokens(system_message)\n",
    "\n",
    "        # Count tokens in conversation history\n",
    "        history_tokens = sum([self.count_tokens(msg[\"content\"]) for msg in self.conversation_history])\n",
    "\n",
    "        total_tokens = fixed_tokens + history_tokens\n",
    "\n",
    "        while total_tokens > self.max_context_tokens and len(self.conversation_history) > 2:\n",
    "            removed_msg = self.conversation_history.pop(0)\n",
    "            total_tokens -= self.count_tokens(removed_msg[\"content\"])\n",
    "\n",
    "        return total_tokens\n",
    "\n",
    "\n",
    "    def summarize_history(self):\n",
    "        \"\"\"Use LLM to summarize the conversation so far.\"\"\"\n",
    "        if len(self.conversation_history) < 3:\n",
    "            return\n",
    "        \n",
    "        history_text = \"\\n\".join([f\"{msg[\"role\"]}:{msg[\"content\"]}\" for msg in self.conversation_history])\n",
    "\n",
    "        summary_prompt = f\"\"\"Summarize this conversation in 3-4 sentences,\n",
    "        preserving key fact, decisions, and actions taken:\n",
    "        {history_text}\n",
    "\n",
    "        Previous Summary: {self.conversation_summary or 'None'}\n",
    "        \"\"\"\n",
    "\n",
    "        response = openai.responses.create(model=self.model, input=[{\"role\":\"user\",\"content\":summary_prompt}])\n",
    "\n",
    "        self.conversation_summary = response.output_text\n",
    "\n",
    "        # Keep only the last few turns + the summary\n",
    "        recent_turns = self.conversation_history[-4:] # Keep the last 4 messages (2 user/assistant exchanges)\n",
    "\n",
    "        self.conversation_history = recent_turns\n",
    "        self.turns_since_summary = 0\n",
    "\n",
    "\n",
    "    def remember(self, key:str, value: str):\n",
    "        \"\"\"Retrieve information from long term memory.\"\"\"\n",
    "        self.long_term_memory[key] = value\n",
    "        self.save_long_term_memory()\n",
    "    \n",
    "    def recall(self,key:str) -> str:\n",
    "        \"\"\"Retrieve information from long term memory\"\"\"\n",
    "        return self.long_term_memory.get(key,\"No memory found for this key.\")\n",
    "    \n",
    "    def get_relevant_memories(self) -> str:\n",
    "        \"\"\"Format long term memories for inclusion in prompts.\"\"\"\n",
    "        if not self.long_term_memory:\n",
    "            return \"No stored memories\"\n",
    "        \n",
    "        memories = \"\\n\".join([f\"- {k}:{v}\" for k, v in self.long_term_memory.items()])\n",
    "        return f\"Relevant memories:\\n{memories}\"\n",
    "    \n",
    "    def save_long_term_memory(self):\n",
    "        \"\"\"Persist long term memory to JSON file\"\"\"\n",
    "        try:\n",
    "            with open(self.memory_file,\"w\") as f:\n",
    "                json.dump(self.long_term_memory,f,indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save memory to {self.memory_file}:  {e}\")\n",
    "\n",
    "    def load_long_term_memory(self):\n",
    "        \"\"\"Load long term memory from JSON file\"\"\"\n",
    "        if os.path.exists(self.memory_file):\n",
    "            try:\n",
    "                with open(self.memory_file, 'r') as f:\n",
    "                    self.long_term_memory = json.load(f)\n",
    "                print(f\"Loaded {len(self.long_term_memory)} memories from {self.memory_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load memory from {self.memory_file}: {e}\")\n",
    "        else:\n",
    "            self.long_term_memory = {}\n",
    "    \n",
    "    def create_plan(self, user_query:str) -> list:\n",
    "        \"\"\"Generate a step by step plan for the user's request\"\"\"\n",
    "        planning_prompt = f\"\"\"\n",
    "        Given this task:\"\"{user_query}\"\"\n",
    "        Create a detailed execution plan with numbered steps. Each step should be a specific action\n",
    "\n",
    "        Available tools:\n",
    "        - read_file(file): Read a file's contents\n",
    "        - analyze_code(code): Get code analysis and suggestions\n",
    "        - patch_file(file_path, content): Update a file\n",
    "        - write_test(file_path, text_code): Create a test file\n",
    "        - run_test(file_path): Execute tests\n",
    "\n",
    "        Format your response as a JSON list of steps\n",
    "        [\n",
    "        {{\"step\":1,\"action\":\"description\",\"tool\":\"tool_name\"}},\n",
    "        {{\"step\":1,\"action\":\"description\",\"tool\":\"tool_name\"}}\n",
    "        ]\n",
    "\n",
    "        Only include necessary steps. Be specific about which files to work with.\n",
    "        \"\"\"\n",
    "\n",
    "        resposnse = openai.responses.create(model=self.model,\n",
    "                                            input=[{\"role\":\"user\",\"content\":planning_prompt}])\n",
    "        \n",
    "        try:\n",
    "            plan = json.loads(resposnse.output_text)\n",
    "            self.current_plan = plan\n",
    "            self.plan_created = True\n",
    "            return plan\n",
    "        except json.JSONDecodeError:\n",
    "            self.current_plan = [{\"step\":1,\"action\":\"Proceed step by step\",\"tool\":\"analyze_code\"}]\n",
    "            self.plan_created= True\n",
    "            return self.current_plan\n",
    "    \n",
    "    def _build_plan_context(self,next_step) -> str:\n",
    "        \"\"\"Format plan information for the prompt\"\"\"\n",
    "        completed = \"\\n\".join([f\"Step {step[\"step\"]}:{step[\"action\"]}\" for step in self.completed_steps])\n",
    "\n",
    "        if next_step:\n",
    "            current = f\"\\nCURRENT: Step {next_step[\"step\"]}: {next_step[\"action\"]}\"\n",
    "        else:\n",
    "            current = \"\\n All steps completed\"\n",
    "        \n",
    "        remaining = \"\\n\".join([f\" Step {step[\"step\"]}: {step[\"action\"]}\" for step in self.current_plan[len(self.completed_steps)+1:]])\n",
    "\n",
    "        execution_plan = f\"\"\"\n",
    "        Completed:\n",
    "        {completed if completed else \"None\"}\n",
    "        {current}\n",
    "        Remaining:\n",
    "        {remaining if remaining else \"None\"}\n",
    "        \"\"\"\n",
    "\n",
    "        return execution_plan\n",
    "\n",
    "    def think(self, user_input:str):\n",
    "        \"\"\"LLM enhanced thinking with logging and tracing\"\"\"\n",
    "        self.logger.log(LogLevel.INFO,\"THINK_START\",\"Starting Reasoning\",{\"user_input\":user_input[:100]})\n",
    "        think_span_id = self.tracer.start_span(\"Think\",\"LLM_CALL\")\n",
    "\n",
    "        try:\n",
    "\n",
    "            # First request: create a plan\n",
    "            if not self.plan_created:\n",
    "                plan = self.create_plan(user_query=user_input)\n",
    "\n",
    "                plan_summary = \"\\n\".join([f\"Step {step[\"step\"]}:{step[\"action\"]}\" for step in plan])\n",
    "\n",
    "                response = f\"\"\"\n",
    "                I have created this execution plan:\n",
    "                {plan_summary}\n",
    "                \n",
    "                I will now begin executing these steps\n",
    "                \"\"\"\n",
    "\n",
    "                return response\n",
    "\n",
    "            # Add user message to history\n",
    "            self.conversation_history.append({\"role\":\"user\",\"content\":user_input})\n",
    "\n",
    "            self.turns_since_summary += 1\n",
    "\n",
    "            # Check if we should summarize\n",
    "            if self.turns_since_summary >= self.summarize_after:\n",
    "                self.summarize_history()\n",
    "\n",
    "            # Get current step from plan\n",
    "            next_step = None\n",
    "            if len(self.completed_steps)<len(self.current_plan):\n",
    "                next_step = self.current_plan[len(self.completed_steps)]\n",
    "            \n",
    "            # Build context with plan information\n",
    "            plan_context = self._build_plan_context(next_step)\n",
    "\n",
    "\n",
    "            #Include long term memory & summary in system context\n",
    "            system_message_context = f\"\"\"You are a code assistant with access to these tools:\n",
    "                    - read_file(filepath)\n",
    "                    - analyze_code(code)\n",
    "                    - patch_file(filepath,content)\n",
    "                    - write_test(file_path,test_code)\n",
    "                    - run_test(file_path)\n",
    "\n",
    "                    {self.get_relevant_memories()}\n",
    "\n",
    "                    Conversation Summary: {self.conversation_summary or 'This is the start of the conversation'}\n",
    "\n",
    "                    {plan_context}\n",
    "\n",
    "                    Follow the ReAct pattern: **Thought**, then **Action** or a final **Answer**\n",
    "                    **Format your response STRICTLY as follows:**\n",
    "\n",
    "                    1. Thought:Your internal reasoning and plan.\n",
    "                    2. Action:The tool call to make in JSON format {{\"tool\": \"tool_name\", \"args\": [\"arg1\", \"arg2\"]}} (e.g., {{\"tool\":\"patch_file\", \"args\":[\"file_path\",\"content\"]}}. **OR**\n",
    "                    3. Answer:Your final response when all steps are complete.\n",
    "\n",
    "\n",
    "\n",
    "                    After each successful action I'll mark that step as complete and move to the next one\n",
    "\n",
    "                    \"\"\"\n",
    "\n",
    "            self.trim_history_to_fit(system_message_context)\n",
    "            \n",
    "            # Build prompt with system instructions\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\":\"system\",\n",
    "                    \"content\":system_message_context\n",
    "                }\n",
    "            ] + self.conversation_history\n",
    "\n",
    "\n",
    "            input_text = json.dumps([msg[\"content\"] for msg in messages])\n",
    "            input_tokens = self.count_tokens(input_text)\n",
    "\n",
    "            start_time = time.time()\n",
    "            response = openai.responses.create(model=self.model, input=messages)\n",
    "\n",
    "            duration_ms = (time.time() - start_time) * 1000\n",
    "\n",
    "            decision = response.output_text\n",
    "\n",
    "            # Add assistant's decision to conversation history\n",
    "            self.conversation_history.append({\n",
    "                \"role\":\"assistant\",\n",
    "                \"content\": decision\n",
    "            })\n",
    "\n",
    "            self.tracer.end_span(think_span_id,\"SUCCESS\",{\"messages\":messages,\"decision\":decision})\n",
    "            self.logger.log(LogLevel.INFO,\"THINK_COMPLETE\",\"Reasoning Complete\",{\"decision\":decision})\n",
    "            output_tokens = self.count_tokens(decision)\n",
    "            self.token_tracker.track_usage(input_tokens,output_tokens,\"think\")\n",
    "            self.metrics.record_llm_latency(duration_ms)\n",
    "\n",
    "            return decision\n",
    "        except Exception as e:\n",
    "            self.tracer.end_span(think_span_id,\"ERROR\"), {\"error\": str(e)}\n",
    "    \n",
    "    def act(self, decision:str):\n",
    "        \"\"\"Execute the chosen tool and update plan progress with logging\"\"\"\n",
    "\n",
    "        self.logger.log(LogLevel.INFO,\"ACT_START\",\"Executing action\",{\"decision\":decision})\n",
    "        act_span_id = self.tracer.start_span(\"Act\",\"TOOL_EXECUTION\")\n",
    "\n",
    "        try:\n",
    "            parsed = json.loads(decision)\n",
    "            tool_name = parsed[\"tool\"]\n",
    "            args = parsed.get(\"args\",[])\n",
    "\n",
    "\n",
    "            self.logger.log(LogLevel.DEBUG,\"TOOL_CALL\",f\"Calling {tool_name}\",{\"tool\":tool_name,\"args\":args})\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            result = self.tools.call(tool_name,*args)\n",
    "\n",
    "            duration = time.time() - start_time\n",
    "\n",
    "            self.logger.log(LogLevel.INFO,\"TOOL_COMPLETE\",f\"{tool_name} completed\", {\"tool\":tool_name,\"duration_ms\":duration*1000})\n",
    "            self.tracer.end_span(act_span_id,\"SUCCESS\")\n",
    "            self.metrics.record_tool_call(tool_name,duration_ms=duration*1000)\n",
    "\n",
    "            #Mark current step as complete\n",
    "            if len(self.completed_steps) < len(self.current_plan):\n",
    "                current_step = self.current_plan[len(self.completed_steps)]\n",
    "                self.completed_steps.append(current_step)\n",
    "\n",
    "            self.conversation_history.append({\"role\":\"system\",\"content\":result})\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error executing tool: {e}\"\n",
    "            self.logger.log(LogLevel.ERROR,\"ACT_ERROR\",error_msg,{\"decision:\":decision,\"error\":str(e)})\n",
    "            self.tracer.end_span(act_span_id,\"ERROR\",{\"error\":str(e)})\n",
    "            self.conversation_history.append({\n",
    "                \"role\":\"system\",\n",
    "                \"content\": error_msg\n",
    "            })\n",
    "            return error_msg\n",
    "\n",
    "    def run(self, user_query:str, max_iterations=10):\n",
    "        \"\"\"\n",
    "        Main execution loop with reflection.\n",
    "        Args:\n",
    "            user_query: The user's request\n",
    "            max_iterations: Maxumum number of think-act-reflect cycles\n",
    "        \n",
    "        Returns:\n",
    "            Final response string\n",
    "        \"\"\"\n",
    "        run_span_id = self.tracer.start_span(f\"Agent Run: {user_query[:50]}\",span_type=\"AGENT_RUN\")\n",
    "\n",
    "        try:\n",
    "            step = 0\n",
    "\n",
    "            current_input = user_query\n",
    "\n",
    "            while step < max_iterations:\n",
    "                #Create a span for each iteration\n",
    "                iter_span_id = self.tracer.start_span(name=f\"Iteration {step + 1}\",span_type=\"ITERATION\")\n",
    "                self.metrics.record_iteration()\n",
    "\n",
    "                print(f\"\\n--- Step {step+1} ---\")\n",
    "\n",
    "                llm_response = self.think(current_input)\n",
    "\n",
    "                print(f\"Agent's LLM Response:\\n{llm_response}\")\n",
    "\n",
    "                #Check if the response is the plan. If it is go to the first step\n",
    "                if \"I have created this execution plan\" in llm_response:\n",
    "                    current_input = \"Proceed with step 1\"\n",
    "                    step +=1\n",
    "                    continue\n",
    "\n",
    "                if \"Answer:\" in llm_response:\n",
    "                    final_answer = llm_response.split(\"Answer:\",1)[1].strip()\n",
    "\n",
    "                    self.tracer.end_span(iter_span_id,\"SUCCESS\",{\"final_answer\":final_answer[:100]})\n",
    "                    self.tracer.end_span(run_span_id,\"SUCCESS\",{\"total_iterations\": step +1})\n",
    "\n",
    "                    # print(f\"\\n Agent Finished: \\n {final_answer}\")\n",
    "                    return final_answer\n",
    "                if \"Action:\" in llm_response:\n",
    "                    action_line = llm_response.split(\"Action:\",1)[1].split(\"\\n\")[0].strip()\n",
    "                    print(f\"Acting: {action_line}\")\n",
    "\n",
    "                    tool_result = self.act(action_line)\n",
    "\n",
    "                    print(f\"\\nTool Result:\\n{tool_result}\")\n",
    "                    current_input = f\"Observation:{tool_result}\"\n",
    "                else:\n",
    "                    error_msg = f\"LLM did not provide valid Action or Answer: LLM Respose:: {llm_response}\"\n",
    "                    print(f\"\\n Error: {error_msg}\")\n",
    "                    return error_msg\n",
    "                \n",
    "                self.tracer.end_span(iter_span_id,\"SUCCESS\")\n",
    "                step +=1\n",
    "\n",
    "            self.tracer.end_span(run_span_id,\"MAX_ITERATIONS\",{\"completed_steps\":len(self.completed_steps),\"total_steps\":len(self.current_plan)})\n",
    "            # Check for anomalies at the end\n",
    "            warnings = self.metrics.check_anomalies()\n",
    "            if warnings:\n",
    "                print(\"\\n Performance Warnings:\") \n",
    "                for warning in warnings:\n",
    "                    print(f\"    {warning}\")\n",
    "\n",
    "            return \"Task Incomplete: Max steps reached\"\n",
    "        except Exception as e:\n",
    "            self.tracer.end_span(run_span_id,\"ERROR\",{\"error\":str(e)})\n",
    "            raise\n",
    "        finally:\n",
    "            self.save_instrumentation()\n",
    "\n",
    "    def save_instrumentation(self, trace_file=\"traces.json\",log_file=\"log.json\",token_file=\"tokens.json\",metrics_file=\"metrics.json\"):\n",
    "        self.tracer.save_traces(trace_file)\n",
    "        self.logger.save_logs(log_file)\n",
    "\n",
    "        with open(token_file,\"w\") as tf:\n",
    "            json.dump({\n",
    "                \"summary\":self.token_tracker.get_summary(),\n",
    "                \"detailed_log\": self.token_tracker.token_log\n",
    "            },tf,indent=2)\n",
    "        \n",
    "        with open(metrics_file,\"w\") as mf:\n",
    "            json.dump({\n",
    "                \"summary\": self.metrics.get_summary(),\n",
    "                \"detailed_metrics\": self.metrics.metrics,\n",
    "                \"anomalies\": self.metrics.check_anomalies()\n",
    "            },mf, indent=2)\n",
    "        \n",
    "        print(f\"\\n Instrumentation save:\")\n",
    "        print(f\" - Traces {trace_file}\")\n",
    "        print(f\" - Logs:{log_file}\")\n",
    "        print(f\" - Tokens: {token_file}\")\n",
    "        print(f\" - Metrics: {metrics_file}\")\n",
    "\n",
    "        # Print summary to console\n",
    "        print(f\"\\n Execution Summary\")\n",
    "        token_summary = self.token_tracker.get_summary()\n",
    "        print(f\" Cost: {token_summary[\"estimated_cost_usd\"]:.4f}\")\n",
    "        print(f\" Tokens: {token_summary[\"total_tokens\"]:,}\")\n",
    "        metric_summary = self.metrics.get_summary()\n",
    "        print(f\" Tools calls: {metric_summary[\"total_tool_calls\"]}\")\n",
    "        print(f\" Iterations: {metric_summary[\"total_iterations\"]}\")\n",
    "    \n",
    "    def print_trace_summary(self):\n",
    "        \"\"\"Print a visual summary of execution traces\"\"\"\n",
    "        traces = [trace.to_dict() for trace in self.tracer.traces]\n",
    "        TraceVisualizer.print_all_traces(traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b0657f",
   "metadata": {},
   "outputs": [],
   "source": [
    "registry = ToolRegistry()\n",
    "registry.register(\"read_file\",read_file)\n",
    "registry.register(\"analyze_code\", analyze_code)\n",
    "registry.register(\"write_test\",write_test)\n",
    "registry.register(\"patch_file\",patch_file)\n",
    "registry.register(\"run_test\",run_test)\n",
    "\n",
    "agent = CodeReviewAgentObservable(tools_registry=registry,model=\"gpt-4.1\",max_context_tokens=8000)\n",
    "\n",
    "user_query = \"Review sample.py\"\n",
    "\n",
    "result = agent.run(user_query)\n",
    "\n",
    "agent.print_trace_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675d9504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a11eb14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
