{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70cceb7b",
   "metadata": {},
   "source": [
    "# Model Context Protocol: Connecting AI Agents to the World\n",
    "As our code review agent grows more sophisticated, we face a challenge; how do we connect it to the growing ecosystem of external tools and data sources.\n",
    "So far, our agent's tools are hardcoded Python functions. Every time we want to add a new capability, like, access to a database, integrations with a project management system or connection to a version control system, we need to write custom code.\n",
    "\n",
    "The **Model Context Protocol (MCP)** solves this problem by providin a standardized way for AI applications to connect to data sources and tools.  \n",
    "Instead of building custom integrations for every external system, MCP provides an universal protocol.\n",
    "\n",
    "Model Context Protocol is an open protocol developed by [Anthropic](https://www.anthropic.com/news/model-context-protocol) that standardizes how AI applications connect to external systems. It defines:\n",
    "* **A client-server architecture:** where your agent (the client) connects to MCP servers that expose tools and data\n",
    "* **A standard message format:** using [JSON-RPC 2.0](https://www.jsonrpc.org/specification) for communication\n",
    "* **MCP architecture primitives:** Resources(data), Tools(actions), and Prompts(templates)\n",
    "\n",
    "## MCP Architecture: Core Concepts\n",
    "\n",
    "### The Three Primitives\n",
    "MCP defines three types of capabilities that servers can expose:\n",
    "\n",
    "#### 1. Resources\n",
    "Resources are **data sources** that an agent can read. These could be files, database records or documents the agent can access\n",
    "\n",
    "```\n",
    "Example Resources:\n",
    "- File contents: file:///home/user/project/\n",
    "- Database records: db://localhost/project_db/users\n",
    "- API responses: github://anthropics/repo/issues\n",
    "```\n",
    "Resources are identified by URIs and can be:\n",
    "- **Listed:** \"What resources are available\"\n",
    "- **Read:** \"Give me the contents of this resource\"\n",
    "- **Subscribed to:** \"Notify me when this resource changes\"\n",
    "\n",
    "#### 2. Tools\n",
    "Tools are **actions** that an agent can execute. They are like the functions our agent already uses, but standardized.\n",
    "\n",
    "```\n",
    "Example Tools:\n",
    "- create_github_issue(title, body)\n",
    "- query_database(sql)\n",
    "- send_email(to, subject, body)\n",
    "```\n",
    "Tools have\n",
    "- **Names:** Unique identifiers\n",
    "- **Descriptions:** What the tool does (This helps the LLM choose)\n",
    "- **Input Schemas:** What parameters they accept - [JSON Schema](https://json-schema.org/docs)\n",
    "- **Outputs:** Results of the action\n",
    "\n",
    "#### 3. Prompts\n",
    "Prompts are **templated interactions** that servers can provide. They are like reusable prompt snippets with variables.\n",
    "\n",
    "```\n",
    "Example Prompts:\n",
    "- code_review: \"Review this code: {{code}}\"\n",
    "- bug_report: \"Report a bug in {{file}} at line {{line}}\"\n",
    "- summarize_pr: \"Summarize PR #{{number}}\"\n",
    "```\n",
    "\n",
    "### Client=Server Architecture\n",
    "#### The Agent (Client)\n",
    "- Discovers what servers are available\n",
    "- Requests resources and tool definitions\n",
    "- Calls tools through the protocol\n",
    "- Processes results\n",
    "\n",
    "#### The Server\n",
    "- Exposes capabilities (resources, tools, prompts)\n",
    "- Handles requests from clients\n",
    "- Manages authentication and permissions\n",
    "- Returns structured responses\n",
    "\n",
    "### Communication Protocol\n",
    "MCP uses JSON-RPC 2.0 over standard transport layer (stdio, HTTP, or WebSocket):\n",
    "\n",
    "```json\n",
    "// Client asks: \"What tools do you have\"\n",
    "{\n",
    "    \"jsonrpc\": \"2.0\",\n",
    "    \"method\": \"tools/list\",\n",
    "    \"params\": {},\n",
    "    \"id\": 1\n",
    "}\n",
    "\n",
    "// Server responds: \"Here are my tools\"\n",
    "{\n",
    "    \"jsonrpc\":\"2.0\",\n",
    "    \"result\": {\n",
    "        \"tools\": [\n",
    "            {\n",
    "                \"name\":\"read_file\",\n",
    "                \"description\": \"Read file contents\",\n",
    "                \"inputSchema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"path\": {\"type\":\"string\"}\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"id\": 1\n",
    "}\n",
    "```\n",
    "\n",
    "## Building our own MCP Server\n",
    "Let's convert our code review tools into an MCP Server. This will make them usable by any MCP compatible client.\n",
    "\n",
    "We will be making changes to this code [CodeReviewAgentWithTools](https://github.com/asanyaga/ai-agents-tutorial/blob/main/code_review_agent_with_tools.ipynb)\n",
    "\n",
    "### Setting up the MCP SDK\n",
    "Install the MCP Python SDK\n",
    "\n",
    "```bash\n",
    "pip install mcp\n",
    "```\n",
    "## Creating a Basic MCP Server\n",
    "\n",
    "Create a seperate file called code_review_mcp_server.py to run the server. Add the following code\n",
    "\n",
    "```python\n",
    "from mcp.server import Server\n",
    "import mcp.server.stdio\n",
    "\n",
    "# Create the server instance\n",
    "server = Server(\"code-review-server\")\n",
    "\n",
    "# We'll add capabilities here\n",
    "\n",
    "# Run the server over stdio\n",
    "async def main():\n",
    "    async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):\n",
    "        await server.run(\n",
    "            read_stream,\n",
    "            write_stream,\n",
    "            server.create_initialization_options()\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import asyncio\n",
    "    asyncio.run(main())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6eb6a8",
   "metadata": {},
   "source": [
    "### Converting read_file to an MCP Resource\n",
    "Resources represent data that can be read. The ```read_file``` function is a perfect candidate\n",
    "Update the server code with the following\n",
    "\n",
    "```python\n",
    "from mcp.server import Server\n",
    "from mcp.types import Tool, TextContent, Resource\n",
    "import mcp.server.stdio\n",
    "import asyncio\n",
    "import os\n",
    "# Create the server instance\n",
    "server = Server(\"code-review-server\")\n",
    "\n",
    "# Define a resource template for files\n",
    "\n",
    "@server.list_resources()\n",
    "async def list_resources() -> list[Resource]:\n",
    "    \"\"\"List available file resources in the current directory\"\"\"\n",
    "    resources = []\n",
    "    for root, dirs, files in os.walk(\".\"):\n",
    "        for file in files:\n",
    "            if file.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                resources.append(\n",
    "                    Resource(uri=f\"file://{os.path.abspath(file_path)}\",\n",
    "                             name=\"file\",\n",
    "                             description=f\"Python file: {file_path}\",\n",
    "                             mimeType=\"text/x-python\")\n",
    "                )\n",
    "    return resources\n",
    "@server.read_resource()\n",
    "async def read_resource(uri: str) -> str:\n",
    "    \"\"\"Read the contents of a file resource\"\"\"\n",
    "    # Extract the file path from the uri\n",
    "    if uri.startswith(\"file://\"):\n",
    "        file_path = uri[:7] # Remove \"file//:\" prefix\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "            raise ValueError(f\"File not found: {file_path}\")\n",
    "        \n",
    "        with open(file_path,\"r\") as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        return TextContent(\n",
    "            type=\"text\",\n",
    "            text=content\n",
    "        )\n",
    "\n",
    "    raise ValueError(f\"Usupported URI {uri}\")\n",
    "```\n",
    "\n",
    "**What changed**\n",
    "- ```read_file(path)``` becomes an MCP resource with a URI scheme\n",
    "- Files are **discovered** through ```list_resources```\n",
    "- Files are ***accessed** through ```read_resource(uri)```\n",
    "- Resources are self describing with metadata (name, type, description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5b7606",
   "metadata": {},
   "source": [
    "### Converting Tools to MCP Tools\n",
    "Now, let's convert our analyze_code tool\n",
    "\n",
    "Update the MCP server code with the following\n",
    "```python\n",
    "from mcp.types import Tool, TextContent, Resource\n",
    "\n",
    "@server.list_tools()\n",
    "async def list_tools() -> list[Tool]:\n",
    "    \"\"\"Declare available tools\"\"\"\n",
    "    return [\n",
    "        Tool(\n",
    "            name=\"analyze_code\",\n",
    "            description=\"Analyze Python code and provide imrovement suggestions\",\n",
    "            inputSchema={\n",
    "                \"type\":\"object\",\n",
    "                \"properties\": {\n",
    "                    \"code\" : {\n",
    "                        \"type\":\"string\",\n",
    "                        \"description\": \"The Python code to analyze\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    ]\n",
    "\n",
    "@server.call_tool()\n",
    "async def call_tool(name: str, arguments: dict) -> list[TextContent]:\n",
    "    if name == \"analyze_code\":\n",
    "        code = arguments[\"code\"]\n",
    "        result = analyze_code_impl(code)\n",
    "\n",
    "        return [TextContent(type=\"text\",text=result)]\n",
    "        \n",
    "# Implementation functions (existing code)\n",
    "def analyze_code_impl(code: str) -> str:\n",
    "    \"\"\"Implementation of code analysis\"\"\"\n",
    "    import openai\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful code review assistant.\n",
    "    Analyze the following Python code and suggest one improvement.\n",
    "\n",
    "    Code:\n",
    "    {code}\n",
    "    \"\"\"\n",
    "    response = openai.responses.create(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        input=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.output_text\n",
    "```\n",
    "\n",
    "#### what changed\n",
    "- Tools are declared with JSON schema for their inputs\n",
    "- Tools are invoked through a standard ```call_tool``` dispatcher\n",
    "- Tool metadata (description, parameters) helps LLMs choose the right tool\n",
    "- Results are wrapped in standard MCP response types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bd0845",
   "metadata": {},
   "source": [
    "## Converting the agent to an MCP client\n",
    "Now let's update our agent to MCP servers instead of using hardcoded tools.\n",
    "\n",
    "### Create an MCP Client Manager\n",
    "Let's create a manager that handles MCP server connections\n",
    "\n",
    "Create a file ```run_mcp_agent.py`` with the following code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cc908f",
   "metadata": {},
   "source": [
    "```run_mcp_agent.py```\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "MCP Code Review Agent - Standalone Script\n",
    "Run: python run_mcp_agent.py\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "from contextlib import AsyncExitStack\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from typing import Optional\n",
    "import json\n",
    "import openai\n",
    "import os\n",
    "\n",
    "class MCPClientManager:\n",
    "    \"\"\"Official MCP Client Manager using AsyncExitStack\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.session: Optional[ClientSession] = None\n",
    "        self.exit_stack = AsyncExitStack()\n",
    "        self.available_tools: dict[str, dict] = {}\n",
    "    \n",
    "    async def connect_to_server(self, server_script_path: str):\n",
    "        \"\"\"Connect to an MCP server\"\"\"\n",
    "        is_python = server_script_path.endswith('.py')\n",
    "        is_js = server_script_path.endswith('.js')\n",
    "        if not (is_python or is_js):\n",
    "            raise ValueError(\"Server script must be a .py or .js file\")\n",
    "        \n",
    "        command = \"python\" if is_python else \"node\"\n",
    "\n",
    "        server_env = {\"OPENAI_API_KEY\": os.getenv(\"OPENAI_API_KEY\")}\n",
    "\n",
    "\n",
    "        server_params = StdioServerParameters(\n",
    "            command=command,\n",
    "            args=[server_script_path],\n",
    "            env=server_env\n",
    "        )\n",
    "        \n",
    "        print(f\"ðŸ”Œ Connecting to MCP server: {server_script_path}\")\n",
    "        \n",
    "        # Use AsyncExitStack (official pattern)\n",
    "        stdio_transport = await self.exit_stack.enter_async_context(\n",
    "            stdio_client(server_params)\n",
    "        )\n",
    "        stdio, write = stdio_transport\n",
    "        \n",
    "        self.session = await self.exit_stack.enter_async_context(\n",
    "            ClientSession(stdio, write)\n",
    "        )\n",
    "        \n",
    "        await self.session.initialize()\n",
    "        \n",
    "        # Discover tools\n",
    "        response = await self.session.list_tools()\n",
    "        for tool in response.tools:\n",
    "            self.available_tools[tool.name] = {\"tool\": tool}\n",
    "            print(f\"Discovered tool: {tool.name}\")\n",
    "        \n",
    "        print(f\"Connected successfully!\\n\")\n",
    "    \n",
    "    async def call_tool(self, tool_name: str, arguments: dict) -> str:\n",
    "        \"\"\"Call a tool through MCP\"\"\"\n",
    "        if not self.session:\n",
    "            raise RuntimeError(\"Not connected to a server\")\n",
    "        \n",
    "        if tool_name not in self.available_tools:\n",
    "            raise ValueError(f\"Unknown tool: {tool_name}\")\n",
    "        \n",
    "        result = await self.session.call_tool(tool_name, arguments)\n",
    "        \n",
    "        if result.content and len(result.content) > 0:\n",
    "            return result.content[0].text\n",
    "        \n",
    "        return \"Tool executed successfully (no output)\"\n",
    "    \n",
    "    def get_tool_descriptions(self) -> list[dict]:\n",
    "        \"\"\"Get tool descriptions for the LLM\"\"\"\n",
    "        return [\n",
    "            {\n",
    "                \"name\": tool_name,\n",
    "                \"description\": tool_info[\"tool\"].description,\n",
    "                \"input_schema\": tool_info[\"tool\"].inputSchema\n",
    "            }\n",
    "            for tool_name, tool_info in self.available_tools.items()\n",
    "        ]\n",
    "    \n",
    "    async def cleanup(self):\n",
    "        \"\"\"Clean up resources\"\"\"\n",
    "        print(\"\\nCleaning up...\")\n",
    "        await self.exit_stack.aclose()\n",
    "        print(\"Cleanup complete!\")\n",
    "\n",
    "class CodeReviewAgentMCP:\n",
    "    \"\"\"Code review agent powered by MCP\"\"\"\n",
    "    \n",
    "    def __init__(self, mcp_manager: MCPClientManager, model=\"gpt-4o-mini\"):\n",
    "        self.mcp = mcp_manager\n",
    "        self.model = model\n",
    "    \n",
    "    async def think(self, user_input: str) -> str:\n",
    "        \"\"\"LLM decides which tool to use\"\"\"\n",
    "        tool_descriptions = self.mcp.get_tool_descriptions()\n",
    "        tools_list = \"\\n\".join([\n",
    "            f\"- {tool['name']}: {tool['description']}\"\n",
    "            for tool in tool_descriptions\n",
    "        ])\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        You are a code assistant with access to these tools:\n",
    "        \n",
    "        {tools_list}\n",
    "        \n",
    "        Based on the user request, decide which tool to use.\n",
    "        Reply ONLY with JSON: {{\"tool\": \"tool_name\", \"args\": {{\"param\": \"value\"}}}}\n",
    "        \n",
    "        Example: {{\"tool\": \"analyze_code\", \"args\": {{\"code\": \"def foo(): pass\"}}}}\n",
    "        \n",
    "        User request: {user_input}\n",
    "        \"\"\"\n",
    "        \n",
    "        response = openai.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    async def act(self, decision: str) -> str:\n",
    "        \"\"\"Execute the chosen tool\"\"\"\n",
    "        try:\n",
    "            parsed = json.loads(decision)\n",
    "            tool_name = parsed[\"tool\"]\n",
    "            args = parsed.get(\"args\", {})\n",
    "            \n",
    "            result = await self.mcp.call_tool(tool_name, args)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            return f\"Error executing tool: {e}\"\n",
    "    \n",
    "    async def run(self, user_input: str) -> str:\n",
    "        \"\"\"Complete think-act loop\"\"\"\n",
    "        print(f\"Thinking about: {user_input}\")\n",
    "        decision = await self.think(user_input)\n",
    "        print(f\"Decision: {decision}\")\n",
    "        \n",
    "        print(f\"\\nExecuting...\")\n",
    "        result = await self.act(decision)\n",
    "        print(f\"\\nResult:\\n{result}\")\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MCP Code Review Agent\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    \n",
    "    # Check for OpenAI API key\n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        print(\"Warning: OPENAI_API_KEY environment variable not set\")\n",
    "        print(\"Please set it with: export OPENAI_API_KEY='your-key-here'\")\n",
    "        return\n",
    "    \n",
    "    mcp_manager = MCPClientManager()\n",
    "    \n",
    "    try:\n",
    "        # Connect to MCP server\n",
    "        await mcp_manager.connect_to_server(\"code_review_mcp_server.py\")\n",
    "        \n",
    "        # Create agent\n",
    "        agent = CodeReviewAgentMCP(mcp_manager)\n",
    "        \n",
    "        # Test with a code snippet\n",
    "        code_snippet = \"\"\"\n",
    "def divide(a, b):\n",
    "    return a / b\n",
    "        \"\"\"\n",
    "        \n",
    "        user_request = f\"Please analyze this code: {code_snippet}\"\n",
    "        result = await agent.run(user_request)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"Demo Complete!\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: code_review_mcp_server.py not found\")\n",
    "        print(\"Make sure the MCP server file is in the same directory\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        await mcp_manager.cleanup()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the async main function\n",
    "    asyncio.run(main())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe988d6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
